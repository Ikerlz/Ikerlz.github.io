<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Slides | Zhe Li</title>
    <link>https://ikerlz.github.io/slides/</link>
      <atom:link href="https://ikerlz.github.io/slides/index.xml" rel="self" type="application/rss+xml" />
    <description>Slides</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 Jan 2024 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Disentangle Mixture Distributions and Instrumental Variable Inequalities</title>
      <link>https://ikerlz.github.io/slides/causalinferencechapter22/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://ikerlz.github.io/slides/causalinferencechapter22/</guid>
      <description>
&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;ZIBIN.jpg&#34;
  &gt;

&lt;h1 id=&#34;disentangle-mixture-distributions-and-instrumental-variable-inequalities&#34;&gt;Disentangle Mixture Distributions and Instrumental Variable Inequalities&lt;/h1&gt;
&lt;p&gt;$$
\begin{aligned}
\
\end{aligned}
$$&lt;/p&gt;
&lt;center&gt; Li Zhe &lt;center&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;School of Data Science, Fudan University &lt;center&gt; 
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;January 3, 2024&lt;center&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The IV model in last chapter imposes three assumptions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(&lt;mark&gt;randomization&lt;/mark&gt;) $Z \perp\!\!\!\perp\{D(1), D(0), Y(1), Y(0)\}$&lt;/li&gt;
&lt;li&gt;(&lt;mark&gt;monotonicity&lt;/mark&gt;) $\operatorname{pr}(U=\mathrm{d})=0$ or $D_i(1)\geq D_i(0)$&lt;/li&gt;
&lt;li&gt;(&lt;mark&gt;exclusionrestriction&lt;/mark&gt;) $Y(1)=Y(0) \text { for } U=\mathrm{a} \text { or } \mathrm{n}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
{\tiny
U_i= \begin{cases}
a, &amp;amp; \text { if } D_i(1)=1 \text { and } D_i(0)=1 \\
c, &amp;amp; \text { if } D_i(1)=1 \text { and } D_i(0)=0 \\
d, &amp;amp; \text { if } D_i(1)=0 \text { and } D_i(0)=1 \\
n, &amp;amp; \text { if } D_i(1)=0 \text { and } D_i(0)=0
\end{cases}
}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Observed groups and latent groups under the assumptions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
{\small
\begin{array}{cccl}
Z=1 &amp;amp; D=1 &amp;amp; D(1)=1 &amp;amp; U=\mathrm{c} \text { or a } \\
Z=1 &amp;amp; D=0 &amp;amp; D(1)=0 &amp;amp; U=\mathrm{n} \\
Z=0 &amp;amp; D=1 &amp;amp; D(0)=1 &amp;amp; U=\mathrm{a} \\
Z=0 &amp;amp; D=0 &amp;amp; D(0)=0 &amp;amp; U=\mathrm{c} \text { or } \mathrm{n}
\end{array}
}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interestingly, the assumptions have some &lt;mark&gt;&lt;strong&gt;testable implications&lt;/strong&gt;&lt;/mark&gt;. Balke and Pearl (1997) called them the &lt;mark&gt;&lt;strong&gt;instrumental variable inequalities&lt;/strong&gt;&lt;/mark&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;outline&#34;&gt;Outline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Disentangle Mixture Distributions and Instrumental Variable Inequalities&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Testable implications&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Examples&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;disentangle-mixture-distributions-and-iv-inequalities&#34;&gt;Disentangle Mixture Distributions and IV Inequalities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recall $\pi_u$ as the proportion of type $U=u$, and define&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\mu_{z u}=E\{Y(z) \mid U=u\}, \quad(z=0,1 ; u=\mathrm{a}, \mathrm{n}, \mathrm{c}) .
$$&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;&lt;strong&gt;Theorem 22.1&lt;/strong&gt;&lt;/mark&gt; Under the three assumptions, we can identify the proportions of the latent types by
$$
\begin{aligned}
&amp;amp; \pi_{\mathrm{n}}=\operatorname{pr}(D=0 \mid Z=1), \\
&amp;amp; \pi_{\mathrm{a}}=\operatorname{pr}(D=1 \mid Z=0), \\
&amp;amp; \pi_{\mathrm{c}}=E(D \mid Z=1)-E(D \mid Z=0),
\end{aligned}
$$
and the type-specific means of the potential outcomes by
$$
\begin{aligned}
\mu_{1 \mathrm{n}}=\mu_{0 \mathrm{n}} \equiv \mu_{\mathrm{n}} &amp;amp; =E(Y \mid Z=1, D=0) \\
\mu_{1 \mathrm{a}}=\mu_{0 \mathrm{a}} \equiv \mu_{\mathrm{a}} &amp;amp; =E(Y \mid Z=0, D=1) \\
\mu_{1 \mathrm{c}} &amp;amp; =\pi_{\mathrm{c}}^{-1}\{E(D Y \mid Z=1)-E(D Y \mid Z=0)\} \\
\mu_{0 \mathrm{c}} &amp;amp; =\pi_{\mathrm{c}}^{-1}[E\{(1-D) Y \mid Z=0\}-E\{(1-D) Y \mid Z=1\}]
\end{aligned}
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;proof-of-theorem-221-part-i&#34;&gt;Proof of Theorem 22.1 (Part I)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We can identify the proportion of the &lt;em&gt;never takers&lt;/em&gt; by&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{aligned}
\operatorname{pr}(D=0 \mid Z=1) &amp;amp; =\operatorname{pr}(U=\mathrm{n} \mid Z=1)=\operatorname{pr}(U=\mathrm{n})=\pi_{\mathrm{n}},
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the proportion of the &lt;em&gt;always takers&lt;/em&gt; by&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{aligned}
\operatorname{pr}(D=1 \mid Z=0) &amp;amp; =\operatorname{pr}(U=\mathrm{a} \mid Z=0)=\operatorname{pr}(U=\mathrm{a})=\pi_{\mathrm{a}} .
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the proportion of compliers is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{aligned}
\pi_{\mathrm{c}} &amp;amp; =\operatorname{pr}(U=\mathrm{c})=1-\pi_{\mathrm{n}}-\pi_{\mathrm{a}} \\
&amp;amp; =1-\operatorname{pr}(D=0 \mid Z=1)-\operatorname{pr}(D=1 \mid Z=0) \\
&amp;amp; =E(D \mid Z=1)-E(D \mid Z=0)=\tau_D,
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; Although we do not know individual latent compliance types for all units, we can identify the proportions of never takers, always takers, and compliers.&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;proof-of-theorem-221-part-ii&#34;&gt;Proof of Theorem 22.1 (Part II)&lt;/h3&gt;
&lt;p&gt;Under the three assumptions, we have&lt;/p&gt;
&lt;p&gt;$$
\mu_{1 \mathrm{a}}=\mu_{0 \mathrm{a}} \equiv \mu_{\mathrm{a}}, \quad \mu_{1 \mathrm{n}}=\mu_{0 \mathrm{n}} \equiv \mu_{\mathrm{n}} .
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The observed group $(Z=1, D=0)$ only has never takers, so&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
E(Y \mid Z=1, D=0)=E\{Y(1) \mid Z=1, U=\mathrm{n}\}=E\{Y(1) \mid U=\mathrm{n}\}=\mu_{\mathrm{n}} .
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The observed group $(Z=0, D=1)$ only has always takers, so&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
E(Y \mid Z=0, D=1)=E\{Y(0) \mid Z=0, U=\mathrm{a}\}=E\{Y(0) \mid U=\mathrm{a}\}=\mu_{\mathrm{a}}
$$&lt;/p&gt;
&lt;p&gt;$$~~$$&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;How about the observed groups $(Z=1, D=1)$ and $(Z=0, D=0)$ ?&lt;/font&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h4 id=&#34;the-observed-group-z1-d1&#34;&gt;The Observed Group $(Z=1, D=1)$&lt;/h4&gt;
&lt;p&gt;The observed group $(Z=1, D=1)$ has both &lt;em&gt;compliers&lt;/em&gt; and &lt;em&gt;always takers&lt;/em&gt;, so
$$
\small
\begin{aligned}
E(Y \mid Z=1, D=1)= &amp;amp; E\{Y(1) \mid Z=1, D(1)=1\} \\
= &amp;amp; E\{Y(1) \mid D(1)=1\} \\
= &amp;amp; \operatorname{pr}\{D(0)=1 \mid D(1)=1\} E\{Y(1) \mid D(1)=1, D(0)=1\} \\
&amp;amp; +\operatorname{pr}\{D(0)=0 \mid D(1)=1\} E\{Y(1) \mid D(1)=1, D(0)=0\} \\
= &amp;amp; \frac{\pi_{\mathrm{c}}}{\pi_{\mathrm{c}}+\pi_{\mathrm{a}}} \mu_{1 \mathrm{c}}+\frac{\pi_{\mathrm{a}}}{\pi_{\mathrm{c}}+\pi_{\mathrm{a}}} \mu_{\mathrm{a}} .
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Solve the linear equation above to obtain
$$
\small
\begin{aligned}
\mu_{1 \mathrm{c}}= &amp;amp; \pi_{\mathrm{c}}^{-1}\left\{\left(\pi_{\mathrm{c}}+\pi_{\mathrm{a}}\right) E(Y \mid Z=1, D=1)-\pi_{\mathrm{a}} E(Y \mid Z=0, D=1)\right\} \\
= &amp;amp; \pi_{\mathrm{c}}^{-1}\{\operatorname{pr}(D=1 \mid Z=1) E(Y \mid Z=1, D=1) \\
&amp;amp; \quad-\operatorname{pr}(D=1 \mid Z=0) E(Y \mid Z=0, D=1)\} \\
= &amp;amp; \pi_{\mathrm{c}}^{-1}\{E(D Y \mid Z=1)-E(D Y \mid Z=0)\} .
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Based on the formulas of $\mu_{1 \mathrm{c}}$ and $\mu_{0 \mathrm{c}}$ in Theorem 22.1, we have&lt;/p&gt;
&lt;p&gt;$$
\tau_{\mathrm{c}}=\mu_{1 \mathrm{c}}-\mu_{0 \mathrm{c}}=\{E(Y \mid Z=1)-E(Y \mid Z=0)\} / \pi_{\mathrm{c}}.
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;testable-implications&#34;&gt;Testable implications&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Is there any additional value of the this detour for deriving the formula of $\tau_c$ ?&lt;/li&gt;
&lt;li&gt;For binary outcome, the following inequalities must be true:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
0 \leq \mu_{1 \mathrm{c}} \leq 1, \quad 0 \leq \mu_{0 \mathrm{c}} \leq 1,
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It implies four inequalities&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\small
\begin{aligned}
E(D Y \mid Z=1)-E(D Y \mid Z=0) &amp;amp; \geq 0, \\
E(D Y \mid Z=1)-E(D Y \mid Z=0) &amp;amp; \leq E(D \mid Z=1)-E(D \mid Z=0), \\
E\{(1-D) Y \mid Z=0\}-E\{(1-D) Y \mid Z=1\} &amp;amp; \geq 0, \\
E\{(1-D) Y \mid Z=0\}-E\{(1-D) Y \mid Z=1\} &amp;amp; \leq E(D \mid Z=1)-E(D \mid Z=0) .
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;&lt;strong&gt;Theorem 22.2 (Instrumental Variable Inequalities)&lt;/strong&gt;&lt;/mark&gt; With a binary outcome $Y$, the three assumptions imply&lt;/p&gt;
&lt;p&gt;$$
E(Q \mid Z=1)-E(Q \mid Z=0) \geq 0,
$$&lt;/p&gt;
&lt;p&gt;where $Q=D Y, D(1-Y),(D-1) Y$ and $D+Y-D Y$.&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;remarks&#34;&gt;Remarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Under the three assumptions, the difference in means for $Q=$ $D Y, D(1-Y),(D-1) Y$ and $D+Y-D Y$ must all be non-negative.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Importantly, these implications only involve the distribution of the &lt;strong&gt;observed&lt;/strong&gt; variables.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rejection of the IV inequalities leads to rejection of the IV assumptions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Balke and Pearl (1997) derived more general IV inequalities without assuming monotonicity.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;eample-1&#34;&gt;Eample 1.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Investigators et al. (2014) assess the effectiveness of the &lt;em&gt;emergency endovascular&lt;/em&gt; versus &lt;em&gt;the open surgical repair strategies&lt;/em&gt; for patients with a clinical diagnosis of ruptured aortic aneurism.&lt;/li&gt;
&lt;li&gt;Patients are randomized to either the emergency endovascular or the open repair strategy.&lt;/li&gt;
&lt;li&gt;The primary outcome is the survival status after 30 days&lt;/li&gt;
&lt;li&gt;Let $Z$ be the treatment assigned, with $Z=1$ for the endovascular strategy and $Z=0$ for the open repair.&lt;/li&gt;
&lt;li&gt;Let $D$ be the treatment received.&lt;/li&gt;
&lt;li&gt;Let $Y$ be the survival status, with $Y=1$ for dead, and $Y=0$ for alive.&lt;/li&gt;
&lt;li&gt;The estimate of $\tau_{\mathrm{c}}$ is 0.131 with $95 \%$ confidence interval $(-0.036,0.298)$ including 0 .&lt;/li&gt;
&lt;li&gt;Using the function &amp;ldquo;&lt;code&gt;IVbinary&lt;/code&gt;&amp;rdquo; above, we can obtain $\hat\mu_{1c}=0.708$ and $\hat\mu_{0c}=0.629$&lt;/li&gt;
&lt;li&gt;There is no evidence of violating the IV assumptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;example-2&#34;&gt;Example 2.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In Hirano et al. (2000), physicians are randomly selected to receive a letter encouraging them to inoculate patients at risk for flu.&lt;/li&gt;
&lt;li&gt;The treatment is the actual flu shot, and the outcome is an indicator for flu-related hospital visits.&lt;/li&gt;
&lt;li&gt;However, some patients do not comply with their assignments. Let $Z_i$ be the indicator of encouragement to receive the flu shot, with $Z=1$ if the physician receives the encouragement letter, and $Z=0$ otherwise.&lt;/li&gt;
&lt;li&gt;Let $D$ be the treatment received.&lt;/li&gt;
&lt;li&gt;Let $Y$ be the outcome, with $Y=0$ if for a flu-related hospitalization during the winter, and $Y=1$ otherwise.&lt;/li&gt;
&lt;li&gt;The estimate of $\tau_{\mathrm{c}}$ is 0.116 with $95 %$ confidence interval $(-0.061,0.293)$ including 0 .&lt;/li&gt;
&lt;li&gt;Using the function above, we can obtain $\hat\mu_{1c}=-0.0045$ and $\hat\mu_{0c}=0.1200$&lt;/li&gt;
&lt;li&gt;Since $\hat\mu_{1c}&amp;lt;0$, there is evidence of violating the IV assumptions.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>On factor models with random missing： EM estimation, inference, and cross validation</title>
      <link>https://ikerlz.github.io/slides/factormissing/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://ikerlz.github.io/slides/factormissing/</guid>
      <description>
&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;ZIBIN.jpg&#34;
  &gt;

&lt;h1 id=&#34;on-factor-models-with-random-missing-em-estimation-inference-and-cross-validation&#34;&gt;On factor models with random missing: EM estimation, inference, and cross validation&lt;/h1&gt;
&lt;p&gt;$$
\begin{aligned}
\
\end{aligned}
$$&lt;/p&gt;
&lt;center&gt; Li Zhe &lt;center&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;School of Data Science, Fudan University &lt;center&gt; 
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;November 30, 2023&lt;center&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;outline&#34;&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Background: &lt;strong&gt;factor model&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Motivation&lt;/li&gt;
&lt;li&gt;Factor models with random missing:
&lt;ul&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt;EM estimator&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt;Asymptotic properties&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;Determining the number of factors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simulation&lt;/li&gt;
&lt;li&gt;Empirical application&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;background-factor-model&#34;&gt;Background: factor model&lt;/h2&gt;
&lt;p&gt;$$
\begin{aligned}
x_{i t} &amp;amp; =\lambda_i f_t+e_{i t} \\
\boldsymbol{X} &amp;amp;= \boldsymbol{\Lambda} \boldsymbol{F}^\top + \boldsymbol{E}
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{X}=(\boldsymbol{x}_{\cdot 1},\ldots,\boldsymbol{x}_{\cdot T})\in\mathbb{R}^{N\times T}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{\Lambda}\in\mathbb{R}^{N\times R}$: factor loadings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{F}\in\mathbb{R}^{T\times R}$: common factors (latent, unobserved)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{E}\in\mathbb{R}^{N\times T}$: idiosyncratic (or error) component&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;${e_{it}}$ can exhibit both cross-sectional and temporal dependence.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given the factor number $k$, we can estimate the factors and factor loadings by
$$
\Big\{\widehat{\boldsymbol{\Lambda}}^k, \widehat{\boldsymbol{F}}^k\Big\}=\arg\min_{\boldsymbol{\Lambda}^k,\boldsymbol{F}^k}\frac{1}{NT}\Big\|\boldsymbol{X}- \boldsymbol{\Lambda}^k{\boldsymbol{F}^k}^\top\Big\|_F^2
$$
where $\boldsymbol{\Lambda}^k\in\mathbb{R}^{N\times k}$ and $\boldsymbol{F}^k\in\mathbb{R}^{T\times k}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Factor model in balanced panel has been thoroughly investigated.
$$$$&lt;/li&gt;
&lt;li&gt;How to handle the missing data problem in factor models?
$$$$
&lt;ul&gt;
&lt;li&gt;the expectation–maximization (EM) algorithm
$$$$&lt;/li&gt;
&lt;li&gt;the Kalman filter (KF)
$$$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;There is no formal study of the &lt;strong&gt;asymptotic properties&lt;/strong&gt; for the EM estimators of the factors and factor loadings for the PC estimation with &lt;mark&gt;missing observations&lt;/mark&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;notations&#34;&gt;Notations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;consider the factor model
$$
\boldsymbol{X} =  \boldsymbol{F}\boldsymbol{\Lambda}^\top + \varepsilon
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol{X}=(X_1,\ldots,X_N)$ where $X_i \equiv\left(X_{i 1}, \ldots, X_{i T}\right)^{\prime}$ and $X_{it}$ are &lt;font color=red&gt; &lt;strong&gt;missing at random&lt;/strong&gt; &lt;/font&gt;&lt;/li&gt;
&lt;li&gt;$\varepsilon=\left(\varepsilon_1, \ldots, \varepsilon_N\right)$ and $\varepsilon_i \equiv\left(\varepsilon_{i 1}, \ldots, \varepsilon_{i T}\right)^{\prime}$ for $i=1, \ldots, N$.&lt;/li&gt;
&lt;li&gt;$F=\left(F_1, \ldots, F_T\right)^{\prime}$ and $\Lambda=\left(\lambda_1, \ldots, \lambda_N\right)^{\prime}$ where $F_t$ and $\lambda_i$ are $R \times 1$ vectors of factors and factor loadings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$F^0=\left(F_1^0, \ldots, F_T^0\right)^{\prime}$ and $\Lambda^0=\left(\lambda_1^0, \ldots, \lambda_N^0\right)^{\prime}$ are the true values of $F$ and $\Lambda$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Omega \subset[N] \times[T]$ be the index set of the observations that are observed. That is,
$$
\Omega=\Big\{(i, t) \in[N] \times[T]: X_{i t} \text { is observed }\Big\}.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let $G$ denote a $T \times N$ matrix with $(t, i)$ th element given by $g_{i t}=\mathbf{1}\{(i, t) \in \Omega\}$ and is &lt;mark&gt;independent of $X, F^0, \Lambda^0$ and $\varepsilon$&lt;/mark&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;the-initial-estimates&#34;&gt;The initial estimates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Let $\tilde{X}=X \circ G$ and $\tilde{X}_{i t}=X_{i t} g_{i t}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The common component $C^0 \equiv F^0 \Lambda^0$ is &lt;mark&gt;a low rank matrix&lt;/mark&gt; $\Rightarrow$ it is possible to recover $C^0$ even when a large proportion of elements in $X$ are missing at random.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Under the standard condition that $E\left(\varepsilon_{i t} \mid F_t^0, \lambda_i^0\right)=0$, we can verify that $E\left(\frac{1}{q} \tilde{X} \mid F^0, \Lambda^0\right)=F^0 \Lambda^{0 \prime}$ $\Rightarrow$ consider the following least squares objective function
$$
\mathcal{L}_{N T}^0(F, \Lambda) \equiv \frac{1}{N T} \operatorname{tr}\left[\left(\frac{1}{\tilde{q}} \tilde{X}-F \Lambda^{\prime}\right)\left(\frac{1}{\tilde{q}} \tilde{X}-F \Lambda^{\prime}\right)^{\prime}\right]
$$
&lt;strong&gt;identification restrictions&lt;/strong&gt;: $F^{\prime} F / T=I_R$ and $\Lambda^{\prime} \Lambda$ is a diagonal matrix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;By concentrating out $\Lambda$ and using the normalization that $F^{\prime} F / T=I_R$ $\Rightarrow$ identical to maximizing $\tilde{q}^{-2} \operatorname{tr}\Big\{F^{\prime} \tilde{X} \tilde{X}^{\prime} F\Big\}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;the-initial-estimates-1&#34;&gt;The initial estimates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The estimated factor matrix, denoted by $\hat{F}^{(0)}$ is $\sqrt{T}$ times the eigenvectors corresponding to the $R$ largest eigenvalues of the $T \times T$ matrix $\frac{1}{N T \tilde{q}^2} \tilde{X} \tilde{X}^{\prime}:$
$$
\frac{1}{N T \tilde{q}^2} \tilde{X} \tilde{X}^{\prime} \hat{F}^{(0)}=\hat{F}^{(0)} \hat{D}^{(0)},
$$
&lt;ul&gt;
&lt;li&gt;$\hat{D}^{(0)}$ is an $R \times R$ diagonal matrix consisting of the $R$ largest eigenvalues of $\left(N T \tilde{q}^2\right)^{-1} \tilde{X} \tilde{X}^{\prime}$, arranged in descending order along its diagonal line.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The estimator of $\Lambda^{\prime}$ is given by
$$
\hat{\Lambda}^{(0) \prime}=\frac{1}{\tilde{q}}\left(\hat{F}^{(0) \prime} \hat{F}^{(0)}\right)^{-1} \hat{F}^{(0) \prime} \tilde{X}=\frac{1}{T \tilde{q}} \hat{F}^{(0) \prime} \tilde{X} .
$$&lt;/li&gt;
&lt;li&gt;We can obtain an initial estimate of the $(t, i)$ th element, $C_{i t}^0$, of $C^0$ by $\hat{C}_{i t}^{(0)}=\hat{\lambda}_i^{(0)\prime} \hat{F}_t^{(0)}$.&lt;/li&gt;
&lt;li&gt;The initial estimators $\hat{F}_t^{(0)}, \hat{\lambda}_i^{(0)}$ and $\hat{C}_{i t}^{(0)}$ are consistent and follow mixture normal distributions under some standard conditions.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;the-iterated-estimates&#34;&gt;The iterated estimates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The initial estimators: consistency but &lt;mark&gt;not asymptotically efficient&lt;/mark&gt; $\Rightarrow$ &lt;strong&gt;iterative estimators&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;In step $\ell$, we can &lt;mark&gt; replace the missing values $\left(X_{i t}\right)$ in the matrix $X$ with the estimated common components $\hat{C}_{i t}^{(\ell-1)}$&lt;/mark&gt;. Define the $T \times N$ matrix $\hat{X}^{(\ell)}$ with its $(t, i)$ th element given by
$$
\hat{X}_{i t}^{(\ell)}=\left\{\begin{array}{ll}
X_{i t} &amp;amp; \text { if }(i, t) \in \Omega \\
\hat{C}_{i t}^{(\ell-1)} &amp;amp; \text { if }(i, t) \in \Omega_{\perp}
\end{array}, \ell \geq 1,\right.
$$
where $\Omega_{\perp}=\{(i, t) \in[N] \times[T]:(i, t) \notin \Omega\}$.&lt;/li&gt;
&lt;li&gt;Then we can conduct the PC analysis based on $\hat{X}^{(\ell)}$ and obtain $\hat{F}^{(\ell) \prime}$ and $\hat{\Lambda}^{(\ell)}$.&lt;/li&gt;
&lt;li&gt;We will study the asymptotic properties of $\hat{F}_t^{(\ell)}, \hat{\lambda}_i^{(\ell)}$ and $\hat{C}_{i t}^{\left(\ell^{\ell}\right)}, \ell=1,2, \ldots$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
~\\\
~\\\
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;asymptotic-properties-of-the-initial-estimators&#34;&gt;Asymptotic properties of the initial estimators&lt;/h2&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;strong&gt;Theorem 2.1.&lt;/strong&gt;&lt;/font&gt; Suppose some assumptions hold. Then
$$\frac{1}{T}\Big\|\hat{F}^{(0)}-F^0 \hat{H}^{(0)}\Big\|_F^2=O_P\left({\color{red}\delta_{N T}^{-2}}\right)$$
where $\delta_{N T}=\sqrt{N} \wedge \sqrt{T}$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{H}^{(0)}$ is defined as
$$
\hat{H}^{(0)}=\left(N^{-1} \Lambda^{0 \prime} \Lambda^0\right) T^{-1} F^{0 \prime} \hat{F}^{(0)}\left(\hat{D}^{(0)}\right)^{-1},
$$
where $\hat{D}^{(0)}$ is asymptotically nonsingular by Lemma A.1.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;asymptotic-distributions&#34;&gt;Asymptotic distributions&lt;/h2&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;strong&gt;Theorem 2.2.&lt;/strong&gt;&lt;/font&gt; Suppose some assumptions hold. Suppose that $\left(T^{1 / 2}+N^{1 / 2}\right) \delta_{N T}^{-2}=o(1)$. Let $\hat{\Pi}_{t N}^{(0)}=\sqrt{N}\Big(\hat{F}_t^{(0)}-\hat{H}^{(0) \prime} F_t^0\Big)$ and $\hat{\Pi}_{i T}^{(0)}=\sqrt{T}\Big(\hat{\lambda}_i^{(0)}-(\hat{H}^{(0)})^{-1} \lambda_i^0\Big)$. Then as $(N, T) \rightarrow \infty$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\hat{\Pi}_{t N}^{(0)}=\Big(\hat{D}^{(0)}\Big)^{-1} \frac{1}{T} \hat{F}^{(0) t} F^0 \frac{1}{\sqrt{N} q} \sum_{i=1}^N \lambda_i^0 \xi_{i t}+O_P\Big(N^{1 / 2} \delta_{N T}^{-2}\Big) \rightarrow N\Big(0, D^{-1} Q \Gamma_{g, t}(q) Q^{\prime} D^{-1}\Big) \mathcal{G}^t$-stably,&lt;/li&gt;
&lt;li&gt;$\hat{\Pi}_{i T}^{(0)}=\hat{H}^{(0) \prime} \frac{1}{\sqrt{T} q} \sum_{t=1}^T F_t^0 \xi_{i t}+O_P\left(T^{1 / 2} \delta_{N T}^{-2}\right) \rightarrow N\left(0,\left(Q^{\prime}\right)^{-1} \Phi_{g, i}(q) Q^{-1}\right) \mathcal{G}^i$-stably,&lt;/li&gt;
&lt;li&gt;$\Big(\frac{1}{N} \Sigma_{F, i t}^{(0)}(q)+\frac{1}{T} \Sigma_{\Lambda, i t}^{(0)}(q)\Big)^{-1 / 2}\Big(\hat{C}_{i t}^{(0)}-C_{i t}^0\Big) \stackrel{d}{\rightarrow} N(0,1)$,&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;asymptotic-properties-of-the-iterated-estimators&#34;&gt;Asymptotic properties of the iterated estimators&lt;/h2&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;strong&gt;Theorem 2.3.&lt;/strong&gt;&lt;/font&gt; Suppose some assumptions hold. Then
$$\frac{1}{T}\Big\|\hat{F}^{(\ell)}-F^0 \hat{H}^{(\ell)}\Big\|^2=O_P\left(\delta_{N T}^{-2}\right)$$
for each $\ell$, where $\hat{H}^{(\ell)}$ is defined as
$$
\hat{H}^{(\ell)}=\left(N^{-1} \Lambda^{0 \prime} \Lambda^0\right) T^{-1} F^{0 \prime} \hat{F}^{(\ell)}\Big(\hat{D}^{(\ell)}\Big)^{-1},
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;asymptotic-distributions-1&#34;&gt;Asymptotic distributions&lt;/h2&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;strong&gt;Theorem 2.4.&lt;/strong&gt;&lt;/font&gt; Suppose some assumptions hold. Suppose that &lt;mark&gt;$\sqrt{N}\big(T^{\gamma_1 / 4} \delta_{N T}^{-2} \ln T+T^{-1+3 \gamma_1 / 4}\big)=o(1)$&lt;/mark&gt; and &lt;mark&gt;$\sqrt{T}\big(N^{\gamma_2 / 4} \delta_{N T}^{-2} \ln N+N^{-1+3 \gamma_2 / 4}\big)=o(1)$&lt;/mark&gt;. Let $\hat{\Pi}_{t N}^{(\ell)}=\sqrt{N}\Big(\hat{F}_t^{(\ell)}-\hat{H}^{(\ell) \prime} F_t^0\Big)$ and $\hat{\Pi}_{i T}^{(\ell)}=\sqrt{T}\Big(\hat{\lambda}_i^{(\ell)}-\hat{H}^{(\ell)-1} \lambda_i^0\Big)$. Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\Pi}_{t N}^{(\ell)}=D^{-1} Q \frac{1}{\sqrt{N}} \sum_{i=1}^N \lambda_i^0 \varepsilon_{i t} g_{i t}+(1-q) \hat{\Pi}_{t N}^{(\ell-1)}+o_P(1)$ uniformly in $t$ and
$$
\hat{\Pi}_{t N}^{(\ell)} \stackrel{d}{\rightarrow} N\left(0, D^{-1} Q {\color{red}\Gamma_{1 g, t}(q)} Q^{\prime} D^{-1}\right) \text { as }(\ell, N, T) \rightarrow \infty
$$&lt;/li&gt;
&lt;li&gt;$\hat{\Pi}_{i T}^{(\ell)}=\left(Q^{\prime}\right)^{-1} \frac{1}{\sqrt{T}} \sum_{t=1}^T F_t^0 \varepsilon_{i t} g_{i t}+(1-q) \hat{\Pi}_{i T}^{(\ell-1)}+o_P(1)$ uniformly in $i$ and
$$
\hat{\Pi}_{i T} \stackrel{d}{\rightarrow} N\left(0,\left(Q^{\prime}\right)^{-1} {\color{red}\Phi_{1 g, i}(q)} Q^{-1}\right) \text { as }(\ell, N, T) \rightarrow \infty,
$$&lt;/li&gt;
&lt;li&gt;$\left(\frac{1}{N} \Sigma_{1 F, i t}+\frac{1}{T} \Sigma_{1 \Lambda, i t}\right)^{-1 / 2}\left(\hat{C}_{i t}^{(\ell)}-C_{i t}^0\right) \stackrel{d}{\rightarrow} N(0,1)$ as $(\ell, N, T) \rightarrow \infty$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;determining-the-number-of-factors-via-cv&#34;&gt;Determining the number of factors via CV&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Given the $T \times N$ matrix of observations $X$:
&lt;ul&gt;
&lt;li&gt;randomly sample elements in $X$ with a fixed probability $p \in(0,1)$&lt;/li&gt;
&lt;li&gt;leave the rest $(1-p)$-proportion of observations as held-out entries for the out-of-sample evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Let $\Omega^\star \subset[N] \times[T]$ be the index set of the training entries and $\Omega_{\perp}^\star$ the index set of the held-out entries.&lt;/li&gt;
&lt;li&gt;Define the operator $P_{\Omega^\star}: \mathbb{R}^{T \times N} \rightarrow \mathbb{R}^{T \times N}$ by
$$
\left(P_{\Omega^\star} X\right)_{t i}=X_{i t} g_{i t}^\star=X_{i t} \mathbf{1}\left\{(i, t) \in \Omega^\star\right\},
$$
where $g_{i t}^\star=\mathbf{1}\left\{(i, t) \in \Omega^\star\right\}$.&lt;/li&gt;
&lt;li&gt;Let $G^\star$ denote a $T \times N$ matrix with $(t, i)$ th element given by $g_{i t}^\star$.&lt;/li&gt;
&lt;li&gt;Now we can regard $P_{\Omega^\star} X$ as the $T \times N$ data matrix with missing values replaced by zeros.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;determining-the-number-of-factors-via-cv-1&#34;&gt;Determining the number of factors via CV&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Given $P_{\Omega^\star} X$, we apply the proposed EM algorithm to recover the data via estimating the common component matrix $C$ for any given number of factors.&lt;/li&gt;
&lt;li&gt;To proceed, we consider the full singular value decomposition (SVD) for $\frac{1}{p} P_{\Omega^\star} X$:
$$
\frac{1}{p} P_{\Omega^\star} X=\tilde{U} \tilde{\Sigma} \tilde{V}^{\prime}=\sum_{r=1}^{T \wedge N} \tilde{u}_r \tilde{v}_r^{\prime} \tilde{\sigma}_r,
$$
&lt;ul&gt;
&lt;li&gt;$\tilde{U}\in\mathbb{R}^{T \times T}=\left(\tilde{u}_1, \ldots, \tilde{u}_T\right)$&lt;/li&gt;
&lt;li&gt;$\tilde{V}\in\mathbb{R}^{N \times N}=\left(\tilde{v}_1, \ldots, \tilde{v}_N\right)$&lt;/li&gt;
&lt;li&gt;$\tilde{\Sigma}\in\mathbb{R}^{T \times N}$ is the diagonal matrix that contains the singular values, $\tilde{\sigma}_1, \tilde{\sigma}_2, \ldots, \tilde{\sigma}_{T \wedge N}$, arranged in descending order along the main diagonal line.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Given any $R \leq T \wedge N$ and the training entries in $P_{\Omega^\star} X$, we can estimate the common component $C$ by the singular value thresholding procedure:
$$
\tilde{C}_R=S_H\left(\frac{1}{p} P_{\Omega^\star} X, R\right)=\tilde{U}_R \tilde{\Sigma}_R \tilde{V}_R^{\prime}=\sum_{r=1}^R \tilde{u}_r \tilde{v}_r^{\prime} \tilde{\sigma}_r,
$$
where $S_H(\cdot, R)$ is the rank-R truncated SVD of $\cdot$, t&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;determining-the-number-of-factors-via-cv-2&#34;&gt;Determining the number of factors via CV&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let $\tilde{C}_{R, i t}$ denote the $(t, i)$ th element of $\tilde{C}_R$ for $R \geq 1$.&lt;/li&gt;
&lt;li&gt;We propose to choose $R$ to minimize the following CV criterion function
$$
{\color{red}\widetilde{C V}(R)=\frac{1}{N T} \sum_{(i, t) \in \Omega_{\perp}^\star}\left[X_{i t}-\tilde{C}_{R, i t}\right]^2.}
$$&lt;/li&gt;
&lt;li&gt;Let $\tilde{R}=\arg \min _{0 \leq R \leq R_{\max }} \widetilde{C V}(R)$ where $R_{\max }$ is a fixed integer that is no less than $R_0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
~\\\
~\\\
~\\\
~\\\
~\\\
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;a-more-efficient-method&#34;&gt;A more efficient method&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt; Can we use the $\ell$-step estimator $\hat C_{R,it}^{(\ell)}$ ?&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;Suppose that we have obtained the estimates $\hat{C}_{R, i t}^{(\ell-1)}$. In step $\ell$, we can replace the zero elements in $X^\star \equiv P_{\Omega^\star} X$ with the estimated common components $\hat{C}_{R_{\max }, i t}^{(\ell-1)}$&lt;/li&gt;
&lt;li&gt;Define the $T \times N$ matrix $\hat{X}^{\star(\ell)}$ with its $(t, i)$ th element given by
$$
\hat{X}_{i t}^{\star(\ell)}= \begin{cases}X_{i t} &amp;amp; \text { if }(i, t) \in \Omega^\star \\ \hat{C}_{R_{\max }^{(\ell-1)}}, &amp;amp; \text { if }(i, t) \in \Omega_{\perp}^\star, \ell \geq 1,\end{cases}
$$
where $\Omega_{\perp}^\star=\big\{(i, t) \in[N] \times[T]:(i, t) \notin \Omega^\star\big\}$&lt;/li&gt;
&lt;li&gt;Conduct the singular value thresholding procedure:
$$
\hat{C}_R^{(\ell)}=S_H\big(\hat{X}^{*(\ell)}, R\big)=\hat{U}_R^{(\ell)} \hat{\Sigma}_R^{(\ell)} \hat{V}_R^{(\ell) \prime},
$$&lt;/li&gt;
&lt;li&gt;repeating the above procedure for $\ell=1, \ldots, \ell^\star \equiv\left\lfloor\ln \left(\epsilon_{N T}\right) / \log (p)\right\rfloor$&lt;/li&gt;
&lt;li&gt;Let $\hat{C}_R=\hat{C}_R^{(\ell^\star)}$ and &lt;font color=&#34;blue&#34;&gt;$\hat{R}=\arg \min _{0 \leq R \leq R_{\max }} \widehat{C V}(R)$&lt;/font&gt;, where
$$
\widehat{C V}(R)=\frac{1}{N T} \sum_{(i, t) \in \Omega_{\perp}^\star}\left[X_{i t}-\hat{C}_{R, i t}\right]^2
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;the-consistency-of-the-cv-method&#34;&gt;The consistency of the CV method&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumption A.7.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For $r=R_{0+1}, \ldots, R_{\max }, \\ P\Big(\big\|\tilde{u}_r\big\|_{\infty}\big\|\tilde{v}_r\big\|_{\infty} \leq 1 /\big(c_0 \sqrt{(N+T) \log (N+T)}\big)\Big) \rightarrow 1$
for some fixed $c_0&amp;lt;\infty$ as $(N, T) \rightarrow \infty$, $\left\|\tilde{u}_r\right\|_{\infty}=o_P(1)$, and $\left\|\tilde{v}_r\right\|_{\infty}=o_P(1)$&lt;/li&gt;
&lt;li&gt;$\max _{(i, t) \in \Omega_{\perp}^\star} \sum_{(j, s) \in \Omega_{\perp}^\star}\left|E\left[\varepsilon_{i t} \varepsilon_{j s} \mid P_{\Omega^\star} X, \Omega^\star\right]\right|=o_P\left(\delta_{N T}^2\right)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt; &lt;strong&gt;Theorem 3.1.&lt;/strong&gt; &lt;/font&gt; Suppose some assumptions hold. Then $P\left(\tilde{R}&amp;lt;R_0\right) \rightarrow 0$ as $(N, T) \rightarrow \infty$. If Assumption A.7 also holds, then $P\left(\tilde{R}&amp;gt;R_0\right) \rightarrow 0$ as $(N, T) \rightarrow \infty$.&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt; &lt;strong&gt;Theorem 3.2.&lt;/strong&gt; &lt;/font&gt; Suppose some assumptions hold. Then $P\left(\hat{R}&amp;lt;R_0\right) \rightarrow 0$ as $(N, T) \rightarrow \infty$. If Assumption A.7 also holds, then $P\left(\hat{R}&amp;gt;R_0\right) \rightarrow 0$ as $(N, T) \rightarrow \infty$.&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;cv-in-the-presence-of-random-missing&#34;&gt;CV in the presence of random missing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Consider the SVD for $\frac{1}{p \tilde{q}} P_{\Omega^\star} P_{\Omega} X$ :
$$
\frac{1}{p \tilde{q}} P_{\Omega^\star} P_{\Omega} X=\tilde{U} \tilde{\Sigma} \tilde{V}^{\prime},
$$&lt;/li&gt;
&lt;li&gt;Then we estimate the common component $C$:
$$
\tilde{C}_R=S_H\left(\frac{1}{p \tilde{q}} P_{\Omega^\star} P_{\Omega} X, R\right)=\tilde{U}_R \tilde{\Sigma}_R \tilde{V}_R^{\prime},
$$
where $\tilde{U}_R, \tilde{V}_R$, and $\tilde{\Sigma}_R$ are defined as before. Let $\tilde{R} \in\big\{0,1,2, \ldots, R_{\max }\big\}$ minimize the following $\mathrm{CV}$ function
$$
\widetilde{C V}(R)=\frac{1}{N T} \sum_{(i, t) \in {\color{red}\Omega_{\perp}^\star \cap \Omega}}\left[X_{i t}-\tilde{C}_{R, i t}\right]^2,
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;simulation&#34;&gt;Simulation&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /slides/factormissing/res1_hud27e756e265fc0d2cb710f209c9d45db_107607_2f7cec5a10d7bfe6d412012a5ab9c0a5.webp 400w,
               /slides/factormissing/res1_hud27e756e265fc0d2cb710f209c9d45db_107607_05f2bf9c31e512fe355ea8d1ef4f5611.webp 760w,
               /slides/factormissing/res1_hud27e756e265fc0d2cb710f209c9d45db_107607_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/slides/factormissing/res1_hud27e756e265fc0d2cb710f209c9d45db_107607_2f7cec5a10d7bfe6d412012a5ab9c0a5.webp&#34;
               width=&#34;760&#34;
               height=&#34;341&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;simulation-1&#34;&gt;Simulation&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /slides/factormissing/res2_hu23b46a819b33dded3f66308db50b25de_133888_88b57d810d199bbfeed191044ef9817f.webp 400w,
               /slides/factormissing/res2_hu23b46a819b33dded3f66308db50b25de_133888_66fba2e54ad960eb84d5c21b2ad1b172.webp 760w,
               /slides/factormissing/res2_hu23b46a819b33dded3f66308db50b25de_133888_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/slides/factormissing/res2_hu23b46a819b33dded3f66308db50b25de_133888_88b57d810d199bbfeed191044ef9817f.webp&#34;
               width=&#34;760&#34;
               height=&#34;341&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;simulation-2&#34;&gt;Simulation&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /slides/factormissing/res3_hu7874055ca76ca648576350362febeb90_146809_05d4439d9d2a0dc25dcadca149a1e7c4.webp 400w,
               /slides/factormissing/res3_hu7874055ca76ca648576350362febeb90_146809_f351b6581254c888c8fc99a8d24e00bc.webp 760w,
               /slides/factormissing/res3_hu7874055ca76ca648576350362febeb90_146809_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/slides/factormissing/res3_hu7874055ca76ca648576350362febeb90_146809_05d4439d9d2a0dc25dcadca149a1e7c4.webp&#34;
               width=&#34;760&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;empirical-application-forecasting-macroeconomic-variables&#34;&gt;Empirical application: Forecasting macroeconomic variables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We use a panel dataset FRED-QD, which is an unbalanced panel at the quarterly frequency.&lt;/li&gt;
&lt;li&gt;The dataset consists of 248 quarterly U.S. indicators from 1959Q1 to 2018Q2.&lt;/li&gt;
&lt;li&gt;Use 125 time series to estimate the latent factors.&lt;/li&gt;
&lt;li&gt;Consider the forecast based on the following factor-augmented autoregression (FA-AR) models:
$$
y_{t+h}^h=\phi_h^{(1)}+\phi_h^{(2)}(L) \hat{F}_t+\phi_h^{(3)}(L) y_t+\varepsilon_{t+h}^h, h=1,2,4,
$$
&lt;ul&gt;
&lt;li&gt;$y_t$ is one of the four macro-variables (i.e., RGDP, GDP, IP, and RDPI)&lt;/li&gt;
&lt;li&gt;$\hat{F}_t$ is the estimated vector of factors&lt;/li&gt;
&lt;li&gt;$\phi_h^{(1)}$ is the intercept term, $L$ is the lag operator&lt;/li&gt;
&lt;li&gt;$\phi_h^{(2)}(L)$ and $\phi_h^{(3)}(L)$ are finite-order polynomials of the lag operators&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;empirical-application-forecasting-macroeconomic-variables-1&#34;&gt;Empirical application: Forecasting macroeconomic variables&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /slides/factormissing/res4_hub014dcadc3f19bd2f11bc127b0f473cb_90038_1f791bf979cecda35c8f819415e4ea57.webp 400w,
               /slides/factormissing/res4_hub014dcadc3f19bd2f11bc127b0f473cb_90038_9e51371ff9ad119475e615645fe18bd8.webp 760w,
               /slides/factormissing/res4_hub014dcadc3f19bd2f11bc127b0f473cb_90038_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/slides/factormissing/res4_hub014dcadc3f19bd2f11bc127b0f473cb_90038_1f791bf979cecda35c8f819415e4ea57.webp&#34;
               width=&#34;760&#34;
               height=&#34;234&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h1 id=&#34;thanks-&#34;&gt;Thanks !&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Using the Propensity Score in Regressions for Causal Effects</title>
      <link>https://ikerlz.github.io/slides/causalinferencechapter14/</link>
      <pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://ikerlz.github.io/slides/causalinferencechapter14/</guid>
      <description>
&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;ZIBIN.jpg&#34;
  &gt;

&lt;h1 id=&#34;using-the-propensity-score-in-regressions-for-causal-effects&#34;&gt;Using the Propensity Score in Regressions for Causal Effects&lt;/h1&gt;
&lt;p&gt;$$
\begin{aligned}
\
\end{aligned}
$$&lt;/p&gt;
&lt;center&gt; Li Zhe &lt;center&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;School of Data Science, Fudan University &lt;center&gt; 
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;November 1, 2023&lt;center&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This chapter discusses two simple methods to use the propensity score:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the propensity score as a covariate in regressions&lt;/li&gt;
&lt;li&gt;running regressions weighted by the inverse of the propensity score&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reasons&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;they are easy to implement, which involve only standard statistical software packages for regressions;&lt;/li&gt;
&lt;li&gt;their properties are comparable to many more complex methods;&lt;/li&gt;
&lt;li&gt;they can be easily extended to allow for flexible statistical models including machine learning algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;outline&#34;&gt;Outline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Regressions with the propensity score as a covariate
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Theorem 14.1&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Proposition 14.1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regressions weighted by the inverse of the propensity score
&lt;ul&gt;
&lt;li&gt;Average causal effect
&lt;ul&gt;
&lt;li&gt;Theorem 14.2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Average causal effect on the treated units
&lt;ul&gt;
&lt;li&gt;Table 14.1&lt;/li&gt;
&lt;li&gt;Proposition 14.2&lt;/li&gt;
&lt;li&gt;Theorem 14.3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regressions-with-the-propensity-score-as-a-covariate&#34;&gt;Regressions with the propensity score as a covariate&lt;/h3&gt;
&lt;p&gt;$$
\text { Theorem 11.1 If } Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid X \text {, then } {\color{red}Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid e(X)} \text {. }
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By Theorem 11.1, if unconfoundedness holds conditioning on $X$, then it also holds conditioning on $e(X)$: $\color{red}{Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid e(X) }.$&lt;/li&gt;
&lt;li&gt;Analogous to (10.5), $\tau$ is also &lt;mark&gt;nonparametrically&lt;/mark&gt; identified by
$$
\tau=E[E\{Y \mid Z=1, e(X)\}-E\{Y \mid Z=0, e(X)\}],
$$&lt;/li&gt;
&lt;li&gt;$\Rightarrow$ The simplest regression specification is the OLS fit of $Y$ on $\{1, Z, e(X)\}$, with the coefficient of $Z$ as an estimator, denoted by $\tau_e$:
$$
\arg \min _{a, b, c} E\{Y-a-b Z-c e(X)\}^2
$$&lt;/li&gt;
&lt;li&gt;$\tau_e$ defined as the coefficient of $Z$.&lt;/li&gt;
&lt;li&gt;It is consistent for $\tau$ if
&lt;ul&gt;
&lt;li&gt;have a correct propensity score model&lt;/li&gt;
&lt;li&gt;the outcome model is indeed linear in $Z$ and $e(X)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\tau_e$ estimates $\tau_{\mathrm{O}}$ if we have a correct propensity score model even if the outcome model is &lt;mark&gt;completely misspecified&lt;/mark&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regressions-with-the-propensity-score-as-a-covariate-1&#34;&gt;Regressions with the propensity score as a covariate&lt;/h3&gt;
&lt;p&gt;&lt;mark&gt;Theorem 14.1&lt;/mark&gt; If $Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid X$, then the coefficient of $Z$ in the OLS fit of $Y$ on $\{1, Z, e(X)\}$ equals
$$
\tau_e=\tau_{\mathrm{O}}=\frac{E\left\{h_{\mathrm{O}}(X) \tau(X)\right\}}{E\left\{h_{\mathrm{O}}(X)\right\}},
$$
recalling that $h_{\mathrm{O}}(X)=e(X)\{1-e(X)\}$ and $\tau(X)=E\{Y(1)-Y(0) \mid X\}$.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\
\
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Corollary 14.1&lt;/mark&gt; If $Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid X$, then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the coefficient of $Z-e(X)$ in the OLS fit of $Y$ on $Z-e(X)$ or $\{1, Z-e(X)\}$ equals $\tau_{\mathrm{O}}$;&lt;/li&gt;
&lt;li&gt;the coefficient of $Z$ in the OLS fit of $Y$ on $\{1, Z, e(X), X\}$ equals $\tau_{\mathrm{O}}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regressions-with-the-propensity-score-as-a-covariate-2&#34;&gt;Regressions with the propensity score as a covariate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An unusual feature of Theorem 14.1 is that &lt;strong&gt;the overlap condition&lt;/strong&gt; ($0 &amp;lt; e(x) &amp;lt; 1$) is not needed any more.&lt;/li&gt;
&lt;li&gt;Even if some units have propensity score $e(X)$ equaling 0 or 1, their associate weight $e(X)\{1-e(X)\}$ is zero so that they do not contribute anything to the final parameter $\tau_O$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;frischwaughlovell-theorem&#34;&gt;Frisch–Waugh–Lovell Theorem&lt;/h2&gt;
&lt;p&gt;The Frisch–Waugh–Lovell (FWL) theorem reduces multivariate OLS to univariate OLS and therefore facilitate the understanding and calculation of the OLS coefficients.&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Theorem A2.2 (sample FWL)&lt;/mark&gt; With data $\left(Y, X_1, X_2, \ldots, X_p\right)$ containing column vectors, the coefficient of $X_1$ equals the coefficient of $\tilde{X}_1$ in the OLS fit of $Y$ or $\tilde{Y}$ on $\tilde{X}_1$, where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\tilde{Y}$ is the residual vector from the OLS fit of $Y$ on $\left(X_2, \ldots, X_p\right)$&lt;/li&gt;
&lt;li&gt;$\tilde{X}_1$ is the residual from the OLS fit of $X_1$ on $\left(X_2, \ldots, X_p\right)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;p&gt;Based on the FWL theorem, we can obtain $\tau_e$ in two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, we obtain the residual $\tilde{Z}$ from the OLS fit of $Z$ on ${1, e(X)}$;&lt;/li&gt;
&lt;li&gt;then, we obtain $\tau_e$ from the OLS fit of $Y$ on $\tilde{Z}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;proof-of-theorem-141&#34;&gt;Proof of Theorem 14.1&lt;/h3&gt;
&lt;p&gt;The coefficient of $e(X)$ in the OLS fit of $Z$ on $\{1, e(X)\}$ is
$$
\begin{aligned}
\frac{\operatorname{cov}\{Z, e(X)\}}{\operatorname{var}\{e(X)\}} &amp;amp; =\frac{E[\operatorname{cov}\{Z, e(X) \mid X\}]+\operatorname{cov}\{E(Z \mid X), e(X)\}}{\operatorname{var}\{e(X)\}} \\ &amp;amp;=\frac{0+\operatorname{var}\{e(X)\}}{\operatorname{var}\{e(X)\}}=1,
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the intercept is $E(Z)-E\{e(X)\}=0$&lt;/li&gt;
&lt;li&gt;the residual is $\tilde{Z}=Z-e(X)$ (This makes sense since $Z-e(X)$ is uncorrelated with any function of $X$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, we can obtain $\tau_e$ from the univariate OLS fit of $Y$ on $Z-e(X)$ :
$$\small{\tau_e=\frac{\operatorname{cov}\{Z-e(X), Y\}}{\operatorname{var}\{Z-e(X)\}}}$$
The denominator simplifies to
$$
\begin{aligned}
\operatorname{var}\{Z-e(X)\} &amp;amp; =E\{Z-e(X)\}^2 =e(X)+e(X)^2-2 e(X)^2=h_{\mathrm{O}}(X)
\end{aligned}
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;proof-of-theorem-141-1&#34;&gt;Proof of Theorem 14.1&lt;/h3&gt;
&lt;p&gt;The numerator simplifies to
$$
\begin{aligned}
&amp;amp; \operatorname{cov}\{Z-e(X), Y\} \\
= &amp;amp; E[\{Z-e(X)\} Y] \\
= &amp;amp; E[\{Z-e(X)\} Z Y(1)]+E[\{Z-e(X)\}(1-Z) Y(0)] \\
&amp;amp; \quad \quad \quad{\color{red}(\text { since } Y=Z Y(1)+(1-Z) Y(0))} \\
= &amp;amp; E[\{Z-Z e(X)\} Y(1)]-E[e(X)(1-Z) Y(0)] \\
= &amp;amp; E[Z\{1-e(X)\} Y(1)]-E[e(X)(1-Z) Y(0)] \\
= &amp;amp; E\left[e(X)\{1-e(X)\} \mu_1(X)\right]-E\left[e(X)\{1-e(X)\} \mu_0(X)\right] \\
&amp;amp; \quad \quad \quad\text { {\color{red}(tower property and ignorability)} } \\
= &amp;amp; E\left\{h_{\mathrm{O}}(X) \tau(X)\right\} .
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From the proof of Theorem 14.1, we can simply run the OLS of $Y$ on the centered treatment $\tilde{Z} = Z - e(X)$.&lt;/li&gt;
&lt;li&gt;Moreover, we can also include $X$ in the OLS fit which may improve efficiency in finite sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;comments-of-theorem-141-and-corollary-141&#34;&gt;Comments of Theorem 14.1 and Corollary 14.1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Theorem 14.1 motivates a two-step estimator for $\tau_{\mathrm{O}}$:
&lt;ul&gt;
&lt;li&gt;first, fit a propensity score model to obtain $\hat{e}\left(X_i\right)$;&lt;/li&gt;
&lt;li&gt;second, run OLS of $Y_i$ on $\left(1, X_i, \hat{e}\left(X_i\right)\right)$ to obtain the coefficient of $Z_i$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Corollary 14.1 motivates another two-step estimator for $\tau_{\mathrm{O}}$:
&lt;ul&gt;
&lt;li&gt;first, fit a propensity score model to obtain $\hat{e}\left(X_i\right)$;&lt;/li&gt;
&lt;li&gt;second, run OLS of $Y_i$ on $Z_i-\hat{e}\left(X_i\right)$ to obtain the coefficient of $Z_i$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;: OLS is convenient for obtaining point estimators, the corresponding standard errors are incorrect due to &lt;mark&gt;the uncertainty in the first step estimation of the propensity score&lt;/mark&gt;. We can use the bootstrap to approximate the standard errors.&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h4 id=&#34;regressions-weighted-by-the-inverse-of-the-propensity-score&#34;&gt;Regressions weighted by the inverse of the propensity score&lt;/h4&gt;
&lt;p&gt;We first re-examine the Hajek estimator of $\tau$ :
$$
\hat{\tau}^{\text {hajek }}=\frac{\sum_{i=1}^n \frac{Z_i Y_i}{\hat{e}\left(X_i\right)}}{\sum_{i=1}^n \frac{Z_i}{\hat{e}\left(X_i\right)}}-\frac{\sum_{i=1}^n \frac{\left(1-Z_i\right) Y_i}{1-\hat{e}\left(X_i\right)}}{\sum_{i=1}^n \frac{1-Z_i}{1-\hat{e}\left(X_i\right)}},
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;which equals the difference between the weighted means of the outcomes in the treatment and control groups.&lt;/li&gt;
&lt;li&gt;Numerically, it is identical to the coefficient of $Z_i$ in the following weighted least squares (WLS) of $Y_i$ on $\left(1, Z_i\right)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;mark&gt;Proposition 14.1&lt;/mark&gt; $\hat{\tau}^{\text {hajek }}$ equals $\hat{\beta}$ from the following $WLS$ :&lt;/p&gt;
&lt;p&gt;$$
(\hat{\alpha}, \hat{\beta})=\arg \min_{\alpha, \beta} \sum_{i=1}^n w_i\left(Y_i-\alpha-\beta Z_i\right)^2
$$&lt;/p&gt;
&lt;p&gt;with weights
$$
w_i=\frac{Z_i}{\hat{e}\left(X_i\right)}+\frac{1-Z_i}{1-\hat{e}\left(X_i\right)}= \begin{cases}\frac{1}{\hat{e}\left(X_i\right)} &amp;amp; \text { if } Z_i=1 \\ \frac{1}{1-\hat{e}\left(X_i\right)} &amp;amp; \text { if } Z_i=0\end{cases}
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;average-causal-effect&#34;&gt;Average causal effect&lt;/h3&gt;
&lt;ul style=&#34;color: black&#34;&gt;
&lt;li&gt;By Proposition 14.1, it is convenient to obtain $\hat{\tau}^{\text {hajek }}$ based on WLS.&lt;/li&gt;
&lt;li&gt;However, due to the uncertainty in the estimated propensity score, the standard error &lt;mark&gt;reported by WLS is incorrect&lt;/mark&gt; for the true standard error $\Rightarrow$ &lt;strong&gt;bootstrap&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul style=&#34;color: red&#34;&gt;
&lt;li&gt;Why does the WLS give a consistent estimator for $\tau$ ?&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Recall that in the CRE with a constant propensity score, we can simply use the coefficient of $Z_i$ in the OLS fit of $Y_i$ on $(1, Z_i)$ to estimate $\tau$.&lt;/li&gt;
&lt;li&gt;In observational studies, units have different probabilities of receiving the treatment and control, respectively.&lt;/li&gt;
&lt;li&gt;If we weight the treated units by $1 / e(X_i)$ and the control units by $1 /\{1-e(X_i)\}$, then they can represent the whole population and we effectively have &lt;strong&gt;a pseudo randomized experiment&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Consequently, the difference between the weighted means are consistent for $\tau$.&lt;/li&gt;
&lt;li&gt;The numerical equivalence of $\hat{\tau}^{\text {hajek }}$ and WLS is not only a fun numerical fact itself but also useful for motivation more complex estimator with covariate adjustment.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;an-extension-to-a-more-complex-estimator&#34;&gt;An extension to a more complex estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the CRE, we can use the coefficient of $Z_i$ in the OLS fit of $Y_i$ on $(1, Z_i, X_i, Z_i X_i)$ to estimate $\tau$, where the covariates are centered with $\bar{X}=0$.&lt;/li&gt;
&lt;li&gt;This is Lin (2013)&amp;rsquo;s estimator which uses covariates to improve efficiency.&lt;/li&gt;
&lt;li&gt;A natural extension to observational studies is to estimate $\tau$ using the coefficient of $Z_i$ in the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ with weights defined in (14.1).&lt;/li&gt;
&lt;li&gt;If the linear models
$$
E(Y \mid Z=1, X)=\beta_{10}+\beta_{1 x}^{\top} X, \quad E(Y \mid Z=0, X)=\beta_{00}+\beta_{0 x}^{\top} X,
$$
are correctly specified, then &lt;mark&gt;both OLS and WLS give consistent estimators for the coefficients&lt;/mark&gt; and the estimators of the coefficient of $Z$ is consistent for $\tau$.&lt;/li&gt;
&lt;li&gt;More interestingly, the estimator of the coefficient of $Z$ based on WLS is also consistent for $\tau$ if &lt;strong&gt;the propensity score model is correct and the outcome model is incorrect&lt;/strong&gt;. $\Rightarrow$ the estimator based on WLS is &lt;strong&gt;doubly robust&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;a-doubly-robust-estimator&#34;&gt;A doubly robust estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Let $\hat{e}(X_i)$ be the fitted propensity score and $(\mu_1(X_i, \hat{\beta}_1), \mu_0(X_i, \hat{\beta}_0))$ be the fitted values of the outcome means based on the WLS.&lt;/li&gt;
&lt;li&gt;The outcome regression estimator is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{wls}}^{\mathrm{reg}}= \frac{1}{n}\sum_{i=1}^n\mu_1\left(X_i, \hat{\beta}_1\right)-\frac{1}{n} \sum_{i=1}^n \mu_0\left(X_i, \hat{\beta}_0\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The doubly robust estimator for $\tau$ is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{wls}}^{\mathrm{reg}}+\frac{1}{n} \sum_{i=1}^n \frac{Z_i\left\{Y_i-\mu_1\left(X_i, \hat{\beta}_1\right)\right\}}{\hat{e}\left(X_i\right)}-\frac{1}{n} \sum_{i=1}^n \frac{\left(1-Z_i\right)\left\{Y_i-\mu_0\left(X_i, \hat{\beta}_0\right)\right\}}{1-\hat{e}\left(X_i\right)} .
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An interesting result is that this doubly robust estimator equals the outcome regression estimator, which reduces to the coefficient of $Z_i$ in the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ if we use weights (14.1).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;theorem-142&#34;&gt;Theorem 14.2&lt;/h3&gt;
&lt;p style=&#34;color: blue&#34;&gt;If $\bar{X}=0$ and $(\mu_1(X_i, \hat{\beta}_1), \mu_0(X_i, \hat{\beta}_0))=(\hat{\beta}_{10}+\hat{\beta}_{1 x}^{\top} X_i, \hat{\beta}_{00}+\hat{\beta}_{0 x}^{\top} X_i)$ based on the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ with weights (14.1), then&lt;/p&gt;
&lt;p style=&#34;color: blue&#34;&gt;$$
\hat{\tau}_{\mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{wls}}^{\mathrm{reg}}=\hat{\beta}_{10}-\hat{\beta}_{00},
$$&lt;/p&gt;
&lt;p style=&#34;color: blue&#34;&gt;which is the coefficient of $Z_i$ in the WLS fit.&lt;/p&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Freedman and Berk (2008) showed that when the outcome model is correct, the WLS estimator is worse than the OLS estimator&lt;/li&gt;
&lt;li&gt;When the errors have variance proportional to the inverse of the propensity scores, the WLS estimator will be more effcient than the OLS estimator.&lt;/li&gt;
&lt;li&gt;The estimated standard error based on the WLS fit is not consistent for the true standard error because it &lt;strong&gt;ignores the uncertainty in the estimated propensity score&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;This can be easily fixed by using the bootstrap to approximate the variance of the WLS estimator.&lt;/li&gt;
&lt;li&gt;Nevertheless, they found that &amp;ldquo;weighting may help under some circumstances&amp;rdquo; because when the outcome model is incorrect, the
WLS estimator is still consistent if the propensity score model is correct.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;average-causal-effect-on-the-treated-units&#34;&gt;Average causal effect on the treated units&lt;/h3&gt;
&lt;p&gt;&lt;mark&gt;Proposition 14.2&lt;/mark&gt; $\hat{\tau}_{\mathrm{T}}^{\text {hajek }}$ is numerically identical to $\hat{\beta}$ in the following WLS:
$$
(\hat{\alpha}, \hat{\beta})=\arg \min_{\alpha, \beta} \sum_{i=1}^n w_{\mathrm{T} i}\left(Y_i-\alpha-\beta Z_i\right)^2
$$
with weights
$$
w_{\mathrm{T} i}=Z_i+\left(1-Z_i\right) \hat{o}\left(X_i\right)= \begin{cases}1 &amp;amp; \text { if } Z_i=1 \\ \hat{o}\left(X_i\right) &amp;amp; \text { if } Z_i=0\end{cases}
$$
where $\hat{o}\left(X_i\right)=\hat{e}\left(X_i\right) /\{1-\hat{e}\left(X_i\right)\}$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regression-estimators&#34;&gt;Regression estimators&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;CRE&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;unconfounded observational studies&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;without $X$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim Z_i$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim Z_i$ with weights $w_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;with $X$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim\left(Z_i, X_i, Z_i X_i\right)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim\left(Z_i, X_i, Z_i X_i\right)$ with weights $w_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;average-causal-effect-on-the-treated-units-1&#34;&gt;Average causal effect on the treated units&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If we center covariates with $\hat{X}(1)=0$, then we can estimate $\tau_{\mathrm{T}}$ using the coefficient of $Z_i$ in the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ with weights defined in (14.2). Similarly, this estimator equals the regression estimator&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{reg}}=\hat{\bar{Y}}(1)-\frac{1}{n_1} \sum_{i=1}^n Z_i \mu_0\left(X_i, \hat{\beta}_0\right)
$$&lt;/p&gt;
&lt;p&gt;which also equals the doubly robust estimator&lt;/p&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{reg}}-\frac{1}{n_1} \sum_{i=1}^n \hat{o}\left(X_i\right)\left(1-Z_i\right)\left\{Y_i-\mu_0\left(X_i, \hat{\beta}_0\right)\right\}.
$$&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Theorem 14.3&lt;/mark&gt; If $\hat{\bar{X}}(1)=0$ and $\mu_0(X_i, \hat{\beta}_0)=\hat{\beta}_{00}+\hat{\beta}_{0x}^{\top} X_i$ based on the WLS fit of $Y_i$ on $(1, Z_i, X_i, Z_i X_i)$ with weights (14.2), then&lt;/p&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{reg}}=\hat{\beta}_{10}-\hat{\beta}_{00},
$$&lt;/p&gt;
&lt;p&gt;which is the coefficient of $Z_i$ in the $W L S$ fit.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
