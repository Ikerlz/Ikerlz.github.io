<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Slides | Zhe Li</title>
    <link>https://ikerlz.github.io/slides/</link>
      <atom:link href="https://ikerlz.github.io/slides/index.xml" rel="self" type="application/rss+xml" />
    <description>Slides</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 27 Nov 2023 00:00:00 +0000</lastBuildDate>
    <item>
      <title>On factor models with random missing： EM estimation, inference, and cross validation</title>
      <link>https://ikerlz.github.io/slides/factormissing/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://ikerlz.github.io/slides/factormissing/</guid>
      <description>
&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;ZIBIN.jpg&#34;
  &gt;

&lt;h1 id=&#34;on-factor-models-with-random-missing-em-estimation-inference-and-cross-validation&#34;&gt;On factor models with random missing: EM estimation, inference, and cross validation&lt;/h1&gt;
&lt;p&gt;$$
\begin{aligned}
\
\end{aligned}
$$&lt;/p&gt;
&lt;center&gt; Li Zhe &lt;center&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;School of Data Science, Fudan University &lt;center&gt; 
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;November 30, 2023&lt;center&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;outline&#34;&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Background: &lt;strong&gt;factor model&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Motivation&lt;/li&gt;
&lt;li&gt;Factor models with random missing:
&lt;ul&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt;EM estimator&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt;Asymptotic properties&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;Determining the number of factors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simulation&lt;/li&gt;
&lt;li&gt;Empirical application&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;background-factor-model&#34;&gt;Background: factor model&lt;/h2&gt;
&lt;p&gt;$$
\begin{aligned}
x_{i t} &amp;amp; =\lambda_i f_t+e_{i t} \\
\boldsymbol{X} &amp;amp;= \boldsymbol{\Lambda} \boldsymbol{F}^\top + \boldsymbol{E}
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{X}=(\boldsymbol{x}_{\cdot 1},\ldots,\boldsymbol{x}_{\cdot T})\in\mathbb{R}^{N\times T}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{\Lambda}\in\mathbb{R}^{N\times R}$: factor loadings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{F}\in\mathbb{R}^{T\times R}$: common factors (latent, unobserved)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{E}\in\mathbb{R}^{N\times T}$: idiosyncratic (or error) component&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;${e_{it}}$ can exhibit both cross-sectional and temporal dependence.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given the factor number $k$, we can estimate the factors and factor loadings by
$$
\Big\{\widehat{\boldsymbol{\Lambda}}^k, \widehat{\boldsymbol{F}}^k\Big\}=\arg\min_{\boldsymbol{\Lambda}^k,\boldsymbol{F}^k}\frac{1}{NT}\Big\|\boldsymbol{X}- \boldsymbol{\Lambda}^k{\boldsymbol{F}^k}^\top\Big\|_F^2
$$
where $\boldsymbol{\Lambda}^k\in\mathbb{R}^{N\times k}$ and $\boldsymbol{F}^k\in\mathbb{R}^{T\times k}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Factor model in balanced panel has been thoroughly investigated.
$$$$&lt;/li&gt;
&lt;li&gt;How to handle the missing data problem in factor models?
$$$$
&lt;ul&gt;
&lt;li&gt;the expectation–maximization (EM) algorithm
$$$$&lt;/li&gt;
&lt;li&gt;the Kalman filter (KF)
$$$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;There is no formal study of the &lt;strong&gt;asymptotic properties&lt;/strong&gt; for the EM estimators of the factors and factor loadings for the PC estimation with &lt;mark&gt;missing observations&lt;/mark&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;notations&#34;&gt;Notations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;consider the factor model
$$
\boldsymbol{X} =  \boldsymbol{F}\boldsymbol{\Lambda}^\top + \varepsilon
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol{X}=(X_1,\ldots,X_N)$ where $X_i \equiv\left(X_{i 1}, \ldots, X_{i T}\right)^{\prime}$ and $X_{it}$ are &lt;font color=red&gt; &lt;strong&gt;missing at random&lt;/strong&gt; &lt;/font&gt;&lt;/li&gt;
&lt;li&gt;$\varepsilon=\left(\varepsilon_1, \ldots, \varepsilon_N\right)$ and $\varepsilon_i \equiv\left(\varepsilon_{i 1}, \ldots, \varepsilon_{i T}\right)^{\prime}$ for $i=1, \ldots, N$.&lt;/li&gt;
&lt;li&gt;$F=\left(F_1, \ldots, F_T\right)^{\prime}$ and $\Lambda=\left(\lambda_1, \ldots, \lambda_N\right)^{\prime}$ where $F_t$ and $\lambda_i$ are $R \times 1$ vectors of factors and factor loadings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$F^0=\left(F_1^0, \ldots, F_T^0\right)^{\prime}$ and $\Lambda^0=\left(\lambda_1^0, \ldots, \lambda_N^0\right)^{\prime}$ are the true values of $F$ and $\Lambda$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Omega \subset[N] \times[T]$ be the index set of the observations that are observed. That is,
$$
\Omega=\Big\{(i, t) \in[N] \times[T]: X_{i t} \text { is observed }\Big\}.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let $G$ denote a $T \times N$ matrix with $(t, i)$ th element given by $g_{i t}=\mathbf{1}\{(i, t) \in \Omega\}$ and is &lt;mark&gt;independent of $X, F^0, \Lambda^0$ and $\varepsilon$&lt;/mark&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;the-initial-estimates&#34;&gt;The initial estimates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Let $\tilde{X}=X \circ G$ and $\tilde{X}_{i t}=X_{i t} g_{i t}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The common component $C^0 \equiv F^0 \Lambda^0$ is &lt;mark&gt;a low rank matrix&lt;/mark&gt; $\Rightarrow$ it is possible to recover $C^0$ even when a large proportion of elements in $X$ are missing at random.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Under the standard condition that $E\left(\varepsilon_{i t} \mid F_t^0, \lambda_i^0\right)=0$, we can verify that $E\left(\frac{1}{q} \tilde{X} \mid F^0, \Lambda^0\right)=F^0 \Lambda^{0 \prime}$ $\Rightarrow$ consider the following least squares objective function
$$
\mathcal{L}_{N T}^0(F, \Lambda) \equiv \frac{1}{N T} \operatorname{tr}\left[\left(\frac{1}{\tilde{q}} \tilde{X}-F \Lambda^{\prime}\right)\left(\frac{1}{\tilde{q}} \tilde{X}-F \Lambda^{\prime}\right)^{\prime}\right]
$$
&lt;strong&gt;identification restrictions&lt;/strong&gt;: $F^{\prime} F / T=I_R$ and $\Lambda^{\prime} \Lambda$ is a diagonal matrix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;By concentrating out $\Lambda$ and using the normalization that $F^{\prime} F / T=I_R$ $\Rightarrow$ identical to maximizing $\tilde{q}^{-2} \operatorname{tr}\Big\{F^{\prime} \tilde{X} \tilde{X}^{\prime} F\Big\}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;the-initial-estimates-1&#34;&gt;The initial estimates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The estimated factor matrix, denoted by $\hat{F}^{(0)}$ is $\sqrt{T}$ times the eigenvectors corresponding to the $R$ largest eigenvalues of the $T \times T$ matrix $\frac{1}{N T \tilde{q}^2} \tilde{X} \tilde{X}^{\prime}:$
$$
\frac{1}{N T \tilde{q}^2} \tilde{X} \tilde{X}^{\prime} \hat{F}^{(0)}=\hat{F}^{(0)} \hat{D}^{(0)},
$$
&lt;ul&gt;
&lt;li&gt;$\hat{D}^{(0)}$ is an $R \times R$ diagonal matrix consisting of the $R$ largest eigenvalues of $\left(N T \tilde{q}^2\right)^{-1} \tilde{X} \tilde{X}^{\prime}$, arranged in descending order along its diagonal line.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The estimator of $\Lambda^{\prime}$ is given by
$$
\hat{\Lambda}^{(0) \prime}=\frac{1}{\tilde{q}}\left(\hat{F}^{(0) \prime} \hat{F}^{(0)}\right)^{-1} \hat{F}^{(0) \prime} \tilde{X}=\frac{1}{T \tilde{q}} \hat{F}^{(0) \prime} \tilde{X} .
$$&lt;/li&gt;
&lt;li&gt;We can obtain an initial estimate of the $(t, i)$ th element, $C_{i t}^0$, of $C^0$ by $\hat{C}_{i t}^{(0)}=\hat{\lambda}_i^{(0)\prime} \hat{F}_t^{(0)}$.&lt;/li&gt;
&lt;li&gt;The initial estimators $\hat{F}_t^{(0)}, \hat{\lambda}_i^{(0)}$ and $\hat{C}_{i t}^{(0)}$ are consistent and follow mixture normal distributions under some standard conditions.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;the-iterated-estimates&#34;&gt;The iterated estimates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The initial estimators: consistency but &lt;mark&gt;not asymptotically efficient&lt;/mark&gt; $\Rightarrow$ &lt;strong&gt;iterative estimators&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;In step $\ell$, we can &lt;mark&gt; replace the missing values $\left(X_{i t}\right)$ in the matrix $X$ with the estimated common components $\hat{C}_{i t}^{(\ell-1)}$&lt;/mark&gt;. Define the $T \times N$ matrix $\hat{X}^{(\ell)}$ with its $(t, i)$ th element given by
$$
\hat{X}_{i t}^{(\ell)}=\left\{\begin{array}{ll}
X_{i t} &amp;amp; \text { if }(i, t) \in \Omega \\
\hat{C}_{i t}^{(\ell-1)} &amp;amp; \text { if }(i, t) \in \Omega_{\perp}
\end{array}, \ell \geq 1,\right.
$$
where $\Omega_{\perp}=\{(i, t) \in[N] \times[T]:(i, t) \notin \Omega\}$.&lt;/li&gt;
&lt;li&gt;Then we can conduct the PC analysis based on $\hat{X}^{(\ell)}$ and obtain $\hat{F}^{(\ell) \prime}$ and $\hat{\Lambda}^{(\ell)}$.&lt;/li&gt;
&lt;li&gt;We will study the asymptotic properties of $\hat{F}_t^{(\ell)}, \hat{\lambda}_i^{(\ell)}$ and $\hat{C}_{i t}^{\left(\ell^{\ell}\right)}, \ell=1,2, \ldots$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
~\\\
~\\\
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using the Propensity Score in Regressions for Causal Effects</title>
      <link>https://ikerlz.github.io/slides/causalinferencechapter14/</link>
      <pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://ikerlz.github.io/slides/causalinferencechapter14/</guid>
      <description>
&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;ZIBIN.jpg&#34;
  &gt;

&lt;h1 id=&#34;using-the-propensity-score-in-regressions-for-causal-effects&#34;&gt;Using the Propensity Score in Regressions for Causal Effects&lt;/h1&gt;
&lt;p&gt;$$
\begin{aligned}
\
\end{aligned}
$$&lt;/p&gt;
&lt;center&gt; Li Zhe &lt;center&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;School of Data Science, Fudan University &lt;center&gt; 
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;November 1, 2023&lt;center&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This chapter discusses two simple methods to use the propensity score:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the propensity score as a covariate in regressions&lt;/li&gt;
&lt;li&gt;running regressions weighted by the inverse of the propensity score&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reasons&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;they are easy to implement, which involve only standard statistical software packages for regressions;&lt;/li&gt;
&lt;li&gt;their properties are comparable to many more complex methods;&lt;/li&gt;
&lt;li&gt;they can be easily extended to allow for flexible statistical models including machine learning algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;outline&#34;&gt;Outline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Regressions with the propensity score as a covariate
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Theorem 14.1&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Proposition 14.1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regressions weighted by the inverse of the propensity score
&lt;ul&gt;
&lt;li&gt;Average causal effect
&lt;ul&gt;
&lt;li&gt;Theorem 14.2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Average causal effect on the treated units
&lt;ul&gt;
&lt;li&gt;Table 14.1&lt;/li&gt;
&lt;li&gt;Proposition 14.2&lt;/li&gt;
&lt;li&gt;Theorem 14.3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regressions-with-the-propensity-score-as-a-covariate&#34;&gt;Regressions with the propensity score as a covariate&lt;/h3&gt;
&lt;p&gt;$$
\text { Theorem 11.1 If } Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid X \text {, then } {\color{red}Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid e(X)} \text {. }
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By Theorem 11.1, if unconfoundedness holds conditioning on $X$, then it also holds conditioning on $e(X)$: $\color{red}{Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid e(X) }.$&lt;/li&gt;
&lt;li&gt;Analogous to (10.5), $\tau$ is also &lt;mark&gt;nonparametrically&lt;/mark&gt; identified by
$$
\tau=E[E\{Y \mid Z=1, e(X)\}-E\{Y \mid Z=0, e(X)\}],
$$&lt;/li&gt;
&lt;li&gt;$\Rightarrow$ The simplest regression specification is the OLS fit of $Y$ on $\{1, Z, e(X)\}$, with the coefficient of $Z$ as an estimator, denoted by $\tau_e$:
$$
\arg \min _{a, b, c} E\{Y-a-b Z-c e(X)\}^2
$$&lt;/li&gt;
&lt;li&gt;$\tau_e$ defined as the coefficient of $Z$.&lt;/li&gt;
&lt;li&gt;It is consistent for $\tau$ if
&lt;ul&gt;
&lt;li&gt;have a correct propensity score model&lt;/li&gt;
&lt;li&gt;the outcome model is indeed linear in $Z$ and $e(X)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\tau_e$ estimates $\tau_{\mathrm{O}}$ if we have a correct propensity score model even if the outcome model is &lt;mark&gt;completely misspecified&lt;/mark&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regressions-with-the-propensity-score-as-a-covariate-1&#34;&gt;Regressions with the propensity score as a covariate&lt;/h3&gt;
&lt;p&gt;&lt;mark&gt;Theorem 14.1&lt;/mark&gt; If $Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid X$, then the coefficient of $Z$ in the OLS fit of $Y$ on $\{1, Z, e(X)\}$ equals
$$
\tau_e=\tau_{\mathrm{O}}=\frac{E\left\{h_{\mathrm{O}}(X) \tau(X)\right\}}{E\left\{h_{\mathrm{O}}(X)\right\}},
$$
recalling that $h_{\mathrm{O}}(X)=e(X)\{1-e(X)\}$ and $\tau(X)=E\{Y(1)-Y(0) \mid X\}$.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\
\
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Corollary 14.1&lt;/mark&gt; If $Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid X$, then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the coefficient of $Z-e(X)$ in the OLS fit of $Y$ on $Z-e(X)$ or $\{1, Z-e(X)\}$ equals $\tau_{\mathrm{O}}$;&lt;/li&gt;
&lt;li&gt;the coefficient of $Z$ in the OLS fit of $Y$ on $\{1, Z, e(X), X\}$ equals $\tau_{\mathrm{O}}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regressions-with-the-propensity-score-as-a-covariate-2&#34;&gt;Regressions with the propensity score as a covariate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An unusual feature of Theorem 14.1 is that &lt;strong&gt;the overlap condition&lt;/strong&gt; ($0 &amp;lt; e(x) &amp;lt; 1$) is not needed any more.&lt;/li&gt;
&lt;li&gt;Even if some units have propensity score $e(X)$ equaling 0 or 1, their associate weight $e(X)\{1-e(X)\}$ is zero so that they do not contribute anything to the final parameter $\tau_O$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;frischwaughlovell-theorem&#34;&gt;Frisch–Waugh–Lovell Theorem&lt;/h2&gt;
&lt;p&gt;The Frisch–Waugh–Lovell (FWL) theorem reduces multivariate OLS to univariate OLS and therefore facilitate the understanding and calculation of the OLS coefficients.&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Theorem A2.2 (sample FWL)&lt;/mark&gt; With data $\left(Y, X_1, X_2, \ldots, X_p\right)$ containing column vectors, the coefficient of $X_1$ equals the coefficient of $\tilde{X}_1$ in the OLS fit of $Y$ or $\tilde{Y}$ on $\tilde{X}_1$, where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\tilde{Y}$ is the residual vector from the OLS fit of $Y$ on $\left(X_2, \ldots, X_p\right)$&lt;/li&gt;
&lt;li&gt;$\tilde{X}_1$ is the residual from the OLS fit of $X_1$ on $\left(X_2, \ldots, X_p\right)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;p&gt;Based on the FWL theorem, we can obtain $\tau_e$ in two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, we obtain the residual $\tilde{Z}$ from the OLS fit of $Z$ on ${1, e(X)}$;&lt;/li&gt;
&lt;li&gt;then, we obtain $\tau_e$ from the OLS fit of $Y$ on $\tilde{Z}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;proof-of-theorem-141&#34;&gt;Proof of Theorem 14.1&lt;/h3&gt;
&lt;p&gt;The coefficient of $e(X)$ in the OLS fit of $Z$ on $\{1, e(X)\}$ is
$$
\begin{aligned}
\frac{\operatorname{cov}\{Z, e(X)\}}{\operatorname{var}\{e(X)\}} &amp;amp; =\frac{E[\operatorname{cov}\{Z, e(X) \mid X\}]+\operatorname{cov}\{E(Z \mid X), e(X)\}}{\operatorname{var}\{e(X)\}} \\ &amp;amp;=\frac{0+\operatorname{var}\{e(X)\}}{\operatorname{var}\{e(X)\}}=1,
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the intercept is $E(Z)-E\{e(X)\}=0$&lt;/li&gt;
&lt;li&gt;the residual is $\tilde{Z}=Z-e(X)$ (This makes sense since $Z-e(X)$ is uncorrelated with any function of $X$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, we can obtain $\tau_e$ from the univariate OLS fit of $Y$ on $Z-e(X)$ :
$$\small{\tau_e=\frac{\operatorname{cov}\{Z-e(X), Y\}}{\operatorname{var}\{Z-e(X)\}}}$$
The denominator simplifies to
$$
\begin{aligned}
\operatorname{var}\{Z-e(X)\} &amp;amp; =E\{Z-e(X)\}^2 =e(X)+e(X)^2-2 e(X)^2=h_{\mathrm{O}}(X)
\end{aligned}
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;proof-of-theorem-141-1&#34;&gt;Proof of Theorem 14.1&lt;/h3&gt;
&lt;p&gt;The numerator simplifies to
$$
\begin{aligned}
&amp;amp; \operatorname{cov}\{Z-e(X), Y\} \\
= &amp;amp; E[\{Z-e(X)\} Y] \\
= &amp;amp; E[\{Z-e(X)\} Z Y(1)]+E[\{Z-e(X)\}(1-Z) Y(0)] \\
&amp;amp; \quad \quad \quad{\color{red}(\text { since } Y=Z Y(1)+(1-Z) Y(0))} \\
= &amp;amp; E[\{Z-Z e(X)\} Y(1)]-E[e(X)(1-Z) Y(0)] \\
= &amp;amp; E[Z\{1-e(X)\} Y(1)]-E[e(X)(1-Z) Y(0)] \\
= &amp;amp; E\left[e(X)\{1-e(X)\} \mu_1(X)\right]-E\left[e(X)\{1-e(X)\} \mu_0(X)\right] \\
&amp;amp; \quad \quad \quad\text { {\color{red}(tower property and ignorability)} } \\
= &amp;amp; E\left\{h_{\mathrm{O}}(X) \tau(X)\right\} .
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From the proof of Theorem 14.1, we can simply run the OLS of $Y$ on the centered treatment $\tilde{Z} = Z - e(X)$.&lt;/li&gt;
&lt;li&gt;Moreover, we can also include $X$ in the OLS fit which may improve efficiency in finite sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;comments-of-theorem-141-and-corollary-141&#34;&gt;Comments of Theorem 14.1 and Corollary 14.1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Theorem 14.1 motivates a two-step estimator for $\tau_{\mathrm{O}}$:
&lt;ul&gt;
&lt;li&gt;first, fit a propensity score model to obtain $\hat{e}\left(X_i\right)$;&lt;/li&gt;
&lt;li&gt;second, run OLS of $Y_i$ on $\left(1, X_i, \hat{e}\left(X_i\right)\right)$ to obtain the coefficient of $Z_i$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Corollary 14.1 motivates another two-step estimator for $\tau_{\mathrm{O}}$:
&lt;ul&gt;
&lt;li&gt;first, fit a propensity score model to obtain $\hat{e}\left(X_i\right)$;&lt;/li&gt;
&lt;li&gt;second, run OLS of $Y_i$ on $Z_i-\hat{e}\left(X_i\right)$ to obtain the coefficient of $Z_i$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;: OLS is convenient for obtaining point estimators, the corresponding standard errors are incorrect due to &lt;mark&gt;the uncertainty in the first step estimation of the propensity score&lt;/mark&gt;. We can use the bootstrap to approximate the standard errors.&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h4 id=&#34;regressions-weighted-by-the-inverse-of-the-propensity-score&#34;&gt;Regressions weighted by the inverse of the propensity score&lt;/h4&gt;
&lt;p&gt;We first re-examine the Hajek estimator of $\tau$ :
$$
\hat{\tau}^{\text {hajek }}=\frac{\sum_{i=1}^n \frac{Z_i Y_i}{\hat{e}\left(X_i\right)}}{\sum_{i=1}^n \frac{Z_i}{\hat{e}\left(X_i\right)}}-\frac{\sum_{i=1}^n \frac{\left(1-Z_i\right) Y_i}{1-\hat{e}\left(X_i\right)}}{\sum_{i=1}^n \frac{1-Z_i}{1-\hat{e}\left(X_i\right)}},
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;which equals the difference between the weighted means of the outcomes in the treatment and control groups.&lt;/li&gt;
&lt;li&gt;Numerically, it is identical to the coefficient of $Z_i$ in the following weighted least squares (WLS) of $Y_i$ on $\left(1, Z_i\right)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;mark&gt;Proposition 14.1&lt;/mark&gt; $\hat{\tau}^{\text {hajek }}$ equals $\hat{\beta}$ from the following $WLS$ :&lt;/p&gt;
&lt;p&gt;$$
(\hat{\alpha}, \hat{\beta})=\arg \min_{\alpha, \beta} \sum_{i=1}^n w_i\left(Y_i-\alpha-\beta Z_i\right)^2
$$&lt;/p&gt;
&lt;p&gt;with weights
$$
w_i=\frac{Z_i}{\hat{e}\left(X_i\right)}+\frac{1-Z_i}{1-\hat{e}\left(X_i\right)}= \begin{cases}\frac{1}{\hat{e}\left(X_i\right)} &amp;amp; \text { if } Z_i=1 \\ \frac{1}{1-\hat{e}\left(X_i\right)} &amp;amp; \text { if } Z_i=0\end{cases}
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;average-causal-effect&#34;&gt;Average causal effect&lt;/h3&gt;
&lt;ul style=&#34;color: black&#34;&gt;
&lt;li&gt;By Proposition 14.1, it is convenient to obtain $\hat{\tau}^{\text {hajek }}$ based on WLS.&lt;/li&gt;
&lt;li&gt;However, due to the uncertainty in the estimated propensity score, the standard error &lt;mark&gt;reported by WLS is incorrect&lt;/mark&gt; for the true standard error $\Rightarrow$ &lt;strong&gt;bootstrap&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul style=&#34;color: red&#34;&gt;
&lt;li&gt;Why does the WLS give a consistent estimator for $\tau$ ?&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Recall that in the CRE with a constant propensity score, we can simply use the coefficient of $Z_i$ in the OLS fit of $Y_i$ on $(1, Z_i)$ to estimate $\tau$.&lt;/li&gt;
&lt;li&gt;In observational studies, units have different probabilities of receiving the treatment and control, respectively.&lt;/li&gt;
&lt;li&gt;If we weight the treated units by $1 / e(X_i)$ and the control units by $1 /\{1-e(X_i)\}$, then they can represent the whole population and we effectively have &lt;strong&gt;a pseudo randomized experiment&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Consequently, the difference between the weighted means are consistent for $\tau$.&lt;/li&gt;
&lt;li&gt;The numerical equivalence of $\hat{\tau}^{\text {hajek }}$ and WLS is not only a fun numerical fact itself but also useful for motivation more complex estimator with covariate adjustment.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;an-extension-to-a-more-complex-estimator&#34;&gt;An extension to a more complex estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the CRE, we can use the coefficient of $Z_i$ in the OLS fit of $Y_i$ on $(1, Z_i, X_i, Z_i X_i)$ to estimate $\tau$, where the covariates are centered with $\bar{X}=0$.&lt;/li&gt;
&lt;li&gt;This is Lin (2013)&amp;rsquo;s estimator which uses covariates to improve efficiency.&lt;/li&gt;
&lt;li&gt;A natural extension to observational studies is to estimate $\tau$ using the coefficient of $Z_i$ in the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ with weights defined in (14.1).&lt;/li&gt;
&lt;li&gt;If the linear models
$$
E(Y \mid Z=1, X)=\beta_{10}+\beta_{1 x}^{\top} X, \quad E(Y \mid Z=0, X)=\beta_{00}+\beta_{0 x}^{\top} X,
$$
are correctly specified, then &lt;mark&gt;both OLS and WLS give consistent estimators for the coefficients&lt;/mark&gt; and the estimators of the coefficient of $Z$ is consistent for $\tau$.&lt;/li&gt;
&lt;li&gt;More interestingly, the estimator of the coefficient of $Z$ based on WLS is also consistent for $\tau$ if &lt;strong&gt;the propensity score model is correct and the outcome model is incorrect&lt;/strong&gt;. $\Rightarrow$ the estimator based on WLS is &lt;strong&gt;doubly robust&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;a-doubly-robust-estimator&#34;&gt;A doubly robust estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Let $\hat{e}(X_i)$ be the fitted propensity score and $(\mu_1(X_i, \hat{\beta}_1), \mu_0(X_i, \hat{\beta}_0))$ be the fitted values of the outcome means based on the WLS.&lt;/li&gt;
&lt;li&gt;The outcome regression estimator is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{wls}}^{\mathrm{reg}}= \frac{1}{n}\sum_{i=1}^n\mu_1\left(X_i, \hat{\beta}_1\right)-\frac{1}{n} \sum_{i=1}^n \mu_0\left(X_i, \hat{\beta}_0\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The doubly robust estimator for $\tau$ is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{wls}}^{\mathrm{reg}}+\frac{1}{n} \sum_{i=1}^n \frac{Z_i\left\{Y_i-\mu_1\left(X_i, \hat{\beta}_1\right)\right\}}{\hat{e}\left(X_i\right)}-\frac{1}{n} \sum_{i=1}^n \frac{\left(1-Z_i\right)\left\{Y_i-\mu_0\left(X_i, \hat{\beta}_0\right)\right\}}{1-\hat{e}\left(X_i\right)} .
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An interesting result is that this doubly robust estimator equals the outcome regression estimator, which reduces to the coefficient of $Z_i$ in the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ if we use weights (14.1).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;theorem-142&#34;&gt;Theorem 14.2&lt;/h3&gt;
&lt;p style=&#34;color: blue&#34;&gt;If $\bar{X}=0$ and $(\mu_1(X_i, \hat{\beta}_1), \mu_0(X_i, \hat{\beta}_0))=(\hat{\beta}_{10}+\hat{\beta}_{1 x}^{\top} X_i, \hat{\beta}_{00}+\hat{\beta}_{0 x}^{\top} X_i)$ based on the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ with weights (14.1), then&lt;/p&gt;
&lt;p style=&#34;color: blue&#34;&gt;$$
\hat{\tau}_{\mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{wls}}^{\mathrm{reg}}=\hat{\beta}_{10}-\hat{\beta}_{00},
$$&lt;/p&gt;
&lt;p style=&#34;color: blue&#34;&gt;which is the coefficient of $Z_i$ in the WLS fit.&lt;/p&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Freedman and Berk (2008) showed that when the outcome model is correct, the WLS estimator is worse than the OLS estimator&lt;/li&gt;
&lt;li&gt;When the errors have variance proportional to the inverse of the propensity scores, the WLS estimator will be more effcient than the OLS estimator.&lt;/li&gt;
&lt;li&gt;The estimated standard error based on the WLS fit is not consistent for the true standard error because it &lt;strong&gt;ignores the uncertainty in the estimated propensity score&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;This can be easily fixed by using the bootstrap to approximate the variance of the WLS estimator.&lt;/li&gt;
&lt;li&gt;Nevertheless, they found that &amp;ldquo;weighting may help under some circumstances&amp;rdquo; because when the outcome model is incorrect, the
WLS estimator is still consistent if the propensity score model is correct.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;average-causal-effect-on-the-treated-units&#34;&gt;Average causal effect on the treated units&lt;/h3&gt;
&lt;p&gt;&lt;mark&gt;Proposition 14.2&lt;/mark&gt; $\hat{\tau}_{\mathrm{T}}^{\text {hajek }}$ is numerically identical to $\hat{\beta}$ in the following WLS:
$$
(\hat{\alpha}, \hat{\beta})=\arg \min_{\alpha, \beta} \sum_{i=1}^n w_{\mathrm{T} i}\left(Y_i-\alpha-\beta Z_i\right)^2
$$
with weights
$$
w_{\mathrm{T} i}=Z_i+\left(1-Z_i\right) \hat{o}\left(X_i\right)= \begin{cases}1 &amp;amp; \text { if } Z_i=1 \\ \hat{o}\left(X_i\right) &amp;amp; \text { if } Z_i=0\end{cases}
$$
where $\hat{o}\left(X_i\right)=\hat{e}\left(X_i\right) /\{1-\hat{e}\left(X_i\right)\}$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regression-estimators&#34;&gt;Regression estimators&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;CRE&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;unconfounded observational studies&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;without $X$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim Z_i$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim Z_i$ with weights $w_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;with $X$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim\left(Z_i, X_i, Z_i X_i\right)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim\left(Z_i, X_i, Z_i X_i\right)$ with weights $w_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;average-causal-effect-on-the-treated-units-1&#34;&gt;Average causal effect on the treated units&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If we center covariates with $\hat{X}(1)=0$, then we can estimate $\tau_{\mathrm{T}}$ using the coefficient of $Z_i$ in the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ with weights defined in (14.2). Similarly, this estimator equals the regression estimator&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{reg}}=\hat{\bar{Y}}(1)-\frac{1}{n_1} \sum_{i=1}^n Z_i \mu_0\left(X_i, \hat{\beta}_0\right)
$$&lt;/p&gt;
&lt;p&gt;which also equals the doubly robust estimator&lt;/p&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{reg}}-\frac{1}{n_1} \sum_{i=1}^n \hat{o}\left(X_i\right)\left(1-Z_i\right)\left\{Y_i-\mu_0\left(X_i, \hat{\beta}_0\right)\right\}.
$$&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Theorem 14.3&lt;/mark&gt; If $\hat{\bar{X}}(1)=0$ and $\mu_0(X_i, \hat{\beta}_0)=\hat{\beta}_{00}+\hat{\beta}_{0x}^{\top} X_i$ based on the WLS fit of $Y_i$ on $(1, Z_i, X_i, Z_i X_i)$ with weights (14.2), then&lt;/p&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{reg}}=\hat{\beta}_{10}-\hat{\beta}_{00},
$$&lt;/p&gt;
&lt;p&gt;which is the coefficient of $Z_i$ in the $W L S$ fit.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
