<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>B样条 | Zhe Li</title>
    <link>https://ikerlz.github.io/tag/b%E6%A0%B7%E6%9D%A1/</link>
      <atom:link href="https://ikerlz.github.io/tag/b%E6%A0%B7%E6%9D%A1/index.xml" rel="self" type="application/rss+xml" />
    <description>B样条</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 12 May 2024 00:39:27 +0800</lastBuildDate>
    <image>
      <url>https://ikerlz.github.io/media/icon_hu9ccd2acdcd774e20fa34966445b706a8_6997380_512x512_fill_lanczos_center_3.png</url>
      <title>B样条</title>
      <link>https://ikerlz.github.io/tag/b%E6%A0%B7%E6%9D%A1/</link>
    </image>
    
    <item>
      <title>KAN: Kolmogorov–Arnold Networks</title>
      <link>https://ikerlz.github.io/post/kan/</link>
      <pubDate>Sun, 12 May 2024 00:39:27 +0800</pubDate>
      <guid>https://ikerlz.github.io/post/kan/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#0mlp&#34;&gt;0、MLP&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#一kolmogorovarnold-networks&#34;&gt;一、Kolmogorov–Arnold Networks&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-kolmogorov-arnold-representation-theorem&#34;&gt;1. Kolmogorov-Arnold Representation theorem&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-kan-architecture&#34;&gt;2. KAN Architecture&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-kans-approximation-abilities-and-scaling-laws&#34;&gt;3. KAN&amp;rsquo;s Approximation Abilities and Scaling Laws&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4-for-interpretability-simplifying-kans-and-making-them-interactive&#34;&gt;4. For Interpretability: Simplifying KANs and Making them interactive&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#二kan-的一些问题&#34;&gt;二、KAN 的一些问题&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#参考文献&#34;&gt;参考文献&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;h2 id=&#34;0mlp&#34;&gt;0、MLP&lt;/h2&gt;
&lt;p&gt;感知机最早由Rosenblatt于1957年提出，由于其简单的结构而得到快速的发展，下图是一个MLP的示意图
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/kan/MLP_fig_hu0087d83c60502258822faa8253c30ecd_296541_cc4a74d6b8bf3c465250cde822bc8145.webp 400w,
               /media/posts/kan/MLP_fig_hu0087d83c60502258822faa8253c30ecd_296541_b7b0e57353b50be120789d9cd27b4dda.webp 760w,
               /media/posts/kan/MLP_fig_hu0087d83c60502258822faa8253c30ecd_296541_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/kan/MLP_fig_hu0087d83c60502258822faa8253c30ecd_296541_cc4a74d6b8bf3c465250cde822bc8145.webp&#34;
               width=&#34;760&#34;
               height=&#34;381&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;我们可以把上面这个 MLP 表示为：
$$
f_{\mathrm{MLP}}(\mathbf{x})=\mathbf{W}_4 \boldsymbol{\sigma}\left(\mathbf{W}_3 \boldsymbol{\sigma}\left(\mathbf{W}_2 \boldsymbol{\sigma}\left(\mathbf{W}_1 \mathbf{x}+\mathbf{b}_1\right)+\mathbf{b}_2\right)+\mathbf{b}_3\right)+\mathbf{b}_4,
$$
可以看出，在 MLP 中，激活函数$\boldsymbol{\sigma}(\cdot)$是作用在&lt;strong&gt;节点&lt;/strong&gt;（node）上的，而&lt;strong&gt;边&lt;/strong&gt;（edge）的连接没有附带任何信息，唯一的作用就是把两层中的所有节点连接起来。&lt;/p&gt;
&lt;p&gt;MLP具有很强的拟合能力，常见的连续非线性函数都可以用MLP来近似
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/kan/fig1_hu639b401a7fefeab6de19635bd0f7128d_179933_111c8d2d86f002692e94673f1941516d.webp 400w,
               /media/posts/kan/fig1_hu639b401a7fefeab6de19635bd0f7128d_179933_d12ecc66e59562635215eaec3fcb7e58.webp 760w,
               /media/posts/kan/fig1_hu639b401a7fefeab6de19635bd0f7128d_179933_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/kan/fig1_hu639b401a7fefeab6de19635bd0f7128d_179933_111c8d2d86f002692e94673f1941516d.webp&#34;
               width=&#34;760&#34;
               height=&#34;510&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;但是，我们是否可以把非线性的激活函数放到边上呢？这就是 KAN 的基本思想&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;一kolmogorovarnold-networks&#34;&gt;一、Kolmogorov–Arnold Networks&lt;/h2&gt;
&lt;h3 id=&#34;1-kolmogorov-arnold-representation-theorem&#34;&gt;1. Kolmogorov-Arnold Representation theorem&lt;/h3&gt;
&lt;p&gt;Vladimir Arnold与Andrey Kolmogorov证明：如果$f: [0,1]^n \rightarrow \mathbb{R}$是一个多元连续函数，则$f$可以被写成单变量连续函数和加法的有限组合。具体而言，
$$
f(\mathbf{x})=f\left(x_1, \ldots, x_n\right)=\sum_{q=1}^{2 n+1} \Phi_q\left(\sum_{p=1}^n \phi_{q, p}\left(x_p\right)\right) .
$$
其中，$\phi_{q, p}:[0,1] \rightarrow \mathbb{R}$，$\Phi_q: \mathbb{R} \rightarrow \mathbb{R}$。&lt;/p&gt;
&lt;p&gt;似乎这个理论告诉我们学习一个高维函数归结为学习多项式个一维函数。然而，这些一维函数可能是&lt;strong&gt;非光滑&lt;/strong&gt;甚至是&lt;strong&gt;分形&lt;/strong&gt;的，因此在实践中可能无法学习到。由于这种&lt;strong&gt;病态行为&lt;/strong&gt;，KA表示定理在机器学习中基本上被判了死刑，被认为&lt;strong&gt;在理论上是正确的但在实践中无用&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-kan-architecture&#34;&gt;2. KAN Architecture&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/kan/fig2_huc050d1ac306643ad1ef18b4106826310_164533_561f46db569a0264b5a74457c4fa8eb6.webp 400w,
               /media/posts/kan/fig2_huc050d1ac306643ad1ef18b4106826310_164533_f82b157fd070e5c32e57c76a93b19793.webp 760w,
               /media/posts/kan/fig2_huc050d1ac306643ad1ef18b4106826310_164533_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/kan/fig2_huc050d1ac306643ad1ef18b4106826310_164533_561f46db569a0264b5a74457c4fa8eb6.webp&#34;
               width=&#34;760&#34;
               height=&#34;359&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;根据KA表示定理，我们只需要找到合适的$\phi_{p,q}$和$\Phi_q$函数，因此可以设计上图这样的一个神经网络，其中$\phi_{p,q}$和$\Phi_q$均为B样条函数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;所以本质上，KA表示定理其实就是一个两层的KAN，但是如何才能像MLP一样把KAN也做深一些呢？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我们首先用$n_i$表示第$i$层的节点数量，定义第$l$层的第$i$个节点的激活值为$x_{l,i}$，在第$l$层与$l+1$层共有$n_ln_{l+1}$个激活函数，我们定义连接$(l,i)$这个节点与$(l+1,j)$这个节点的激活函数为
$$
\phi_{l, j, i}, \quad l=0, \cdots, L-1, \quad i=1, \cdots, n_l, \quad j=1, \cdots, n_{l+1} .
$$
这样就有
$$
\mathbf{x}_{l+1}\in\mathbb{R}^{n_{l+1}}=\underbrace{\left(\begin{array}{cccc}
\phi_{l, 1,1}(\cdot) &amp;amp; \phi_{l, 1,2}(\cdot) &amp;amp; \cdots &amp;amp; \phi_{l, 1, n_l}(\cdot) \\
\phi_{l, 2,1}(\cdot) &amp;amp; \phi_{l, 2,2}(\cdot) &amp;amp; \cdots &amp;amp; \phi_{l, 2, n_l}(\cdot) \\
\vdots &amp;amp; \vdots &amp;amp; &amp;amp; \vdots \\
\phi_{l, n_{l+1}, 1}(\cdot) &amp;amp; \phi_{l, n_{l+1}, 2}(\cdot) &amp;amp; \cdots &amp;amp; \phi_{l, n_{l+1}, n_l}(\cdot)
\end{array}\right)}_{\boldsymbol{\Phi}_l\in\mathbb{R}^{n_{l+1}\times n_l}} \mathbf{x}_l \in \mathbb{R}^{n_l},
$$
$\boldsymbol{\Phi}_l$相当于是第$l$层 KAN 的函数矩阵，因此一个$L$层的KAN就可以表示为&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\operatorname{KAN}(\mathbf{x})=\left(\boldsymbol{\Phi}_{L-1} \circ \boldsymbol{\Phi}_{L-2} \circ \cdots \circ \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_0\right) \mathbf{x}
$$
对比 MLP：
$$
\operatorname{MLP}(\mathbf{x})=\left(\mathbf{W}_{L-1} \circ \sigma \circ \mathbf{W}_{L-2} \circ \sigma \circ \cdots \circ \mathbf{W}_1 \circ \sigma \circ \mathbf{W}_0\right) \mathbf{x} .
$$
可以看出，MLP将线性变换与非线性激活分别用$\mathbf{W}$和$\sigma$实现，而KAN直接在$\boldsymbol{\Phi}_l$中实现了线性变换和非线性激活&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/kan/fig3_hu5c92866393955c20e581765d7ba34ed5_289391_a4ca7ab8752c47d5b68365e981967ce9.webp 400w,
               /media/posts/kan/fig3_hu5c92866393955c20e581765d7ba34ed5_289391_88d55528293d2f11e0bd5732df2ada73.webp 760w,
               /media/posts/kan/fig3_hu5c92866393955c20e581765d7ba34ed5_289391_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/kan/fig3_hu5c92866393955c20e581765d7ba34ed5_289391_a4ca7ab8752c47d5b68365e981967ce9.webp&#34;
               width=&#34;760&#34;
               height=&#34;474&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;3-kans-approximation-abilities-and-scaling-laws&#34;&gt;3. KAN&amp;rsquo;s Approximation Abilities and Scaling Laws&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2.1 (Approximation theory, KAT).&lt;/strong&gt; Let $\mathbf{x}=\left(x_1, x_2, \cdots, x_n\right)$. Suppose that a function $f(\mathbf{x})$ admits a representation
$$
f=\left(\boldsymbol{\Phi}_{L-1} \circ \boldsymbol{\Phi}_{L-2} \circ \cdots \circ \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_0\right) \mathbf{x}
$$
as in Eq. (2.7), where each one of the $\Phi_{l, i, j}$ are $(k+1)$-times continuously differentiable. Then there exists a constant $C$ depending on $f$ and its representation, such that we have the following approximation bound in terms of the grid size $G$ : there exist $k$-th order $B$-spline functions $\Phi_{l, i, j}^G$ such that for any $0 \leq m \leq k$, we have the bound
$$
\left\|f-\left(\boldsymbol{\Phi}_{L-1}^G \circ \boldsymbol{\Phi}_{L-2}^G \circ \cdots \circ \boldsymbol{\Phi}_1^G \circ \boldsymbol{\Phi}_0^G\right) \mathbf{x}\right\|_{C^m} \leq C G^{-k-1+m} .
$$&lt;/p&gt;
&lt;p&gt;Here we adopt the notation of $C^m$-norm measuring the magnitude of derivatives up to order $m$ :
$$
\|g\|_{C^m}=\max _{|\beta| \leq m} \sup _{x \in[0,1]^n}\left|D^\beta g(x)\right| .
$$&lt;/p&gt;
&lt;p&gt;从结果来看，KAN的 upper bound 与维度$n$无关，破除了“维数诅咒”，从统计的角度开本质上就是引入了``可加结构&amp;rsquo;&amp;rsquo;，使得每个节点类似于一维的速度&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Neural scaling laws: comparison to other theories&lt;/strong&gt;：神经缩放定律描述了测试损失随着模型参数增加而减少的现象，即 $\ell \propto N^{-\alpha}$，其中 $\ell$ 表示测试均方根误差 (RMSE)，$N$ 为参数数量，$\alpha$ 为缩放指数。较大的 $\alpha$意味着通过简单地扩展模型可以获得更多的改进
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sharma 和 Kaplan 提出$\alpha$与输入流形维度$d$相关，如果模型函数类是阶数为$k$的分段多项式（对于 ReLU，$k=1$），那么标准逼近理论暗示$\alpha = \frac{k+1}{d}$。这一界限受限于维度诅咒。Michaud 等人考虑了仅涉及一元（例如，平方、正弦、指数）和二元（加法和乘法）运算的计算图，发现 $\alpha = \frac{k+1}{d^*} = \frac{k+1}{2}$，其中 $d^* = 2$ 是最大元数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;KAN假设存在平滑的 Kolmogorov-Arnold 表示，将高维函数分解为若干一维函数，得出 $\alpha = k+1$（其中 $k$ 是分段多项式样条的阶数）。如果选择 $k=3$ 的三次样条，因此 $\alpha=4$。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/kan/fig5_huee43b5f00266f0bc1e48bf7f6f88cea1_119679_05e097bbc07e056a4c6a379670b4c96b.webp 400w,
               /media/posts/kan/fig5_huee43b5f00266f0bc1e48bf7f6f88cea1_119679_7b08db614904234f598664a255c4b3d9.webp 760w,
               /media/posts/kan/fig5_huee43b5f00266f0bc1e48bf7f6f88cea1_119679_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/kan/fig5_huee43b5f00266f0bc1e48bf7f6f88cea1_119679_05e097bbc07e056a4c6a379670b4c96b.webp&#34;
               width=&#34;760&#34;
               height=&#34;159&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Comparison between KAT and UAT&lt;/strong&gt;：全连接神经网络的强大功能由&lt;strong&gt;普适逼近定理&lt;/strong&gt;（Universal Approximation Theorem, UAT）证明，该定理表明，给定一个函数和误差容忍度 $\epsilon &amp;gt; 0$，一个具有 $k &amp;gt; N(\epsilon)$ 个神经元的两层网络可以在误差 $\epsilon$ 范围内逼近该函数。然而，UAT 并未保证 $N(\epsilon)$ 如何随 $\epsilon$ 缩放的界限。实际上，它受制于维度诅咒（COD），并且在某些情况下 $N$ 被证明会随着 $d$ 指数级增长 [15]。KAT 和 UAT 之间的区别在于，KANs 利用了函数的内在低维表示，而 MLPs 则没有。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/kan/fig6_hud680800eab81a61bd2a8c28caacc360c_113409_468484747b9922db218e4e2311474236.webp 400w,
               /media/posts/kan/fig6_hud680800eab81a61bd2a8c28caacc360c_113409_a121f9e73a9c2243e48411b5bc69769e.webp 760w,
               /media/posts/kan/fig6_hud680800eab81a61bd2a8c28caacc360c_113409_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/kan/fig6_hud680800eab81a61bd2a8c28caacc360c_113409_468484747b9922db218e4e2311474236.webp&#34;
               width=&#34;760&#34;
               height=&#34;366&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;4-for-interpretability-simplifying-kans-and-making-them-interactive&#34;&gt;4. For Interpretability: Simplifying KANs and Making them interactive&lt;/h3&gt;
&lt;p&gt;如果我们需要拟合函数$f(x,y)=\exp\big(\sin(\pi x)+y^2\big)$，那么用$[2,1,1]$这个结构的 KAN 是最合适的，但是我们无法提前知道函数的形式，应该如何与 KAN 交互得到我们想要的结构呢？&lt;/p&gt;
&lt;p&gt;首先定义一个有$N_p$输入的激活函数$\phi$的$L_1$范数：&lt;/p&gt;
&lt;p&gt;$$
|\phi|_1 \equiv \frac{1}{N_p} \sum_{s=1}^{N_p}\left|\phi\left(x^{(s)}\right)\right| .
$$&lt;/p&gt;
&lt;p&gt;因此，对于一个$n_{\text {in }}$输入$n_{\text {out }}$输出的 KAN 层，定义$L_1$范数为&lt;/p&gt;
&lt;p&gt;$$
|\boldsymbol{\Phi}|_1 \equiv \sum_{i=1}^{n_{\text {in }}} \sum_{j=1}^{n_{\text {out }}}\left|\phi_{i, j}\right|_1
$$&lt;/p&gt;
&lt;p&gt;定义$\Phi$的交叉熵为&lt;/p&gt;
&lt;p&gt;$$
S(\boldsymbol{\Phi}) \equiv-\sum_{i=1}^{n_{\text {in }}} \sum_{j=1}^{n_{\text {out }}} \frac{\left|\phi_{i, j}\right|_1}{|\boldsymbol{\Phi}|_1} \log \left(\frac{\left|\phi_{i, j}\right|_1}{|\boldsymbol{\Phi}|_1}\right)
$$&lt;/p&gt;
&lt;p&gt;因此，损失函数$\ell_{\text {total }}$表示为：&lt;/p&gt;
&lt;p&gt;$$
\ell_{\text {total }}=\ell_{\text {pred }}+\lambda\left(\mu_1 \sum_{l=0}^{L-1}\left|\boldsymbol{\Phi}_l\right|_1+\mu_2 \sum_{l=0}^{L-1} S\left(\boldsymbol{\Phi}_l\right)\right),
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/kan/fig4_hu4c1eb9caa99cdf0c7662a90e5dfa868c_155173_edfb07896bed9e2566893bca645e171b.webp 400w,
               /media/posts/kan/fig4_hu4c1eb9caa99cdf0c7662a90e5dfa868c_155173_fa8ce302a4b7c6570772d27eb2b6d6c9.webp 760w,
               /media/posts/kan/fig4_hu4c1eb9caa99cdf0c7662a90e5dfa868c_155173_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/kan/fig4_hu4c1eb9caa99cdf0c7662a90e5dfa868c_155173_edfb07896bed9e2566893bca645e171b.webp&#34;
               width=&#34;760&#34;
               height=&#34;385&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Continual Learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/kan/fig7_hu7a7e27415d00c1525e5c07d0180c8525_56524_9b752d9860902e8058c8a70d9a86cfd3.webp 400w,
               /media/posts/kan/fig7_hu7a7e27415d00c1525e5c07d0180c8525_56524_60fdb5cc7ccf711bb8c5ba42f142e39b.webp 760w,
               /media/posts/kan/fig7_hu7a7e27415d00c1525e5c07d0180c8525_56524_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/kan/fig7_hu7a7e27415d00c1525e5c07d0180c8525_56524_9b752d9860902e8058c8a70d9a86cfd3.webp&#34;
               width=&#34;760&#34;
               height=&#34;327&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;KAN具有局部可塑性，并且通过利用样条的局部性可以避免&lt;strong&gt;灾难性遗忘&lt;/strong&gt;。原因是由于样条基是局部的，一个样本只会影响附近的几个样条系数，远处的系数则保持不变。相反，由于 MLPs 通常使用全局激活函数，例如 ReLU/Tanh/SiLU 等，任何局部变化都可能不受控制地传播到远离的区域，破坏那里存储的信息&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;结不变量是用来描述和区分不同结的性质的量，它们是结论中的一种重要概念。结不变量是指那些在结变形时保持不变的特征或量，它们可以帮助我们确定两个结是否同构或等价。结不变量可以是几何特征、代数特征或拓扑特征，常见的包括但不限于Jones多项式、Jones多项式的变种、Alexander多项式、交错数、三色定理等。&lt;/p&gt;
&lt;p&gt;在结论中，&amp;ldquo;signature&amp;rdquo;（签名）是一个结不变量，它是由代数结论家 John Milnor 在1957年提出的。签名是结的一个数字特征，可以通过对结进行某种标准操作（如连接和平移）得到。签名的概念在结论中有着广泛的应用，并在理解结的性质和关系方面提供了重要的见解。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/kan/fig8_hu6be06e4c13b52058d29fd8a36875b2f7_296548_2ebd66a3834974bdf9f7a22b67eec705.webp 400w,
               /media/posts/kan/fig8_hu6be06e4c13b52058d29fd8a36875b2f7_296548_ff269228f808e657924694c3baae9e94.webp 760w,
               /media/posts/kan/fig8_hu6be06e4c13b52058d29fd8a36875b2f7_296548_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/kan/fig8_hu6be06e4c13b52058d29fd8a36875b2f7_296548_2ebd66a3834974bdf9f7a22b67eec705.webp&#34;
               width=&#34;760&#34;
               height=&#34;520&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;符号主义 or 连接主义？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/kan/fig10_hu4699fb8dd9ced5f8af9ca21271a8a011_485038_12e5d7d80b69e108488b82a09347e9bc.webp 400w,
               /media/posts/kan/fig10_hu4699fb8dd9ced5f8af9ca21271a8a011_485038_7af8d044cec929a174e3c3e88f498a50.webp 760w,
               /media/posts/kan/fig10_hu4699fb8dd9ced5f8af9ca21271a8a011_485038_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/kan/fig10_hu4699fb8dd9ced5f8af9ca21271a8a011_485038_12e5d7d80b69e108488b82a09347e9bc.webp&#34;
               width=&#34;760&#34;
               height=&#34;456&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;KAN 介于符号主义与连接主义之间&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MLP 与 KAN 如何选择？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/kan/fig9_hu0ba5239902d77b59e6e91a501d847e5b_118614_8f2e0903ff6111426ad558ec2ed1c7d9.webp 400w,
               /media/posts/kan/fig9_hu0ba5239902d77b59e6e91a501d847e5b_118614_d690d639c3468258f25af454b7b28d99.webp 760w,
               /media/posts/kan/fig9_hu0ba5239902d77b59e6e91a501d847e5b_118614_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/kan/fig9_hu0ba5239902d77b59e6e91a501d847e5b_118614_8f2e0903ff6111426ad558ec2ed1c7d9.webp&#34;
               width=&#34;760&#34;
               height=&#34;371&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;二kan-的一些问题&#34;&gt;二、KAN 的一些问题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;目前在图片分类任务上（例如MNIST数据集）的表现不如MLP&lt;/li&gt;
&lt;li&gt;KAN的训练速度比 MLP 慢很多&lt;/li&gt;
&lt;li&gt;短期看KAN还很难取代MLP，主要原因还是因为KAN的训练算力要求明显高于MLP，想做大模型会有很高的算力壁垒。但是作者强调 KAN 更适用于 AI4Science 的任务，在这样的任务中，不需要这么大的模型，而是更关心收敛的速度&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;参考文献&#34;&gt;参考文献&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Poggio, T., Banburski, A., &amp;amp; Liao, Q. (2020). &lt;a href=&#34;https://cbmm.mit.edu/sites/default/files/publications/PNASlast.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Theoretical issues in deep networks&lt;/a&gt;. Proceedings of the National Academy of Sciences, 117(48), 30039-30045.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>B样条（B-Splines)</title>
      <link>https://ikerlz.github.io/post/spline/</link>
      <pubDate>Tue, 05 Mar 2024 00:39:27 +0800</pubDate>
      <guid>https://ikerlz.github.io/post/spline/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#一lagrange插值法&#34;&gt;一、Lagrange插值法&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#二bezier贝塞尔曲线与b-splines&#34;&gt;二、（Bezier）贝塞尔曲线与B-Splines&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1bezier贝塞尔曲线&#34;&gt;1、（Bezier）贝塞尔曲线&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2b-splines&#34;&gt;2、B-Splines&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#三样条估计&#34;&gt;三、样条估计&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#四拟合样条对深度学习中的双下降double-decent现象的解释&#34;&gt;四、拟合样条对深度学习中的双下降（Double Decent）现象的解释&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;h2 id=&#34;一lagrange插值法&#34;&gt;一、Lagrange插值法&lt;/h2&gt;
&lt;p&gt;已知若干点，如何得到&lt;strong&gt;光滑&lt;/strong&gt;曲线？是否可以通过在原有数据点上进行点的填充生成曲线？&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig1_hu56ed4bdc945701bcbc543a02d841351a_5272_0999ba0256e004e76caddbc2a1bb6fe9.webp 400w,
               /media/posts/spline/fig1_hu56ed4bdc945701bcbc543a02d841351a_5272_451087a27e071e66517027cbbe5b8df0.webp 760w,
               /media/posts/spline/fig1_hu56ed4bdc945701bcbc543a02d841351a_5272_1200x1200_fit_q75_h2_lanczos_2.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig1_hu56ed4bdc945701bcbc543a02d841351a_5272_0999ba0256e004e76caddbc2a1bb6fe9.webp&#34;
               width=&#34;695&#34;
               height=&#34;520&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

首先，可以考虑两个点的插值：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig2_hucb27f28971e8e84fa66a03f8e6b729f2_167146_32bfc7313d8dc81e10c4d2f0e73fa1fe.webp 400w,
               /media/posts/spline/fig2_hucb27f28971e8e84fa66a03f8e6b729f2_167146_22543dd06a9ae9464ecb074f3452c76c.webp 760w,
               /media/posts/spline/fig2_hucb27f28971e8e84fa66a03f8e6b729f2_167146_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig2_hucb27f28971e8e84fa66a03f8e6b729f2_167146_32bfc7313d8dc81e10c4d2f0e73fa1fe.webp&#34;
               width=&#34;760&#34;
               height=&#34;459&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

考虑$P_0$和$P_1$之间的任意一点$P_x$，可表示为：
$$P_x=P_0+\left(P_1-P_0\right) t=(1-t) P_0+t P_1$$
其中$t={(P_0 P_x)}/{(P_0 P_1)}$。直观上，我们可以把$P_0$和$P_1$视为&lt;strong&gt;控制点&lt;/strong&gt;，$(1-t)$和$t$视作&lt;strong&gt;基函数&lt;/strong&gt;。【思考：两点如何推广到多个点？】&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果知道三个点: $P_0, P_1, P_2$, 如何确定一条曲线 ?
&lt;ul&gt;
&lt;li&gt;想法: 将$P_0, P_1$ 进行连接，然后将$P_1, P_2$ 进行连接,。但是这样的一个曲线并不光滑&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;注意到，直线可以由2个点确定， 而二次曲线由三个点即可确定，推广到一般情况， $n-1$ 阶曲线可以由$n$个点确定&lt;/li&gt;
&lt;li&gt;这本质上就是Lagrange插值法的思想 (必须经过所有点)&lt;/li&gt;
&lt;li&gt;一般来说，如果我们有 $n$ 个点 $\left(x_1, y_1\right), \ldots,\left(x_n, y_n\right)$ ，各 $x_i$ 互不相同。对于 1 到 $\mathrm{n}$ 之间的每个 $k$, 定义 $n-1$ 次多项式
$$
L_k(x)=\frac{\left(x-x_1\right) \ldots\left(x-x_{k-1}\right)\left(x-x_{k+1}\right) \ldots\left(x-x_n\right)}{\left(x_k-x_1\right) \ldots\left(x_k-x_{k-1}\right)\left(x_k-x_{k+1}\right) \ldots\left(x_k-x_n\right)}
$$&lt;/li&gt;
&lt;li&gt;$L_k(x)$ 具有有趣的性质: $L_k\left(x_k\right)=1, L_k\left(x_j\right)=0, j \neq k$. 然后定义一个 $n-1$ 次多项式
$$
P_{n-1}(x)=y_1 L_1(x)+\ldots+y_n L_n(x)=\sum_{i=1}^ny_iL_i(x) .
$$
这样的多项式 $P_{n-1}(x)$ 满足 $P_{n-1}\left(x_i\right)=y_i, i=1,2, \ldots, n$. 因此必过控制点$\{(x_i,y_i)\mid i=1,\ldots,n\}$；这就是著名的拉格朗日插值多项式！&lt;/li&gt;
&lt;li&gt;我们可以把$L_1(x),\ldots,L_n(x)$看作基函数，而Lagrange插值本质上就是一组基的线性组合！&lt;/li&gt;
&lt;/ul&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Lagrange插值法例子&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;已知区间 $[-1,1]$ 上函数 $f(x)=[1+(5 x)^2]^{-1}$。取等距节点
$$
x_i=-1+\frac{i}{5}, \quad i=0,1,2, \cdots, 10
$$&lt;/li&gt;
&lt;li&gt;作Lagrange插值多项式
$$
P_{10}(x)=\sum_{i=0}^{10} f\left(x_i\right) L_i(x) .
$$
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig3_hu351a4dfac429a8bfc34629108fccb8d0_313529_6629893d1fd8ef7ad6631243ce3143d6.webp 400w,
               /media/posts/spline/fig3_hu351a4dfac429a8bfc34629108fccb8d0_313529_fa8cf5705a5f81531403c612ecdb2c6f.webp 760w,
               /media/posts/spline/fig3_hu351a4dfac429a8bfc34629108fccb8d0_313529_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig3_hu351a4dfac429a8bfc34629108fccb8d0_313529_6629893d1fd8ef7ad6631243ce3143d6.webp&#34;
               width=&#34;760&#34;
               height=&#34;752&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

从图中可以看出, 在 $x=0$ 附近, $P_{10}(x)$ 能较好地逼近 $f(x)$, 但在有些地方, 如在 $[-1,-0.8]$ 和 $[0.8,1]$ 之间, $P_{10}(x)$ 与 $f(x)$ 差异很大，这种现象被称为&lt;a href=&#34;https://en.wikipedia.org/wiki/Runge%27s_phenomenon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Runge现象&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;Runge现象出现的原因是在因为在进行Lagrange插值时要求必须要过控制点，因此可以考虑不过控制点进行插值，这样就能避免Runge现象&lt;/font&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;ul&gt;
&lt;li&gt;插值法因为要求经过所有节点, 所以导致这种结果, 因此在此基础上提出了拟合的概念:
&lt;ul&gt;
&lt;li&gt;依据原有数据点，通过参数调整设置，使得生成曲线与原有点差距最小 (最小二乘), 因此曲线未必会经过原有数据点&lt;/li&gt;
&lt;li&gt;样条曲线 (Spline curves): 是给定一系列控制点而得到的一条曲线，曲线形状由这些控制点控制。一般分为&lt;strong&gt;插值样条&lt;/strong&gt;和&lt;strong&gt;拟合样条&lt;/strong&gt;。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig4_hucbdfa747d9f91d6fc6936cd9674c23cd_415381_75d79890c994575678027044f0672a5a.webp 400w,
               /media/posts/spline/fig4_hucbdfa747d9f91d6fc6936cd9674c23cd_415381_6b4d894436fa7d9c9cf0995c04506441.webp 760w,
               /media/posts/spline/fig4_hucbdfa747d9f91d6fc6936cd9674c23cd_415381_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig4_hucbdfa747d9f91d6fc6936cd9674c23cd_415381_75d79890c994575678027044f0672a5a.webp&#34;
               width=&#34;760&#34;
               height=&#34;269&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;二bezier贝塞尔曲线与b-splines&#34;&gt;二、（Bezier）贝塞尔曲线与B-Splines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;均匀节点意义下的一元 B 样条 (B-splines, Basis Splines 缩写)是在 1946 年由 1.J.Schoenberg 提出&lt;/li&gt;
&lt;li&gt;1962 年, 法国数学家 Pierre Bezier 研究了一种曲线, 即 Bezier 曲线&lt;/li&gt;
&lt;li&gt;1972 年, de Boor 与 cox 分别独立提出了计算 B 样条基函数的公式这个公式对 B 样条作为计算机辅助几何设计 (CAGD)重要工具起到了至关重要的作用，称之为 de Boor-Cox 公式，即著名的 de Boor 算法&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1bezier贝塞尔曲线&#34;&gt;1、（Bezier）贝塞尔曲线&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig5_huabbb1fb5b7a7d33de92e8051846b5338_223478_358469c34f76ce019d490d6b79ae99ef.webp 400w,
               /media/posts/spline/fig5_huabbb1fb5b7a7d33de92e8051846b5338_223478_a6c07ad37a250b75791e7eed75cdbff3.webp 760w,
               /media/posts/spline/fig5_huabbb1fb5b7a7d33de92e8051846b5338_223478_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig5_huabbb1fb5b7a7d33de92e8051846b5338_223478_358469c34f76ce019d490d6b79ae99ef.webp&#34;
               width=&#34;760&#34;
               height=&#34;504&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不过 $P_1$ :
$$
\begin{gathered}
P_i&amp;amp;=(1-t) P_0+t P_1 ;\\
P_j&amp;amp;=(1-t) P_1+t P_2 ; \\
P_x&amp;amp;=(1-t) P_i+t P_j.
\end{gathered}
$$
其中，
$$
\frac{P_0 P_i}{P_0 P_1}=\frac{P_1 P_j}{P_1 P_2}=\frac{P_j P_x}{P_i P_j}=t
$$&lt;/li&gt;
&lt;li&gt;因此我们可以得到：$P_x=\left(1-t^2\right) P_0+2 t(1-t) P_1+t^2 P_2$&lt;/li&gt;
&lt;li&gt;我们可以把$P_0,P_1,P_2$看作控制点，把$(1-t^2)$，$2t(1-t)$和$t^2$看作是&lt;strong&gt;基函数&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;推广到一般情况，假设一共有 $n+1$ 个点, 就可以确定了$n$次的贝塞尔曲线
$$
B(t)=\sum_{i=0}^n C_n^i(1-t)^{n-i} t^i P_i, \quad t \in[0,1]
$$&lt;/li&gt;
&lt;li&gt;或者写成这样
$$
B(t)=W_{t, n}^0 P_0+W_{t, n}^1 P_1+\cdots+W_{t, n}^n P_n
$$&lt;/li&gt;
&lt;li&gt;可以理解为以$W$为基, $P$为系数的线性组合；其中$W_i=C_n^i(1-t)^{n-i} t^i$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;注：当有$n+1$个点, 有$n+1$个基函数, 确定$n$阶函数曲线&lt;/font&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$W_{t, n}^i$为$P_i$的系数, 是最高幂次为$n$的关于$t$的多项式。当 $t$确定后, 该值就为定值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;因此整个式子可以理解为$B(t)$插值点是这$n+1$个点施加各自的权重$W$后累加得到的。这也是为什么改变其中一个控制点, 整个贝塞尔曲线都会受到影响。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其实对于样条曲线的生成，本质上就对于各个控制点施加权重&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$n$阶贝塞尔曲线$B^n(t)$可以由&lt;strong&gt;前$n$个点&lt;/strong&gt;决定的$n-1$次贝塞尔曲线$B^{n-1}\left(t \mid P_0, \cdots, P_{n-1}\right)$与&lt;strong&gt;后$n$个点&lt;/strong&gt;决定的$n-1$次贝塞尔曲线$B^{n-1}\left(t \mid P_1, \cdots, P_n\right)$线性组合递推而来，即
$$
{\color{red}
\begin{aligned}
&amp;amp; B^n\left(t \mid P_0, P_1, \cdots, P_n\right)= \\
&amp;amp; (1-t) B^{n-1}\left(t \mid P_0, P_1, \cdots, P_{n-1}\right)+t B^{n-1}\left(t \mid P_1, P_2, \cdots, P_n\right)
\end{aligned}
}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2b-splines&#34;&gt;2、B-Splines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;我们比较好奇的是对于B样条，怎么得到各控制点前的权重（基函数）&lt;/li&gt;
&lt;li&gt;B-Splines的一些重要定义：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;控制点&lt;/strong&gt;: 控制曲线的点, 等价于贝塞尔函数的控制点, 通过控制点可以控制曲线形状。假设有 $n+1$ 个控制点 $P_0, P_1, \ldots, P_n$ 。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;节点&lt;/strong&gt;: 与控制点无关，是人为地将目标曲线分为若干个部分,其目的就是尽量使得各个部分有所影响但也有一定独立性,这也是为什么 $\mathrm{B}$ 样条中, 有时一个控制点的改变, 不会很大影响到整条曲线，而只影响到局部的原因，这区别于贝塞尔曲线。
&lt;ul&gt;
&lt;li&gt;节点划分影响权重计算，假设我们划分 $m+1$ 个节点 $t_0, t_1, \ldots, t_m$ ，将曲线分成了 $m$ 段。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;次 (degree) 与阶 (order)&lt;/strong&gt;: 次的概念是贝塞尔中次的概念, 即权重中$t$的最高幂次。阶 (order) $=$ 次 (degree) +1 。通常我们用 $k$ 表示次。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;注意到：
$$
B(t)=\sum_{i=0}^n W_i P_i=\sum_{i=0}^n B_{i, k}(t) P_i
$$
我们需要获得$W_i$即可。$W_i$是关于$t$的函数, 最高幂次为$k$。$B$样条中通常记为$B_{i, k}(t)$, 即表示第$i$点的权重, 是关于$t$的函数，且最高幂次为$k$。而这个权重函数$B_{i, k}(t)$，在 B样条里叫做&lt;font color=&#34;red&#34;&gt;$k$次B样条基函数&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;$B_{i, k}(t)$ 满足如下递推式 (&lt;strong&gt;de Boor 递推式&lt;/strong&gt;)
$$
{\color{blue}
\begin{gathered}
k=0, \quad B_{i, 0}(t)= \begin{cases}1, &amp;amp; t \in\left[t_i, t_{i+1}\right] \\
0, &amp;amp; \text { Otherwise }\end{cases} \\
k&amp;gt;0, \quad B_{i, k}(t)=\frac{t-t_i}{t_{i+k}-t_i} B_{i, k-1}(t)+\frac{t_{i+k+1}-t}{t_{i+k+1}-t_{i+1}} B_{i+1, k-1}(t)
\end{gathered}
}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;节点数、控制点数与次数的关系&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;控制点有$n+1=5$个, $n=4$ ，即 $P_0, P_1, P_2, P_3, P_4$&lt;/li&gt;
&lt;li&gt;节点规定为 $m+1=10$ 个, $m=9$ ，即 $t_0, t_1, \cdots, t_9$ ，该节点将要生成的目标曲线分为了 9 份，
&lt;ul&gt;
&lt;li&gt;这里的 $t$ 取值一般为 $0-1$的一系列非递减数。 $t_0, t_1, \cdots, t_9$ 组成的序列，叫做节点表，如等分的节点表 $\{0, \frac{1}{9}, \frac{2}{9}, \frac{3}{9}, \frac{4}{9}, \frac{5}{9}, \frac{6}{9}, \frac{7}{9}, \frac{8}{9}, 1\}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;次为$k$。&lt;/li&gt;
&lt;li&gt;三者有个必须要满足的关系式为
$$m=n+k+1$$
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig6_hucd6f327c77bf5a6fc4a28977e78830fc_176036_a4f1bb67cf9fc4bcc1c9c2b86f5a8723.webp 400w,
               /media/posts/spline/fig6_hucd6f327c77bf5a6fc4a28977e78830fc_176036_57bab267a8b45ca588e8544a47519cb2.webp 760w,
               /media/posts/spline/fig6_hucd6f327c77bf5a6fc4a28977e78830fc_176036_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig6_hucd6f327c77bf5a6fc4a28977e78830fc_176036_a4f1bb67cf9fc4bcc1c9c2b86f5a8723.webp&#34;
               width=&#34;760&#34;
               height=&#34;335&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;为什么满足$m=n+k+1$？
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig7_hu92917b485e74823b486796e8abf1aa8a_473303_8cb28449cf0f32c8b340d74819855a8c.webp 400w,
               /media/posts/spline/fig7_hu92917b485e74823b486796e8abf1aa8a_473303_452e0e3c04f767e91fdfd73e2fb45c3d.webp 760w,
               /media/posts/spline/fig7_hu92917b485e74823b486796e8abf1aa8a_473303_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig7_hu92917b485e74823b486796e8abf1aa8a_473303_8cb28449cf0f32c8b340d74819855a8c.webp&#34;
               width=&#34;760&#34;
               height=&#34;528&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig8_hu7f6ecb8edaf2c7d073ed44b848a97cde_1515393_d9c6c93dc0fa76d5eb6230735f24decf.webp 400w,
               /media/posts/spline/fig8_hu7f6ecb8edaf2c7d073ed44b848a97cde_1515393_bc3691af10c0f5a95686060ec641411a.webp 760w,
               /media/posts/spline/fig8_hu7f6ecb8edaf2c7d073ed44b848a97cde_1515393_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig8_hu7f6ecb8edaf2c7d073ed44b848a97cde_1515393_d9c6c93dc0fa76d5eb6230735f24decf.webp&#34;
               width=&#34;760&#34;
               height=&#34;605&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;实例计算&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;节点设置：
&lt;ul&gt;
&lt;li&gt;节点向量: $x=0,0.25,0.5,0.75,1$&lt;/li&gt;
&lt;li&gt;节点数: $m+1=5(m=4)$&lt;/li&gt;
&lt;li&gt;节点: $x_0=0, x_1=0.25, x_2=0.5, x_3=0.75, x_4=1$&lt;/li&gt;
&lt;li&gt;节点区间： $\left[x_0, x_1\right),\left[x_1, x_2\right),\left[x_2, x_3\right),\left[x_3, x_4\right)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;0 次 (degree) 基函数为:
$$
\begin{aligned}
&amp;amp; B_{0,0}(x)=\left\{\begin{array}{lcc}
1 &amp;amp; \text { if } &amp;amp; x_0 \leq x&amp;lt;x_1 \\
0 &amp;amp; &amp;amp; \text{ other }
\end{array}\right. \\
&amp;amp; B_{1,0}(x)=\left\{\begin{array}{ccc}
1 &amp;amp; \text { if } &amp;amp; x_1 \leq x&amp;lt;x_2 \\
0 &amp;amp; &amp;amp; \text{ other }
\end{array}\right.\\
&amp;amp; B_{2,0}(x)=\left\{\begin{array}{ccc}
1 &amp;amp; \text { if } &amp;amp; x_2 \leq x&amp;lt;x_3 \\
0 &amp;amp; &amp;amp; \text{ other }
\end{array}\right.\\
&amp;amp; B_{3,0}(x)=\left\{\begin{array}{ccc}
1 &amp;amp; \text { if } &amp;amp; x_3 \leq x&amp;lt;x_4 \\
0 &amp;amp; &amp;amp; \text{ other }
\end{array}\right.\\
&amp;amp; B_{4,0}(x)=\left\{\begin{array}{ccc}
1 &amp;amp; \text { if } &amp;amp; x_4 \leq x&amp;lt;x_5 \\
0 &amp;amp; &amp;amp; \text{ other }
\end{array}\right.
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;因此由上可知, 0次(degree)基函数均是分段函数&lt;/li&gt;
&lt;li&gt;基函数的迭代公式：
$$
\begin{aligned}
&amp;amp; B_{i, k}(x)=\frac{x-x_i}{x_{i+k}-x_i} B_{i, k-1}(x)+\frac{x_{i+k+1}-x}{x_{i+k+1}-x_{i+1}} B_{i+1, k-1}(x)
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;$k=1, i=0$时, 一次基函数
$$
\begin{aligned}
B_{0,1}(x)&amp;amp;=\frac{x-x_0}{x_1-x_0} B_{0,0}(x)+\frac{x_2-x}{x_2-x_1} B_{1,0}(x)\\
&amp;amp;=4 x B_{0,0}(x)+(2-4 x) B_{1,0}(x) \\
B_{0,1}(x)&amp;amp;=\left\{\begin{array}{cc}
4 x &amp;amp; 0 \leq x&amp;lt;0.25 \\
2-4 x &amp;amp; 0.25 \leq x&amp;lt;0.5 \\
0 &amp;amp; \text { other }
\end{array}\right.
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;若 $m=4, n=2, k=1$ 基函数 $n+1=3$ 个，基函数次数为 $k=1$&lt;/li&gt;
&lt;li&gt;若 $m=4, n=1, k=2$ 基函数 $n+1=2$ 个, 基函数次数为 $k=2$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig9_hu4029514fea627bd19700d5211331b497_340604_2738a064db6787d03d1d0084d3a4d540.webp 400w,
               /media/posts/spline/fig9_hu4029514fea627bd19700d5211331b497_340604_2f1e57cf81a50e716c60f257a907007a.webp 760w,
               /media/posts/spline/fig9_hu4029514fea627bd19700d5211331b497_340604_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig9_hu4029514fea627bd19700d5211331b497_340604_2738a064db6787d03d1d0084d3a4d540.webp&#34;
               width=&#34;760&#34;
               height=&#34;533&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h2 id=&#34;三样条估计&#34;&gt;三、样条估计&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;对于多元线性回归，我们有$\widehat{Y}=\mathbf{H}Y$，其中$\mathbf{H}$为帽子矩阵（hat matrix），定义为$\mathbf{H}=X(X^\top X)^{-1}X^\top$&lt;/li&gt;
&lt;li&gt;对于样条估计，假设模型形式为：
$$
\begin{aligned}
y&amp;amp;=\beta+\sum_{i=0}^n B_{i, k}(x) \beta_i\\
&amp;amp;=\beta+\beta_0 B_{0, k}(x)+\ldots+\beta_{m-k-1} B_{m-k-1, k}(x)
\end{aligned}
$$
其中 $\beta_i$ 是待估参数, $B_{i, k}(x)$ 为样条基函数，此时模型中的基函数个数为 $n=m-k-1$&lt;/li&gt;
&lt;li&gt;设计矩阵: 设计矩阵 $k=1, n=2, m=4$ ：(矩阵的列对应节点区间，相应的区间位置，填入相应的基函数)
$$
X=\left[\begin{array}{llll}
1 &amp;amp; B_{01} &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; B_{01} &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; B_{01} &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; B_{01} &amp;amp; B_{11} &amp;amp; B_{21}
\end{array}\right]=\left[\begin{array}{cccc}
1 &amp;amp; 4 x &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; 2-4 x &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; 0 &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; 0 &amp;amp; B_{11} &amp;amp; B_{21}
\end{array}\right]
$$&lt;/li&gt;
&lt;li&gt;帽子矩阵: $H=X\left(X^T X\right)^{-1} X^T$, 样条估计为: $\hat{y}=H y$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;四拟合样条对深度学习中的双下降double-decent现象的解释&#34;&gt;四、拟合样条对深度学习中的双下降（Double Decent）现象的解释&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://threadreaderapp.com/thread/1292293102103748609.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;参考 Prof. Daniela Witten发在Twitter上的文章&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig10_hu0ca6d9fa5e692adffadf293007778908_458298_e33d673138bff9da2c08b6ca5648f04d.webp 400w,
               /media/posts/spline/fig10_hu0ca6d9fa5e692adffadf293007778908_458298_62698b2655eb5636044058c534368b3d.webp 760w,
               /media/posts/spline/fig10_hu0ca6d9fa5e692adffadf293007778908_458298_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig10_hu0ca6d9fa5e692adffadf293007778908_458298_e33d673138bff9da2c08b6ca5648f04d.webp&#34;
               width=&#34;742&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bias-Variance Trade-off ?
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig11_hue0e5b97ed47da14796153e2957ee5fc4_173808_4470bed254fb93229994a514612ca29c.webp 400w,
               /media/posts/spline/fig11_hue0e5b97ed47da14796153e2957ee5fc4_173808_62796b489d601067ca50d513b6324c67.webp 760w,
               /media/posts/spline/fig11_hue0e5b97ed47da14796153e2957ee5fc4_173808_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig11_hue0e5b97ed47da14796153e2957ee5fc4_173808_4470bed254fb93229994a514612ca29c.webp&#34;
               width=&#34;760&#34;
               height=&#34;403&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;$\mathrm{U}$型测试误差曲线基于以下公式：
$$\text{Exp. Pred. Error} = \text{Irreducible Error}+\operatorname{Bias}^2+ \text{Var}$$&lt;/li&gt;
&lt;li&gt;随着灵活性的增加, (平方) 偏差减少, 方差增加。sweet spot需要权衡偏差和方差, 即具有中等程度灵活性的模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;过去的几年中，尤其是在深度学习领域，已经出现双下降现象。当你继续拟合越来越灵活且对训练数据进行插值处理的模型时，测试误差会再次减小！
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig12_hu787ed22831ed752e51031e26be985eac_102739_5ebb47993bfa56002fa09938c91fd4f5.webp 400w,
               /media/posts/spline/fig12_hu787ed22831ed752e51031e26be985eac_102739_51cc0a311ce3c3ce9deea09dbcbaf19d.webp 760w,
               /media/posts/spline/fig12_hu787ed22831ed752e51031e26be985eac_102739_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig12_hu787ed22831ed752e51031e26be985eac_102739_5ebb47993bfa56002fa09938c91fd4f5.webp&#34;
               width=&#34;702&#34;
               height=&#34;461&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;考虑自然三次样条曲线（natural cubic spline），拟合模型为$Y=f(x)+\epsilon$，所用基函数的数量与样条曲线的自由度（degrees of freedom, DF）相同。基函数基本形式如下：
$$
\left(X-\psi_1\right)_{+}^3,\quad \ldots,\quad\left(X-\psi_K\right)_{+}^3
$$&lt;/li&gt;
&lt;li&gt;我们首先生成$n=20$组$(X,Y)$数据，首先拟合一个$df=4$的样条曲线【$n=20$ 时的观测值为灰色小圆点，$f(x)=\sin(x)$ 为黑色曲线，拟合函数为浅蓝色曲线。】
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig13_hu9c3226fce5e0df57ecb32b8fa1875fdf_58494_0e271fef4f18de9a42b1f4fd23dbf39a.webp 400w,
               /media/posts/spline/fig13_hu9c3226fce5e0df57ecb32b8fa1875fdf_58494_1bba8e95ecb048c93ad0d8276220fd58.webp 760w,
               /media/posts/spline/fig13_hu9c3226fce5e0df57ecb32b8fa1875fdf_58494_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig13_hu9c3226fce5e0df57ecb32b8fa1875fdf_58494_0e271fef4f18de9a42b1f4fd23dbf39a.webp&#34;
               width=&#34;676&#34;
               height=&#34;364&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;然后分别拟合$df=6$和$df=20$的样条曲线
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig14_hu33ef8ba9e1ea428490d24ba3846b3659_54374_1c0bcd67fbae243924fa005c1234f64d.webp 400w,
               /media/posts/spline/fig14_hu33ef8ba9e1ea428490d24ba3846b3659_54374_40cc56b6bc7bfd4635493fb1b2fffb47.webp 760w,
               /media/posts/spline/fig14_hu33ef8ba9e1ea428490d24ba3846b3659_54374_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig14_hu33ef8ba9e1ea428490d24ba3846b3659_54374_1c0bcd67fbae243924fa005c1234f64d.webp&#34;
               width=&#34;681&#34;
               height=&#34;359&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig15_hu76ae5e1e118b955f2590ebd220748f58_69288_0c1c80d0e27fcb40aab840ad5e08bc67.webp 400w,
               /media/posts/spline/fig15_hu76ae5e1e118b955f2590ebd220748f58_69288_d42be289742ae241cf457b9bf846a982.webp 760w,
               /media/posts/spline/fig15_hu76ae5e1e118b955f2590ebd220748f58_69288_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig15_hu76ae5e1e118b955f2590ebd220748f58_69288_0c1c80d0e27fcb40aab840ad5e08bc67.webp&#34;
               width=&#34;681&#34;
               height=&#34;351&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

注意到：为了拟合$df=20$的样条曲线，需要用20个数据点来运行最小二乘法！结果显示在训练集上零误差，但在测试集上误差非常大！这些糟糕的结果也非常符合偏差-方差权衡&lt;/li&gt;
&lt;li&gt;如果继续拟合$df=36$（$p&amp;gt;n$）的样条曲线，由于解是不唯一的，为了在无穷多个解中进行选择，可以选择范数拟合：系数平方和最小的那个（利用SVD计算）
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig16_hu4fb763d4e7cf60838a1e8b720bc87b4b_65565_c626d953e59475bef66158e904d81cd5.webp 400w,
               /media/posts/spline/fig16_hu4fb763d4e7cf60838a1e8b720bc87b4b_65565_d0452b852ec123a043e5615418ddd971.webp 760w,
               /media/posts/spline/fig16_hu4fb763d4e7cf60838a1e8b720bc87b4b_65565_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig16_hu4fb763d4e7cf60838a1e8b720bc87b4b_65565_c626d953e59475bef66158e904d81cd5.webp&#34;
               width=&#34;688&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;对比了$df=20$和$df=36$的结果，可见$df=36$的结果比$df=20$要好一点。这是什么原因呢？
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig17_hu4f65f573c3a188111c745770ec28ca50_49933_57611722881f134a3846df8043203293.webp 400w,
               /media/posts/spline/fig17_hu4f65f573c3a188111c745770ec28ca50_49933_7c580ee94b25688ff51b6101faa52f56.webp 760w,
               /media/posts/spline/fig17_hu4f65f573c3a188111c745770ec28ca50_49933_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig17_hu4f65f573c3a188111c745770ec28ca50_49933_57611722881f134a3846df8043203293.webp&#34;
               width=&#34;700&#34;
               height=&#34;179&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;下图是训练误差和测试误差曲线，两者的变化曲线差别非常大。以虚线为分界线，当$p&amp;gt;n$时，为什么测试误差（暂时）减少？
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig18_hu4c909df76bd900404e830bcee74a1f31_49763_461252ed6375aea96873374b408206e6.webp 400w,
               /media/posts/spline/fig18_hu4c909df76bd900404e830bcee74a1f31_49763_cc7f2bf6b30ca26f29e7d25fe338e1ce.webp 760w,
               /media/posts/spline/fig18_hu4c909df76bd900404e830bcee74a1f31_49763_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig18_hu4c909df76bd900404e830bcee74a1f31_49763_461252ed6375aea96873374b408206e6.webp&#34;
               width=&#34;706&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;一个合理的解释为：关键在于当$df=20$，即$df=n$时，只有一个最小二乘拟合的训练误差为零。这种拟合会出现大量的振荡。&lt;/li&gt;
&lt;li&gt;但是当增加自由度，使得$p&amp;gt;n$时，则会出现大量的插值最小二乘拟合。最小范数的最小二乘拟合是这无数多个拟合中振荡最小的，甚至比$p=n$时的拟合更稳定。&lt;/li&gt;
&lt;li&gt;所以，选择最小范数最小二乘拟合实际上意味着$df=36$的样条曲线比$df=20$ 的样条曲线的灵活性差。&lt;/li&gt;
&lt;li&gt;如果在拟合样条曲线时使用了ridge penalty，而不是最小二乘，结果会怎么样呢？这时将不会有插值训练集，也不会看到双下降，而且会得到更好的测试误差&lt;/li&gt;
&lt;li&gt;所以，这些与深度学习有何关系？当使用（随机）梯度下降法来拟合神经网络时，实际上是在挑选最小范数解！因此，样条曲线示例非常类似于神经网络双下降时发生的情况。&lt;/li&gt;
&lt;li&gt;换种说法，当模型能力恰好能够产生零训练误差时，该现象导致测试误差达到峰值。但是，峰值不会出现在多层网络中，因为多层网络在优化时存在&lt;strong&gt;隐式正则化&lt;/strong&gt;的现象&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
