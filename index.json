
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Welcome! I am currently a PhD Student in School of Data Science at Fudan University, under the supervision of Prof. Xuening Zhu. My research interests include econometrics, distributed statistical modelling \u0026amp; inference, network data modelling.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Welcome! I am currently a PhD Student in School of Data Science at Fudan University, under the supervision of Prof. Xuening Zhu. My research interests include econometrics, distributed statistical modelling \u0026 inference, network data modelling.","tags":null,"title":"Li Zhe","type":"authors"},{"authors":["Zhe Li"],"categories":["Classical Statistics"],"content":" Table of Contents 一、Lagrange插值法 二、（Bezier）贝塞尔曲线与B-Splines 1、（Bezier）贝塞尔曲线 2、B-Splines 三、样条估计 四、拟合样条对深度学习中的双下降（Double Decent）现象的解释 一、Lagrange插值法 已知若干点，如何得到光滑曲线？是否可以通过在原有数据点上进行点的填充生成曲线？\n首先，可以考虑两个点的插值： 考虑$P_0$和$P_1$之间的任意一点$P_x$，可表示为： $$P_x=P_0+\\left(P_1-P_0\\right) t=(1-t) P_0+t P_1$$ 其中$t={(P_0 P_x)}/{(P_0 P_1)}$。直观上，我们可以把$P_0$和$P_1$视为控制点，$(1-t)$和$t$视作基函数。【思考：两点如何推广到多个点？】\n如果知道三个点: $P_0, P_1, P_2$, 如何确定一条曲线 ? 想法: 将$P_0, P_1$ 进行连接，然后将$P_1, P_2$ 进行连接,。但是这样的一个曲线并不光滑 注意到，直线可以由2个点确定， 而二次曲线由三个点即可确定，推广到一般情况， $n-1$ 阶曲线可以由$n$个点确定 这本质上就是Lagrange插值法的思想 (必须经过所有点) 一般来说，如果我们有 $n$ 个点 $\\left(x_1, y_1\\right), \\ldots,\\left(x_n, y_n\\right)$ ，各 $x_i$ 互不相同。对于 1 到 $\\mathrm{n}$ 之间的每个 $k$, 定义 $n-1$ 次多项式 $$ L_k(x)=\\frac{\\left(x-x_1\\right) \\ldots\\left(x-x_{k-1}\\right)\\left(x-x_{k+1}\\right) \\ldots\\left(x-x_n\\right)}{\\left(x_k-x_1\\right) \\ldots\\left(x_k-x_{k-1}\\right)\\left(x_k-x_{k+1}\\right) \\ldots\\left(x_k-x_n\\right)} $$ $L_k(x)$ 具有有趣的性质: $L_k\\left(x_k\\right)=1, L_k\\left(x_j\\right)=0, j \\neq k$. 然后定义一个 $n-1$ 次多项式 $$ P_{n-1}(x)=y_1 L_1(x)+\\ldots+y_n L_n(x)=\\sum_{i=1}^ny_iL_i(x) . $$ 这样的多项式 $P_{n-1}(x)$ 满足 $P_{n-1}\\left(x_i\\right)=y_i, i=1,2, \\ldots, n$. 因此必过控制点$\\{(x_i,y_i)\\mid i=1,\\ldots,n\\}$；这就是著名的拉格朗日插值多项式！ 我们可以把$L_1(x),\\ldots,L_n(x)$看作基函数，而Lagrange插值本质上就是一组基的线性组合！ Lagrange插值法例子 已知区间 $[-1,1]$ 上函数 $f(x)=[1+(5 x)^2]^{-1}$。取等距节点 $$ x_i=-1+\\frac{i}{5}, \\quad i=0,1,2, \\cdots, 10 $$ 作Lagrange插值多项式 $$ P_{10}(x)=\\sum_{i=0}^{10} f\\left(x_i\\right) L_i(x) . $$ 从图中可以看出, 在 $x=0$ 附近, $P_{10}(x)$ 能较好地逼近 $f(x)$, 但在有些地方, 如在 $[-1,-0.8]$ 和 $[0.8,1]$ 之间, $P_{10}(x)$ 与 $f(x)$ 差异很大，这种现象被称为Runge现象。 Runge现象出现的原因是在因为在进行Lagrange插值时要求必须要过控制点，因此可以考虑不过控制点进行插值，这样就能避免Runge现象\n插值法因为要求经过所有节点, 所以导致这种结果, 因此在此基础上提出了拟合的概念: 依据原有数据点，通过参数调整设置，使得生成曲线与原有点差距最小 (最小二乘), 因此曲线未必会经过原有数据点 样条曲线 (Spline curves): 是给定一系列控制点而得到的一条曲线，曲线形状由这些控制点控制。一般分为插值样条和拟合样条。 二、（Bezier）贝塞尔曲线与B-Splines 均匀节点意义下的一元 B 样条 (B-splines, Basis Splines 缩写)是在 1946 年由 1.J.Schoenberg 提出 1962 年, 法国数学家 Pierre Bezier 研究了一种曲线, 即 Bezier 曲线 1972 年, de Boor 与 cox 分别独立提出了计算 B 样条基函数的公式这个公式对 B 样条作为计算机辅助几何设计 (CAGD)重要工具起到了至关重要的作用，称之为 de Boor-Cox 公式，即著名的 de Boor 算法 1、（Bezier）贝塞尔曲线 不过 $P_1$ : $$ \\begin{gathered} P_i\u0026amp;=(1-t) P_0+t P_1 ;\\\\ P_j\u0026amp;=(1-t) P_1+t P_2 ; \\\\ P_x\u0026amp;=(1-t) P_i+t P_j. \\end{gathered} $$ 其中， $$ \\frac{P_0 P_i}{P_0 P_1}=\\frac{P_1 P_j}{P_1 P_2}=\\frac{P_j P_x}{P_i P_j}=t $$ 因此我们可以得到：$P_x=\\left(1-t^2\\right) P_0+2 t(1-t) P_1+t^2 P_2$ 我们可以把$P_0,P_1,P_2$看作控制点，把$(1-t^2)$，$2t(1-t)$和$t^2$看作是基函数 推广到一般情况，假设一共有 $n+1$ 个点, 就可以确定了$n$次的贝塞尔曲线 $$ B(t)=\\sum_{i=0}^n C_n^i(1-t)^{n-i} t^i P_i, \\quad t \\in[0,1] $$ 或者写成这样 $$ B(t)=W_{t, n}^0 P_0+W_{t, n}^1 P_1+\\cdots+W_{t, n}^n P_n $$ 可以理解为以$W$为基, $P$为系数的线性组合；其中$W_i=C_n^i(1-t)^{n-i} t^i$ 注：当有$n+1$个点, 有$n+1$个基函数, 确定$n$阶函数曲线\n$W_{t, n}^i$为$P_i$的系数, 是最高幂次为$n$的关于$t$的多项式。当 $t$确定后, 该值就为定值。\n因此整个式子可以理解为$B(t)$插值点是这$n+1$个点施加各自的权重$W$后累加得到的。这也是为什么改变其中一个控制点, 整个贝塞尔曲线都会受到影响。\n其实对于样条曲线的生成，本质上就对于各个控制点施加权重\n$n$阶贝塞尔曲线$B^n(t)$可以由前$n$个点决定的$n-1$次贝塞尔曲线$B^{n-1}\\left(t \\mid P_0, \\cdots, P_{n-1}\\right)$与后$n$个点决定的$n-1$次贝塞尔曲线$B^{n-1}\\left(t \\mid P_1, \\cdots, P_n\\right)$线性组合递推而来，即 $$ {\\color{red} \\begin{aligned} \u0026amp; B^n\\left(t \\mid P_0, P_1, \\cdots, P_n\\right)= \\\\ \u0026amp; (1-t) B^{n-1}\\left(t \\mid P_0, P_1, \\cdots, P_{n-1}\\right)+t B^{n-1}\\left(t \\mid P_1, P_2, \\cdots, P_n\\right) \\end{aligned} } $$\n2、B-Splines 我们比较好奇的是对于B样条，怎么得到各控制点前的权重（基函数） B-Splines的一些重要定义： 控制点: 控制曲线的点, 等价于贝塞尔函数的控制点, 通过控制点可以控制曲线形状。假设有 $n+1$ 个控制点 $P_0, P_1, \\ldots, P_n$ 。 节点: 与控制点无关，是人为地将目标曲线分为若干个部分,其目的就是尽量使得各个部分有所影响但也有一定独立性,这也是为什么 $\\mathrm{B}$ 样条中, 有时一个控制点的改变, 不会很大影响到整条曲线，而只影响到局部的原因，这区别于贝塞尔曲线。 节点划分影响权重计算，假设我们划分 $m+1$ 个节点 $t_0, t_1, \\ldots, t_m$ ，将曲线分成了 $m$ 段。 次 (degree) 与阶 (order): 次的概念是贝塞尔中次的概念, 即权重中$t$的最高幂次。阶 (order) $=$ 次 (degree) +1 。通常我们用 $k$ 表示次。 注意到： $$ B(t)=\\sum_{i=0}^n W_i P_i=\\sum_{i=0}^n B_{i, k}(t) P_i $$ 我们需要获得$W_i$即可。$W_i$是关于$t$的函数, 最高幂次为$k$。$B$样条中通常记为$B_{i, k}(t)$, 即表示第$i$点的权重, 是关于$t$的函数，且最高幂次为$k$。而这个权重函数$B_{i, k}(t)$，在 B样条里叫做$k$次B样条基函数 $B_{i, k}(t)$ 满足如下递推式 (de Boor 递推式) $$ {\\color{blue} \\begin{gathered} k=0, \\quad B_{i, 0}(t)= \\begin{cases}1, \u0026amp; t \\in\\left[t_i, t_{i+1}\\right] \\\\ 0, \u0026amp; \\text { Otherwise }\\end{cases} \\\\ k\u0026gt;0, \\quad B_{i, k}(t)=\\frac{t-t_i}{t_{i+k}-t_i} B_{i, k-1}(t)+\\frac{t_{i+k+1}-t}{t_{i+k+1}-t_{i+1}} B_{i+1, k-1}(t) \\end{gathered} } $$ 节点数、控制点数与次数的关系 控制点有$n+1=5$个, $n=4$ ，即 $P_0, P_1, P_2, P_3, P_4$ 节点规定为 $m+1=10$ 个, $m=9$ ，即 $t_0, t_1, \\cdots, t_9$ ，该节点将要生成的目标曲线分为了 9 份， 这里的 $t$ 取值一般为 $0-1$的一系列非递减数。 $t_0, t_1, \\cdots, t_9$ 组成的序列，叫做节点表，如等分的节点表 $\\{0, \\frac{1}{9}, \\frac{2}{9}, \\frac{3}{9}, \\frac{4}{9}, \\frac{5}{9}, \\frac{6}{9}, \\frac{7}{9}, \\frac{8}{9}, 1\\}$ 次为$k$。 三者有个必须要满足的关系式为 $$m=n+k+1$$ 为什么满足$m=n+k+1$？ 实例计算 节点设置： 节点向量: $x=0,0.25,0.5,0.75,1$ 节点数: $m+1=5(m=4)$ 节点: $x_0=0, x_1=0.25, x_2=0.5, x_3=0.75, x_4=1$ 节点区间： $\\left[x_0, x_1\\right),\\left[x_1, x_2\\right),\\left[x_2, x_3\\right),\\left[x_3, x_4\\right)$ 0 次 (degree) 基函数为: $$ \\begin{aligned} \u0026amp; B_{0,0}(x)=\\left\\{\\begin{array}{lcc} 1 \u0026amp; \\text { if } \u0026amp; x_0 \\leq x\u0026lt;x_1 \\\\ 0 \u0026amp; \u0026amp; \\text{ other } \\end{array}\\right. \\\\ \u0026amp; B_{1,0}(x)=\\left\\{\\begin{array}{ccc} 1 \u0026amp; \\text { if } \u0026amp; x_1 \\leq x\u0026lt;x_2 \\\\ 0 \u0026amp; …","date":1709570367,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709570367,"objectID":"4b9a037dd38b01138183d35b6350727a","permalink":"https://ikerlz.github.io/post/spline/","publishdate":"2024-03-05T00:39:27+08:00","relpermalink":"/post/spline/","section":"post","summary":"Table of Contents 一、Lagrange插值法 二、（Bezier）贝塞尔曲线与B-Splines 1、（Bezier）贝塞尔曲线 2、B-Splines 三、样条估计 四、拟合样条对深度学习中的双下降（Double Decent）现象的解释 一、Lagrange插值法 已知若干点，如何得到光滑曲线？是否可以通过在原有数据点上进行点的填充生成曲线？\n首先，可以考虑两个点的插值： 考虑$P_0$和$P_1$之间的任意一点$P_x$，可表示为： $$P_x=P_0+\\left(P_1-P_0\\right) t=(1-t) P_0+t P_1$$ 其中$t={(P_0 P_x)}/{(P_0 P_1)}$。直观上，我们可以把$P_0$和$P_1$视为控制点，$(1-t)$和$t$视作基函数。【思考：两点如何推广到多个点？】\n如果知道三个点: $P_0, P_1, P_2$, 如何确定一条曲线 ? 想法: 将$P_0, P_1$ 进行连接，然后将$P_1, P_2$ 进行连接,。但是这样的一个曲线并不光滑 注意到，直线可以由2个点确定， 而二次曲线由三个点即可确定，推广到一般情况， $n-1$ 阶曲线可以由$n$个点确定 这本质上就是Lagrange插值法的思想 (必须经过所有点) 一般来说，如果我们有 $n$ 个点 $\\left(x_1, y_1\\right), \\ldots,\\left(x_n, y_n\\right)$ ，各 $x_i$ 互不相同。对于 1 到 $\\mathrm{n}$ 之间的每个 $k$, 定义 $n-1$ 次多项式 $$ L_k(x)=\\frac{\\left(x-x_1\\right) \\ldots\\left(x-x_{k-1}\\right)\\left(x-x_{k+1}\\right) \\ldots\\left(x-x_n\\right)}{\\left(x_k-x_1\\right) \\ldots\\left(x_k-x_{k-1}\\right)\\left(x_k-x_{k+1}\\right) \\ldots\\left(x_k-x_n\\right)} $$ $L_k(x)$ 具有有趣的性质: $L_k\\left(x_k\\right)=1, L_k\\left(x_j\\right)=0, j \\neq k$.","tags":["B样条","Lagrange插值","贝塞尔曲线"],"title":"B样条（B-Splines)","type":"post"},{"authors":["Yimeng Ren","Zhe Li","Xuening Zhu","Yuan Gao","Hansheng Wang"],"categories":null,"content":" ","date":1706313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706313600,"objectID":"94e8403afddfc5052342001cd845ce2c","permalink":"https://ikerlz.github.io/publication/dsar/","publishdate":"2023-11-27T00:00:00Z","relpermalink":"/publication/dsar/","section":"publication","summary":"The rapid growth of online network platforms generates large-scale network data and it poses great challenges for statistical analysis using the spatial autoregression (SAR) model. In this work, we develop a novel distributed estimation and statistical inference framework for the SAR model on a distributed system. We first propose a distributed network least squares approximation (DNLSA) method. This enables us to obtain a one-step estimator by taking a weighted average of local estimators on each worker. Afterwards, a refined two-step estimation is designed to further reduce the estimation bias. For statistical inference, we utilize a random projection method to reduce the expensive communication cost. Theoretically, we show the consistency and asymptotic normality of both the one-step and two-step estimators. In addition, we provide theoretical guarantee of the distributed statistical inference procedure. The theoretical findings and computational advantages are validated by several numerical simulations implemented on the Spark system. Lastly, an experiment on the Yelp dataset further illustrates the usefulness of the proposed methodology.","tags":null,"title":"Distributed Estimation and Inference for Spatial Autoregression Model with Large Scale Networks","type":"publication"},{"authors":["Zhe Li"],"categories":["Causal Inference"],"content":" Disentangle Mixture Distributions and Instrumental Variable Inequalities $$ \\begin{aligned} \\ \\end{aligned} $$\nLi Zhe $$$$\nSchool of Data Science, Fudan University $$$$\nJanuary 3, 2024 Introduction The IV model in last chapter imposes three assumptions\n(randomization) $Z \\perp\\!\\!\\!\\perp\\{D(1), D(0), Y(1), Y(0)\\}$ (monotonicity) $\\operatorname{pr}(U=\\mathrm{d})=0$ or $D_i(1)\\geq D_i(0)$ (exclusionrestriction) $Y(1)=Y(0) \\text { for } U=\\mathrm{a} \\text { or } \\mathrm{n}$ $$ {\\tiny U_i= \\begin{cases} a, \u0026amp; \\text { if } D_i(1)=1 \\text { and } D_i(0)=1 \\\\ c, \u0026amp; \\text { if } D_i(1)=1 \\text { and } D_i(0)=0 \\\\ d, \u0026amp; \\text { if } D_i(1)=0 \\text { and } D_i(0)=1 \\\\ n, \u0026amp; \\text { if } D_i(1)=0 \\text { and } D_i(0)=0 \\end{cases} } $$\nObserved groups and latent groups under the assumptions $$ {\\small \\begin{array}{cccl} Z=1 \u0026amp; D=1 \u0026amp; D(1)=1 \u0026amp; U=\\mathrm{c} \\text { or a } \\\\ Z=1 \u0026amp; D=0 \u0026amp; D(1)=0 \u0026amp; U=\\mathrm{n} \\\\ Z=0 \u0026amp; D=1 \u0026amp; D(0)=1 \u0026amp; U=\\mathrm{a} \\\\ Z=0 \u0026amp; D=0 \u0026amp; D(0)=0 \u0026amp; U=\\mathrm{c} \\text { or } \\mathrm{n} \\end{array} } $$\nInterestingly, the assumptions have some testable implications. Balke and Pearl (1997) called them the instrumental variable inequalities. Outline Disentangle Mixture Distributions and Instrumental Variable Inequalities\nTestable implications\nExamples\nDisentangle Mixture Distributions and IV Inequalities Recall $\\pi_u$ as the proportion of type $U=u$, and define $$ \\mu_{z u}=E\\{Y(z) \\mid U=u\\}, \\quad(z=0,1 ; u=\\mathrm{a}, \\mathrm{n}, \\mathrm{c}) . $$\nTheorem 22.1 Under the three assumptions, we can identify the proportions of the latent types by $$ \\begin{aligned} \u0026amp; \\pi_{\\mathrm{n}}=\\operatorname{pr}(D=0 \\mid Z=1), \\\\ \u0026amp; \\pi_{\\mathrm{a}}=\\operatorname{pr}(D=1 \\mid Z=0), \\\\ \u0026amp; \\pi_{\\mathrm{c}}=E(D \\mid Z=1)-E(D \\mid Z=0), \\end{aligned} $$ and the type-specific means of the potential outcomes by $$ \\begin{aligned} \\mu_{1 \\mathrm{n}}=\\mu_{0 \\mathrm{n}} \\equiv \\mu_{\\mathrm{n}} \u0026amp; =E(Y \\mid Z=1, D=0) \\\\ \\mu_{1 \\mathrm{a}}=\\mu_{0 \\mathrm{a}} \\equiv \\mu_{\\mathrm{a}} \u0026amp; =E(Y \\mid Z=0, D=1) \\\\ \\mu_{1 \\mathrm{c}} \u0026amp; =\\pi_{\\mathrm{c}}^{-1}\\{E(D Y \\mid Z=1)-E(D Y \\mid Z=0)\\} \\\\ \\mu_{0 \\mathrm{c}} \u0026amp; =\\pi_{\\mathrm{c}}^{-1}[E\\{(1-D) Y \\mid Z=0\\}-E\\{(1-D) Y \\mid Z=1\\}] \\end{aligned} $$\nProof of Theorem 22.1 (Part I) We can identify the proportion of the never takers by $$ \\begin{aligned} \\operatorname{pr}(D=0 \\mid Z=1) \u0026amp; =\\operatorname{pr}(U=\\mathrm{n} \\mid Z=1)=\\operatorname{pr}(U=\\mathrm{n})=\\pi_{\\mathrm{n}}, \\end{aligned} $$\nthe proportion of the always takers by $$ \\begin{aligned} \\operatorname{pr}(D=1 \\mid Z=0) \u0026amp; =\\operatorname{pr}(U=\\mathrm{a} \\mid Z=0)=\\operatorname{pr}(U=\\mathrm{a})=\\pi_{\\mathrm{a}} . \\end{aligned} $$\nthe proportion of compliers is $$ \\begin{aligned} \\pi_{\\mathrm{c}} \u0026amp; =\\operatorname{pr}(U=\\mathrm{c})=1-\\pi_{\\mathrm{n}}-\\pi_{\\mathrm{a}} \\\\ \u0026amp; =1-\\operatorname{pr}(D=0 \\mid Z=1)-\\operatorname{pr}(D=1 \\mid Z=0) \\\\ \u0026amp; =E(D \\mid Z=1)-E(D \\mid Z=0)=\\tau_D, \\end{aligned} $$\nRemark: Although we do not know individual latent compliance types for all units, we can identify the proportions of never takers, always takers, and compliers.\nProof of Theorem 22.1 (Part II) Under the three assumptions, we have\n$$ \\mu_{1 \\mathrm{a}}=\\mu_{0 \\mathrm{a}} \\equiv \\mu_{\\mathrm{a}}, \\quad \\mu_{1 \\mathrm{n}}=\\mu_{0 \\mathrm{n}} \\equiv \\mu_{\\mathrm{n}} . $$\nThe observed group $(Z=1, D=0)$ only has never takers, so $$ E(Y \\mid Z=1, D=0)=E\\{Y(1) \\mid Z=1, U=\\mathrm{n}\\}=E\\{Y(1) \\mid U=\\mathrm{n}\\}=\\mu_{\\mathrm{n}} . $$\nThe observed group $(Z=0, D=1)$ only has always takers, so $$ E(Y \\mid Z=0, D=1)=E\\{Y(0) \\mid Z=0, U=\\mathrm{a}\\}=E\\{Y(0) \\mid U=\\mathrm{a}\\}=\\mu_{\\mathrm{a}} $$\n$$~~$$\nHow about the observed groups $(Z=1, D=1)$ and $(Z=0, D=0)$ ?\nThe Observed Group $(Z=1, D=1)$ The observed group $(Z=1, D=1)$ has both compliers and always takers, so $$ \\small \\begin{aligned} E(Y \\mid Z=1, D=1)= \u0026amp; E\\{Y(1) \\mid Z=1, D(1)=1\\} \\\\ = \u0026amp; E\\{Y(1) \\mid D(1)=1\\} \\\\ = \u0026amp; \\operatorname{pr}\\{D(0)=1 \\mid D(1)=1\\} E\\{Y(1) \\mid D(1)=1, D(0)=1\\} \\\\ \u0026amp; +\\operatorname{pr}\\{D(0)=0 \\mid D(1)=1\\} E\\{Y(1) \\mid D(1)=1, D(0)=0\\} \\\\ = \u0026amp; \\frac{\\pi_{\\mathrm{c}}}{\\pi_{\\mathrm{c}}+\\pi_{\\mathrm{a}}} \\mu_{1 \\mathrm{c}}+\\frac{\\pi_{\\mathrm{a}}}{\\pi_{\\mathrm{c}}+\\pi_{\\mathrm{a}}} \\mu_{\\mathrm{a}} . \\end{aligned} $$\nSolve the linear equation above to obtain $$ \\small \\begin{aligned} \\mu_{1 \\mathrm{c}}= \u0026amp; \\pi_{\\mathrm{c}}^{-1}\\left\\{\\left(\\pi_{\\mathrm{c}}+\\pi_{\\mathrm{a}}\\right) E(Y \\mid Z=1, D=1)-\\pi_{\\mathrm{a}} E(Y \\mid Z=0, D=1)\\right\\} \\\\ = \u0026amp; \\pi_{\\mathrm{c}}^{-1}\\{\\operatorname{pr}(D=1 \\mid Z=1) E(Y \\mid Z=1, D=1) \\\\ \u0026amp; \\quad-\\operatorname{pr}(D=1 \\mid Z=0) E(Y \\mid Z=0, D=1)\\} \\\\ = \u0026amp; \\pi_{\\mathrm{c}}^{-1}\\{E(D Y \\mid Z=1)-E(D Y \\mid Z=0)\\} . \\end{aligned} $$\nBased on the formulas of $\\mu_{1 \\mathrm{c}}$ and $\\mu_{0 \\mathrm{c}}$ in Theorem 22.1, we have\n$$ \\tau_{\\mathrm{c}}=\\mu_{1 \\mathrm{c}}-\\mu_{0 \\mathrm{c}}=\\{E(Y \\mid Z=1)-E(Y \\mid Z=0)\\} / \\pi_{\\mathrm{c}}. $$\nTestable implications Is there any additional value of the this detour for deriving the formula of $\\tau_c$ …","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"eabdaabaf1469b7f03581e40e166b6e7","permalink":"https://ikerlz.github.io/slides/causalinferencechapter22/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/slides/causalinferencechapter22/","section":"slides","summary":"Chapter 22 in \"A First Course in Causal Inference\".","tags":[],"title":"Disentangle Mixture Distributions and Instrumental Variable Inequalities","type":"slides"},{"authors":["Zhe Li"],"categories":["Factor Model"],"content":" On factor models with random missing: EM estimation, inference, and cross validation $$ \\begin{aligned} \\ \\end{aligned} $$\nLi Zhe $$$$\nSchool of Data Science, Fudan University $$$$\nNovember 30, 2023 Outline Background: factor model Motivation Factor models with random missing: EM estimator Asymptotic properties Determining the number of factors Simulation Empirical application Background: factor model $$ \\begin{aligned} x_{i t} \u0026amp; =\\lambda_i f_t+e_{i t} \\\\ \\boldsymbol{X} \u0026amp;= \\boldsymbol{\\Lambda} \\boldsymbol{F}^\\top + \\boldsymbol{E} \\end{aligned} $$\n$\\boldsymbol{X}=(\\boldsymbol{x}_{\\cdot 1},\\ldots,\\boldsymbol{x}_{\\cdot T})\\in\\mathbb{R}^{N\\times T}$\n$\\boldsymbol{\\Lambda}\\in\\mathbb{R}^{N\\times R}$: factor loadings\n$\\boldsymbol{F}\\in\\mathbb{R}^{T\\times R}$: common factors (latent, unobserved)\n$\\boldsymbol{E}\\in\\mathbb{R}^{N\\times T}$: idiosyncratic (or error) component\n${e_{it}}$ can exhibit both cross-sectional and temporal dependence. Given the factor number $k$, we can estimate the factors and factor loadings by $$ \\Big\\{\\widehat{\\boldsymbol{\\Lambda}}^k, \\widehat{\\boldsymbol{F}}^k\\Big\\}=\\arg\\min_{\\boldsymbol{\\Lambda}^k,\\boldsymbol{F}^k}\\frac{1}{NT}\\Big\\|\\boldsymbol{X}- \\boldsymbol{\\Lambda}^k{\\boldsymbol{F}^k}^\\top\\Big\\|_F^2 $$ where $\\boldsymbol{\\Lambda}^k\\in\\mathbb{R}^{N\\times k}$ and $\\boldsymbol{F}^k\\in\\mathbb{R}^{T\\times k}$.\nMotivation Factor model in balanced panel has been thoroughly investigated. $$$$ How to handle the missing data problem in factor models? $$$$ the expectation–maximization (EM) algorithm $$$$ the Kalman filter (KF) $$$$ There is no formal study of the asymptotic properties for the EM estimators of the factors and factor loadings for the PC estimation with missing observations Notations consider the factor model $$ \\boldsymbol{X} = \\boldsymbol{F}\\boldsymbol{\\Lambda}^\\top + \\varepsilon $$\n$\\boldsymbol{X}=(X_1,\\ldots,X_N)$ where $X_i \\equiv\\left(X_{i 1}, \\ldots, X_{i T}\\right)^{\\prime}$ and $X_{it}$ are missing at random $\\varepsilon=\\left(\\varepsilon_1, \\ldots, \\varepsilon_N\\right)$ and $\\varepsilon_i \\equiv\\left(\\varepsilon_{i 1}, \\ldots, \\varepsilon_{i T}\\right)^{\\prime}$ for $i=1, \\ldots, N$. $F=\\left(F_1, \\ldots, F_T\\right)^{\\prime}$ and $\\Lambda=\\left(\\lambda_1, \\ldots, \\lambda_N\\right)^{\\prime}$ where $F_t$ and $\\lambda_i$ are $R \\times 1$ vectors of factors and factor loadings $F^0=\\left(F_1^0, \\ldots, F_T^0\\right)^{\\prime}$ and $\\Lambda^0=\\left(\\lambda_1^0, \\ldots, \\lambda_N^0\\right)^{\\prime}$ are the true values of $F$ and $\\Lambda$\n$\\Omega \\subset[N] \\times[T]$ be the index set of the observations that are observed. That is, $$ \\Omega=\\Big\\{(i, t) \\in[N] \\times[T]: X_{i t} \\text { is observed }\\Big\\}. $$\nLet $G$ denote a $T \\times N$ matrix with $(t, i)$ th element given by $g_{i t}=\\mathbf{1}\\{(i, t) \\in \\Omega\\}$ and is independent of $X, F^0, \\Lambda^0$ and $\\varepsilon$\nThe initial estimates Let $\\tilde{X}=X \\circ G$ and $\\tilde{X}_{i t}=X_{i t} g_{i t}$.\nThe common component $C^0 \\equiv F^0 \\Lambda^0$ is a low rank matrix $\\Rightarrow$ it is possible to recover $C^0$ even when a large proportion of elements in $X$ are missing at random.\nUnder the standard condition that $E\\left(\\varepsilon_{i t} \\mid F_t^0, \\lambda_i^0\\right)=0$, we can verify that $E\\left(\\frac{1}{q} \\tilde{X} \\mid F^0, \\Lambda^0\\right)=F^0 \\Lambda^{0 \\prime}$ $\\Rightarrow$ consider the following least squares objective function $$ \\mathcal{L}_{N T}^0(F, \\Lambda) \\equiv \\frac{1}{N T} \\operatorname{tr}\\left[\\left(\\frac{1}{\\tilde{q}} \\tilde{X}-F \\Lambda^{\\prime}\\right)\\left(\\frac{1}{\\tilde{q}} \\tilde{X}-F \\Lambda^{\\prime}\\right)^{\\prime}\\right] $$ identification restrictions: $F^{\\prime} F / T=I_R$ and $\\Lambda^{\\prime} \\Lambda$ is a diagonal matrix.\nBy concentrating out $\\Lambda$ and using the normalization that $F^{\\prime} F / T=I_R$ $\\Rightarrow$ identical to maximizing $\\tilde{q}^{-2} \\operatorname{tr}\\Big\\{F^{\\prime} \\tilde{X} \\tilde{X}^{\\prime} F\\Big\\}$\nThe initial estimates The estimated factor matrix, denoted by $\\hat{F}^{(0)}$ is $\\sqrt{T}$ times the eigenvectors corresponding to the $R$ largest eigenvalues of the $T \\times T$ matrix $\\frac{1}{N T \\tilde{q}^2} \\tilde{X} \\tilde{X}^{\\prime}:$ $$ \\frac{1}{N T \\tilde{q}^2} \\tilde{X} \\tilde{X}^{\\prime} \\hat{F}^{(0)}=\\hat{F}^{(0)} \\hat{D}^{(0)}, $$ $\\hat{D}^{(0)}$ is an $R \\times R$ diagonal matrix consisting of the $R$ largest eigenvalues of $\\left(N T \\tilde{q}^2\\right)^{-1} \\tilde{X} \\tilde{X}^{\\prime}$, arranged in descending order along its diagonal line. The estimator of $\\Lambda^{\\prime}$ is given by $$ \\hat{\\Lambda}^{(0) \\prime}=\\frac{1}{\\tilde{q}}\\left(\\hat{F}^{(0) \\prime} \\hat{F}^{(0)}\\right)^{-1} \\hat{F}^{(0) \\prime} \\tilde{X}=\\frac{1}{T \\tilde{q}} \\hat{F}^{(0) \\prime} \\tilde{X} . $$ We can obtain an initial estimate of the $(t, i)$ th element, $C_{i t}^0$, of $C^0$ by $\\hat{C}_{i t}^{(0)}=\\hat{\\lambda}_i^{(0)\\prime} \\hat{F}_t^{(0)}$. The initial estimators $\\hat{F}_t^{(0)}, \\hat{\\lambda}_i^{(0)}$ and $\\hat{C}_{i t}^{(0)}$ are consistent and …","date":1701043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701043200,"objectID":"b3e16f3b496f99be8b9b023efab776da","permalink":"https://ikerlz.github.io/slides/factormissing/","publishdate":"2023-11-27T00:00:00Z","relpermalink":"/slides/factormissing/","section":"slides","summary":"Notes on \"On factor models with random missing： EM estimation, inference, and cross validation\".","tags":[],"title":"On factor models with random missing： EM estimation, inference, and cross validation","type":"slides"},{"authors":[],"categories":["factor-model"],"content":" Table of Contents 1. Introduction 2. The three-pass regression filter 2.1. The estimator 2.2. Assumptions 2.3. Consistency 2.4. Asymptotic distributions 2.5. Proxy selection 3. Related procedures 3.1. Constrained least squares 3.2. Partial least squares 4. Empirical evidence 4.1. Forecasting macroeconomic aggregates 4.2. Forecasting market returns 1. Introduction How to use the vast predictive information to forecast important economic aggregates like national product or stock market value.\nIf the predictors number near or more than the number of observations, the standard OLS forecaster is known to be poorly behaved or nonexistent (Huber, 1973) A economic view: data are generated from a model in which latent factors drive the systematic variation of both the forecast target, $\\boldsymbol{y}$, and the matrix of predictors, $\\boldsymbol{X}$ $\\Rightarrow$ the best prediction of $\\boldsymbol{y}$ is infeasible since the factors are unobserved $\\Rightarrow$ require a factor estimation step A benchmark method: extract factors that are significant drivers of variation in $\\boldsymbol{X}$ and then uses these to forecast $\\boldsymbol{y}$. Motivations\nthe factors that are relevant to $\\boldsymbol{y}$ may be a strict subset of all the factors driving $\\boldsymbol{X}$ The method, called the three-pass regression filter (3PRF), selectively identifies only the subset of factors that influence the forecast target while discarding factors that are irrelevant for the target but that may be pervasive among predictors The 3PRF has the advantage of being expressed in closed form and virtually instantaneous to compute. Contributions\ndevelop asymptotic theory for the 3PRF verify the finite sample accuracy of the asymptotic theory compare the 3PRF to other methods provide empirical support for the 3PRF’s strong forecasting performance 2. The three-pass regression filter 2.1. The estimator The environment for 3PRF There is a target variable which we wish to forecast. There exist many predictors which may contain information useful for predicting the target variable. The number of predictors $N$ may be large and number near or more than the available time series observations $T$, which makes OLS problematic. Therefore we look to reduce the dimension of predictive information $\\Rightarrow$ assume the data can be described by an approximate factor model. In order to make forecasts, the 3PRF uses proxies: These are variables, driven by the factors (and as we emphasize below, driven by target-relevant factors in particular), which we show are always available from the target and predictors themselves the econometrician on the basis of economic theory. The target is a linear function of a subset of the latent factors plus some unforecastable noise. The optimal forecast therefore comes from a regression on the true underlying relevant factors. However, since these factors are unobservable, we call this the infeasible best forecast. $$ \\boldsymbol{y} = \\boldsymbol{Z} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} $$\n$\\boldsymbol{y}\\in\\mathbb{R}^{T \\times 1}$: the target variable time series from $2,3, \\ldots, T+1$ $\\boldsymbol{X}\\in\\mathbb{R}^{T \\times N}$: the predictors that have been standardized to have unit time series variance. Temporal dimension: $\\boldsymbol{X}=\\left(\\boldsymbol{x}_1^{\\prime}, \\boldsymbol{x}_2^{\\prime}, \\ldots, \\boldsymbol{x}_T^{\\prime}\\right)^{\\prime}$ Cross section: $\\boldsymbol{X}=\\left(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N\\right)$ $\\boldsymbol{Z}\\in\\mathbb{R}^{T \\times L}$: stacks period-by-period proxy data as $\\boldsymbol{Z}=\\left(\\boldsymbol{z}_1^{\\prime}, \\boldsymbol{z}_2^{\\prime}, \\ldots, \\boldsymbol{z}_T^{\\prime}\\right)^{\\prime}$ Make no assumption on the relationship between $N$ and $T$ but assume $L \\ll \\min (N, T)$ Construct the 3PRF\nPass Description 1. Run time series regression of $\\mathbf{x}_i$ on $\\boldsymbol{Z}$ for $i=1, \\ldots, N$, $x_{i, t}=\\phi_{0, i}+\\boldsymbol{z}^{\\prime} \\boldsymbol{\\phi}_i+\\epsilon_{i t}$, retain slope estimate $\\hat{\\boldsymbol{\\phi}}_i$. 2. Run cross section regression of $\\boldsymbol{x}_t$ on $\\hat{\\boldsymbol{\\phi}}_i$ for $t=1, \\ldots, T$, $x_{i, t}=\\phi_{0, t}+\\hat{\\boldsymbol{\\phi}}^{\\prime} \\boldsymbol{F}_t+\\varepsilon_{i t}$, retain slope estimate $\\hat{\\boldsymbol{F}}_t$. 3. Run time series regression of $y_{t+1}$ on predictive factors $\\hat{\\boldsymbol{F}}_t$, $y_{t+1}=\\beta_0+\\hat{\\boldsymbol{F}}^{\\prime} \\boldsymbol{\\beta}+\\eta_{t+1}$, delivers forecast $\\hat{y}_{t+1}$. Pass 1 : the estimated coefficients describe the sensitivity of the predictor to factors represented by the proxies. Pass 2 : first-stage coefficient estimates map the cross-sectional distribution of predictors to the latent factors. Second-stage cross section regressions use this map to back out estimates of the factors at each point in time. Pass 3 : This is a single time series forecasting regression of the target variable $y_{t +1}$ on the second-pass estimated predictive factors …","date":1700489805,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700489805,"objectID":"cb83c79cd79d6d0dabc1fa683061dca9","permalink":"https://ikerlz.github.io/notes/3prf/","publishdate":"2023-11-20T22:16:45+08:00","relpermalink":"/notes/3prf/","section":"notes","summary":"Table of Contents 1. Introduction 2. The three-pass regression filter 2.1. The estimator 2.2. Assumptions 2.3. Consistency 2.4. Asymptotic distributions 2.5. Proxy selection 3. Related procedures 3.1. Constrained least squares 3.","tags":[],"title":"Notes on ''The three-pass regression filter: A new approach to forecasting using many predictors''","type":"notes"},{"authors":[],"categories":["factor-model"],"content":" Table of Contents 0. Background 1. Introduction 2. Large dimensional factor models with random missing 2.1. EM estimation 3. Determining the number of factors via cross validation 4. Monte Carlo simulations 5. Empirical application: Forecasting macroeconomic variables 0. Background 1. Introduction Factor model in balanced panel has been thoroughly investigated. How to handle the missing data problem in factor models ? the expectation–maximization (EM) algorithm the Kalman filter (KF) There is no formal study of the asymptotic properties for the EM estimators of the factors and factor loadings for the PC estimation with missing observations 2. Large dimensional factor models with random missing 2.1. EM estimation 3. Determining the number of factors via cross validation 4. Monte Carlo simulations 5. Empirical application: Forecasting macroeconomic variables ","date":1699971405,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699971405,"objectID":"b7b894a0969bba15a9144064f9b5af4d","permalink":"https://ikerlz.github.io/notes/factor-model/","publishdate":"2023-11-14T22:16:45+08:00","relpermalink":"/notes/factor-model/","section":"notes","summary":"Table of Contents 0. Background 1. Introduction 2. Large dimensional factor models with random missing 2.1. EM estimation 3. Determining the number of factors via cross validation 4. Monte Carlo simulations 5.","tags":[],"title":"Notes on ''On factor models with random missing: EM estimation, inference, and cross validation''","type":"notes"},{"authors":[],"categories":["causal-inference"],"content":" You can locate the original blog post on observablehq.com, written by Herb Susmann.\nTable of Contents 1. Functional 2. Pathwise Derivatives 3. One-step Estimator 4. Pathwise differentiability 5. Influence function 5.1 The Gâteaux derivative 5.2 The influence function References 1. Functional A functional (you could also call it a parameter) is any function that takes in a probability distribution and returns a number (or multiple numbers). For example, the mean is a functional: given a probability distribution, you compute $\\mathbb{E}(X)=\\int x p(x) d x$ and get back a number. Mathematically, we can define the functional as $T(P):\\mathscr{P} \\rightarrow \\mathbb{R}^q$, where ${\\color{red}\\mathscr{P}=\\{P\\}}$ is the family of the distributions.\nA toy example $$ T(P)=\\int p(x)^2 d x\\qquad(\\star) $$\nThis functional doesn’t usually have any practical significance Other functionals might be the average treatment effect in causal inference the probability of survival after a certain time point in survival analysis. For the true data generating distribution $P$ (here, we can use $N(0,1)$), the value of this squared density parameter is $T(P)=0.282$. However, we don’t know the true data generating distribution and we need to estimate it. Once we have an estimate of the distribution, we can “plug it in” to the formula for the functional to get an estimate for the parameter.\nTake 100 samples from the true data generating distribution and use a kernel density estimator to obtain the estimated distribution $\\tilde{P}$, with a corresponding density $\\tilde{p}$. The following FIGURE shows how the estimated $\\tilde{P}$ compares to the truth, $P$.\nThe difference between $\\tilde{P}$ and the truth $P$ The estimate of the functional $(\\star)$ using this estimated distribution is $T(\\tilde P)=0.282$ (off by 1.5% compared to the true value).\nHow would our estimate of the functional change if we could nudge our estimated distribution towards the true distribution ?\nWe define a family of distributions $P_\\epsilon$ which are mixtures of the estimated distribution and the true distribution. The density of $p_\\epsilon$ is given by $$ p_\\epsilon=(1-\\epsilon) p+\\epsilon \\tilde{p} $$ where $\\epsilon\\in[0,1]$. For every value of $\\epsilon$ we can compute a corresponding value of the functional: $T\\left(P_\\epsilon\\right)$. These values trace out a trajectory as they move from the initual guess, $T(\\tilde{P})$, to the true value, $T(P)$, as shown in FIGURE below.\nThe difference between $\\tilde{P}$ and the truth $P$ 2. Pathwise Derivatives A pathwise derivative is the derivative of $T\\left(P_\\epsilon\\right)$ with respect to $\\epsilon$.\nIt’s useful to think of a derivative as a direction: for each value of $\\epsilon$, the pathwise derivative tells us which direction the functional $T\\left(P_\\epsilon\\right)$ is moving.\nThe pathwise derivative at $\\epsilon=1$ When $\\epsilon=1, \\tilde{P}$ is the estimated distribution. The pathwise derivative at that point tells us how our estimate of $T(\\tilde{P})$ would change if we nudged our estimate in the right direction, towards the true distribution.\n3. One-step Estimator We can use this to form a better estimate of the functional by using this pathwise derivative to approximate the trajectory as a linear function (shown in the following FIGURE). This is called a “one-step” estimator, because it’s like performing a single step of Newton’s method.\nThe one-step estimate In any practical setting, we would need to use the observed data to estimate the pathwise derivative. One-step estimators don’t respect any bounds on the target functional, which can lead to nonsensical results. An alternative family of estimators, Targeted Maximum Likelihood Estimators (TMLE), provide one way around this problem 4. Pathwise differentiability A functional is called pathwise differentiable if it’s possible to compute it’s pathwise derivative.\nThis is a desirable property:\nit allows for the creation of one-step estimators it also allows for other techniques like Targeted Maximum Likelihood Estimation (TMLE). Fortunately, many parameters of real-world interest are pathwise differentiable: for example:\nthe average treatment effect for a binary intervention is a pathwise differentiable parameter.\n5. Influence function The term influence function comes from its origins in robust statistics, where it was developed to quantify how much an estimator will change when its input changes (in other words, how much influence each input data point has on the outcome). In robust statistics, they’re mostly concerned with what happens when your data is perturbed in a bad direction, so they can understand how things like outliers will impact an estimator.\nIn our case, we are more interested in how estimators change when our estimates are nudged in the right direction, towards the true data distribution. Notes of Statistical functionals and influence functions\nIs the plug-in estimator $T(\\tilde F)$ a good estimator?\nThe Glivenko-Cantelli …","date":1699193805,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699193805,"objectID":"dc5bcf9e36aac07100e67e43a919dc45","permalink":"https://ikerlz.github.io/notes/path-dev/","publishdate":"2023-11-05T22:16:45+08:00","relpermalink":"/notes/path-dev/","section":"notes","summary":"You can locate the original blog post on observablehq.com, written by Herb Susmann.\nTable of Contents 1. Functional 2. Pathwise Derivatives 3. One-step Estimator 4. Pathwise differentiability 5. Influence function 5.","tags":[],"title":"Notes on ''One-step Estimators and Pathwise Derivatives''","type":"notes"},{"authors":[],"categories":["causal-inference"],"content":" Table of Contents 0. Background: Semiparametric Model 1. Introduction 2. The Pathwise Derivative Formula For the Asymptotic Variance Regular path \u0026amp; regular estimator The pathwise derivative asymptotic variance formula 3. Semiparametric $M$-estimators When will the adjustment term be zero ? A more direct condition 4. Functions of Mean-square Projection and Densities 5. Regularity Conditions 6. Series Estimation of Projection Functionals 7. Power Series Estimators For Semiparametric Individual Effects and Average Derivatives References 0. Background: Semiparametric Model Reference: Lecture notes from Prof. Bodhisattva Sen (Columbia University)\nA semiparametric model is a statistical model that involves both parametric and nonparametric (infinite-dimensional) components. However, we are mostly interested in estimation and inference of a finite-dimensional parameter in the model.\nExample 1 (population mean) Suppose that $X_1, \\ldots, X_n$ are i.i.d. $P$ belonging to the class $\\mathcal{P}$ of distribution. Let $\\psi(P) \\equiv \\mathbb{E}_P\\left[X_1\\right]$, the mean of the distribution, be the parameter of interest.\nQuestion:\nSuppose that $\\mathcal{P}$ is the class of all distributions that have a finite variance. What is the most efficient estimator of $\\psi(P)$, i.e., what is the estimator with the best asymptotic performance? A model $\\mathcal{P}$ is simply a collection of probability distributions for the data we observe. Example 2 (partial linear regression model) Suppose that we observe i.i.d. data $\\left\\{X_i \\equiv\\left(Y_i, Z_i, V_i\\right): i=1, \\ldots, n\\right\\}$ from the following partial linear regression model: $$ Y_i=Z_i^{\\top} \\beta+g\\left(V_i\\right)+\\epsilon_i $$\n$Y_i$ is the scalar response variable $Z_i$ and $V_i$ are vectors of predictors $g(\\cdot)$ is the unknown (nonparametric) function $\\epsilon_i$ is the unobserved error. For simplicity and to focus on the semiparametric nature of the problem:\nAssume that $\\left(Z_i, V_i\\right) \\sim f(\\cdot, \\cdot)$, where we assume that the density $f(\\cdot, \\cdot)$ is known, is independent of $\\epsilon_i \\sim N\\left(0, \\sigma^2\\right)$ (with $\\sigma^2$ known). The model, under these assumptions, has a parametric component $\\beta$ and a nonparametric component $g(\\cdot)$ “Separated semiparametric model”: We say that the model $\\mathcal{P}=\\left\\{P_{\\nu, \\eta}\\right\\}$ is a “separated semiparametric model”, where $\\nu$ is a “Euclidean parameter” and $\\eta$ runs through a nonparametric class of distributions (or some infinite-dimensional set). This gives a semiparametric model in the strict sense, in which we aim at estimating $\\nu$ and consider $\\eta$ as a nuisance parameter.\n“Frequent questions for semiparametric model”: consider the estimation of a parameter of interest $\\nu=\\nu(P)$, where the data has distribution $P \\in \\mathcal{P}$:\n(Q1) How well can we estimate $\\nu=\\nu(P)$ ? What is our “gold standard”? (Q2) Can we compare absolute “in principle” standards for estimation of $\\nu$ in a model $\\mathcal{P}$ with estimation of $\\nu$ in a submodel $\\mathcal{P}_0 \\subset \\mathcal{P}$ ? What is the effect of not knowing $\\eta$ on estimation of $\\nu$ when $\\mathcal{P}=\\left\\{P_\\theta: \\theta \\equiv(\\nu, \\eta) \\in \\Theta\\right\\}$ ? (Q3) How do we construct efficient estimators of $\\nu(P)$ ? A model $\\mathcal{P}$ is simply a collection of probability distributions for the data we observe. 1. Introduction Develop a general form for the asymptotic variance of semiparametric estimators that depend on nonparametric estimators of functions. The formula is often straightforward to derive, requiring only some calculus. Although the formula is not based on primitive conditions, it should be useful for semiparametric estimators, just as analogous formulae are for parametric estimators. The formula gives the form of remainder terms, which facilitates specification of primitive conditions. It can also be used to make asymptotic efficiency comparisons and to find an efficient estimator in some class. - - Derive the formula: **Section 2** - Propositions about semiparametric estimator - **Section 3** - **Section 4** - High-level regularity conditions: **Section 5** - Conditions for $\\sqrt{n}$-consistency and asymptotic normality: **Section 6** - Primitive conditions for the examples: **Section 7** 2. The Pathwise Derivative Formula For the Asymptotic Variance Preliminary: one-step estimators and pathwise derivatives\nThe formula is based on the observation that $\\sqrt{n}$-consistent nonparametric estimators are often efficient.\nFor example, the sample mean is known to be an efficient estimator of the population mean in a nonparametric model where no restrictions, other than regularity conditions (e.g. existence of the second moment) are placed on the distribution of the data.\nIdea\nCalculate the asymptotic variance of a semiparametric estimator as the variance bound for the functional that it nonparametrically estimates. In other words, the formula is the variance bound for …","date":1699193805,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699193805,"objectID":"a6e05984cc9ab3ef73cd3c315289272a","permalink":"https://ikerlz.github.io/notes/var-semiparam/","publishdate":"2023-11-05T22:16:45+08:00","relpermalink":"/notes/var-semiparam/","section":"notes","summary":"Table of Contents 0. Background: Semiparametric Model 1. Introduction 2. The Pathwise Derivative Formula For the Asymptotic Variance Regular path \u0026 regular estimator The pathwise derivative asymptotic variance formula 3.","tags":[],"title":"Notes on ''The Asymptotic Variance of Semiparametric Estimators''","type":"notes"},{"authors":["Zhe Li"],"categories":["Classical Statistics"],"content":" Table of Contents 一、生成混合高斯数据 1、原理 2、Python实现 3、R实现 二、EM算法 1、原理 2、Python实现 3、R实现 4、第三方工具库 三、真实数据 1、Python实现 2、R实现 四、后记 一、生成混合高斯数据 1、原理 利用混合高斯模型的公式：\n$$ P(y \\mid \\theta)=\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y \\mid \\theta_{k}\\right) $$\n只需要给定：\n各组分布的均值向量及协方差矩阵$\\theta_k = \\left(\\mu_k, \\Sigma_k \\right)$ 需要生成的数据总数$N$以及各组权重$\\alpha = (\\alpha_1,\\ldots,\\alpha_K)$ 即可生成GMM数据 注意：$\\Sigma_k$需要是一个对称正定的矩阵\n两种思路：\n思路一 利用权重$\\alpha = (\\alpha_1,\\ldots,\\alpha_K)$进行带权重的抽样，从$[1,2,\\ldots,K]$中抽样$N$次得到一个长度为$N$的列表（向量），记为group_list，第$i$个元素表示数据应该从第$i$个分模型中生成 遍历列表（向量）group_list中的每个元素，若第$i$次遍历中，group_list[i]=k，则表示从$\\mathcal{N}(\\mu_k,\\Sigma_k)$中生成一个数据$X_i$ 将$X_1,\\ldots,X_N$排列合并得到一个矩阵$X=(X_1,\\ldots,X_N)$ 思路二 利用$\\alpha = (\\alpha_1,\\ldots,\\alpha_K)$，乘上总数$N$，得到每个类别大致的数量，记为group_num_list，第$k$个元素表示有group_num_list[k]个数据来源于第$k$个分模型 遍历列表（向量）group_num_list，若在第$k$次遍历中，group_num_list[k]=$n_k$，则表示从$\\mathcal{N}(\\mu_k,\\Sigma_k)$中独立生成$n_k$个数据$X^k=(X_1,\\ldots,X_{n_k})$ 将$X^1,\\ldots,X^k$合并得到矩阵$X=(X_1,\\ldots,X_N)$ 注意：思路一的方法是严格按照GMM的定理来生成的，复杂度为$O(N)$；思路二严格来说与理论有少许差异，但是时间复杂度会低一些，为$O(K)$\n2、Python实现 def generate_gmm_data(self, shuffle=True): \u0026#34;\u0026#34;\u0026#34; :return: 返回一个自变量矩阵X_mat以及一个因变量向量y_vec \u0026#34;\u0026#34;\u0026#34; assert len(self.weight_vec) == self.K # 判断类别数量是否等于参数的数量 num_list = [int(self.total_num * self.weight_vec[k]) for k in range(self.K)] if sum(num_list) != self.total_num: num_list[self.K] += self.total_num - sum(num_list) # 取整后可能存在加和与总数不等的情况，考虑在最后一个组上做处理 assert sum(num_list) == self.total_num for k in range(self.K): if len(self.mean_list[k]) \u0026gt; 1: X_mat_k = np.random.multivariate_normal(self.mean_list[k], self.cov_mat_list[k], num_list[k]) else: X_mat_k = np.random.normal(self.mean_list[k], self.cov_mat_list[k], num_list[k]).reshape( (num_list[k], -1)) y_vec_k = np.array([k for _ in range(num_list[k])]) if not k: X_mat = X_mat_k y_vec = y_vec_k else: X_mat = np.vstack((X_mat, X_mat_k)) y_vec = np.hstack((y_vec, y_vec_k)) if shuffle: shuffle_index = [x for x in range(self.total_num)] random.shuffle(shuffle_index) X_mat = X_mat[shuffle_index] # if len(X_mat.shape) == 3: # X_mat = X_mat[0] # 维度为1的情况会升维 y_vec = y_vec[shuffle_index] self.data = X_mat self.y_vec = y_vec 3、R实现 generate_gmm_data \u0026lt;- function (params_list=NULL, shuffle=TRUE) { # params_list是一个list类型，应当包含以下几个数据 # total_num: 需要生成数据的数量 # weight_vec: 各类别的权重 # mean_list: 均值列表 # cov_mat_list: 协方差矩阵列表 if (is.list(params_list)) { weight_vec \u0026lt;- params_list$weight_vec total_num \u0026lt;- params_list$total_num mean_list \u0026lt;- params_list$mean_list cov_mat_list \u0026lt;- params_list$cov_mat_list cluster_num \u0026lt;- length(weight_vec) } else { # 如果没有给定参数列表，则使用默认设置 weight_vec \u0026lt;- c(0.3, 0.3, 0.4) total_num \u0026lt;- 100 mean_list \u0026lt;- list(c(-0.5), c(0.5), c(0)) cov_mat_list \u0026lt;- list(matrix(1), matrix(1), matrix(1)) cluster_num \u0026lt;- 3 print(\u0026#34;hhh\u0026#34;) } # 判断cluster数量与mean_list和cov_mat_list中元素个数是否相等 if (length(weight_vec) != cluster_num) { stop(\u0026#34;The length of weight vector is not equal to number of cluster\u0026#34;) } if (length(mean_list) != cluster_num) { stop(\u0026#34;The length of mean list is not equal to number of cluster\u0026#34;) } if (length(cov_mat_list) != cluster_num) { stop(\u0026#34;The length of covariance matrix list is not equal to number of cluster\u0026#34;) } # 各类别的数量 num_before_k \u0026lt;- 0 for (k in 1:cluster_num) { if (k \u0026lt; cluster_num) { num_k \u0026lt;- floor(weight_vec[k] * total_num) # 采用向下取整的策略，并在最后一组做处理 num_before_k \u0026lt;- num_before_k + num_k } else { num_k \u0026lt;- total_num - num_before_k } X_mat_k \u0026lt;- rmvnorm(num_k, mean_list[[k]], cov_mat_list[[k]]) # 生成一个num_k * p 维的matrix y_vec_k \u0026lt;- rep(k, num_k) if (k == 1) { X_mat \u0026lt;- X_mat_k y_vec \u0026lt;- y_vec_k } else { # 纵向合并 X_mat \u0026lt;- rbind(X_mat, X_mat_k) y_vec \u0026lt;- c(y_vec, y_vec_k) } } # shuffle if (shuffle) { shuffle_index \u0026lt;- sample(total_num) X_mat \u0026lt;- X_mat[shuffle_index, ] y_vec \u0026lt;- y_vec[shuffle_index] } # 构造返回结果 res \u0026lt;- list(X_mat=as.matrix(X_mat), y_vec=y_vec) return(res) } 由于GMM类似于聚类模型，因此可以使用Python中Sklearn库中的make_blobs函数快速生成数据\n(X, y) = make_blobs(n_samples=[100,300,250,400], n_features=2, centers=[[100,150],[250,400], [600,100],[300,500]], cluster_std=50, random_state=1) plt.style.use(\u0026#34;ggplot\u0026#34;) plt.figure() plt.title(\u0026#34;Data\u0026#34;) plt.scatter(X[:, 0], X[:,1], marker=\u0026#34;o\u0026#34;, c=np.squeeze(y), s=30) 利用Sklearn生成的聚类数据 二、EM算法 1、原理 GMM的EM实现使用的是传统的EM算法框架，Python实现中，主要使用了Numpy做矩阵运算，R实现中，其自带的矩阵运算已经能满足要求，不需要另外导包 由于EM算法本身是一个迭代求解算法，因此需要给出终止条件，在本文的实现中，使用了两个终止条件： 最大迭代次数：max_iter 对数似然更新阈值：eps print_log参数用于控制是否输出每次迭代的信息 思路 首先进行初始化，即初始化$\\alpha_k$、$\\mu_k$、$\\Sigma_k$；可以采取多种初始化方法：\nkmeans方法（第三方包常用） $\\alpha_k$初始化为$\\frac{1}{K}$，$\\mu_k$选择$K$个数据点的值作为初始化的值，$\\Sigma_k$可以选择整体的协方差矩阵作为初始化值 E-Step：利用下式更新$\\Gamma\\in \\mathbb{R}^{N\\times K}$\n$$ \\widehat{\\gamma}_{j k}=\\frac{\\alpha_{k} \\phi\\left(y_{j} \\mid \\theta_{k}\\right)}{\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y_{j} \\mid \\theta_{k}\\right)} $$\nM-Step：利用下式更新待估计的参数 $$ \\widehat{\\mu}_{k}=\\frac{\\sum_{j=1}^{N} \\widehat{\\gamma}_{j k} y_{j}}{\\sum_{j=1}^{N} \\widehat{y}_{j k}}\\quad \\widehat{\\sigma}_{k}^{2}=\\frac{\\sum_{j=1}^{N} \\widehat{\\gamma}_{j k}\\left(y_{j}-\\mu_{k}\\right)^{2}}{\\sum_{j=1}^{N} \\widehat{\\gamma}_{j k}}\\quad \\widehat{\\alpha}_{k}=\\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{j …","date":1698856767,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698856767,"objectID":"a90f6c4f605f68e7632803e124f38604","permalink":"https://ikerlz.github.io/post/gmm/","publishdate":"2023-11-02T00:39:27+08:00","relpermalink":"/post/gmm/","section":"post","summary":"Table of Contents 一、生成混合高斯数据 1、原理 2、Python实现 3、R实现 二、EM算法 1、原理 2、Python实现 3、R实现 4、第三方工具库 三、真实数据 1、Python实现 2、R实现 四、后记 一、生成混合高斯数据 1、原理 利用混合高斯模型的公式：\n$$ P(y \\mid \\theta)=\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y \\mid \\theta_{k}\\right) $$\n只需要给定：","tags":["Code","GMM","Python","R"],"title":"混合高斯模型的代码实现","type":"post"},{"authors":["Zhe Li"],"categories":["Causal Inference"],"content":" Using the Propensity Score in Regressions for Causal Effects $$ \\begin{aligned} \\ \\end{aligned} $$\nLi Zhe $$$$\nSchool of Data Science, Fudan University $$$$\nNovember 1, 2023 Introduction This chapter discusses two simple methods to use the propensity score:\nthe propensity score as a covariate in regressions running regressions weighted by the inverse of the propensity score Reasons:\nthey are easy to implement, which involve only standard statistical software packages for regressions; their properties are comparable to many more complex methods; they can be easily extended to allow for flexible statistical models including machine learning algorithms. Outline Regressions with the propensity score as a covariate Theorem 14.1 Proposition 14.1 $$$$\nRegressions weighted by the inverse of the propensity score Average causal effect Theorem 14.2 Average causal effect on the treated units Table 14.1 Proposition 14.2 Theorem 14.3 Regressions with the propensity score as a covariate $$ \\text { Theorem 11.1 If } Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid X \\text {, then } {\\color{red}Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid e(X)} \\text {. } $$\nBy Theorem 11.1, if unconfoundedness holds conditioning on $X$, then it also holds conditioning on $e(X)$: $\\color{red}{Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid e(X) }.$ Analogous to (10.5), $\\tau$ is also nonparametrically identified by $$ \\tau=E[E\\{Y \\mid Z=1, e(X)\\}-E\\{Y \\mid Z=0, e(X)\\}], $$ $\\Rightarrow$ The simplest regression specification is the OLS fit of $Y$ on $\\{1, Z, e(X)\\}$, with the coefficient of $Z$ as an estimator, denoted by $\\tau_e$: $$ \\arg \\min _{a, b, c} E\\{Y-a-b Z-c e(X)\\}^2 $$ $\\tau_e$ defined as the coefficient of $Z$. It is consistent for $\\tau$ if have a correct propensity score model the outcome model is indeed linear in $Z$ and $e(X)$ $\\tau_e$ estimates $\\tau_{\\mathrm{O}}$ if we have a correct propensity score model even if the outcome model is completely misspecified Regressions with the propensity score as a covariate Theorem 14.1 If $Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid X$, then the coefficient of $Z$ in the OLS fit of $Y$ on $\\{1, Z, e(X)\\}$ equals $$ \\tau_e=\\tau_{\\mathrm{O}}=\\frac{E\\left\\{h_{\\mathrm{O}}(X) \\tau(X)\\right\\}}{E\\left\\{h_{\\mathrm{O}}(X)\\right\\}}, $$ recalling that $h_{\\mathrm{O}}(X)=e(X)\\{1-e(X)\\}$ and $\\tau(X)=E\\{Y(1)-Y(0) \\mid X\\}$.\n$$ \\begin{aligned} \\ \\ \\end{aligned} $$\nCorollary 14.1 If $Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid X$, then\nthe coefficient of $Z-e(X)$ in the OLS fit of $Y$ on $Z-e(X)$ or $\\{1, Z-e(X)\\}$ equals $\\tau_{\\mathrm{O}}$; the coefficient of $Z$ in the OLS fit of $Y$ on $\\{1, Z, e(X), X\\}$ equals $\\tau_{\\mathrm{O}}$. Regressions with the propensity score as a covariate An unusual feature of Theorem 14.1 is that the overlap condition ($0 \u0026lt; e(x) \u0026lt; 1$) is not needed any more. Even if some units have propensity score $e(X)$ equaling 0 or 1, their associate weight $e(X)\\{1-e(X)\\}$ is zero so that they do not contribute anything to the final parameter $\\tau_O$. Frisch–Waugh–Lovell Theorem The Frisch–Waugh–Lovell (FWL) theorem reduces multivariate OLS to univariate OLS and therefore facilitate the understanding and calculation of the OLS coefficients.\nTheorem A2.2 (sample FWL) With data $\\left(Y, X_1, X_2, \\ldots, X_p\\right)$ containing column vectors, the coefficient of $X_1$ equals the coefficient of $\\tilde{X}_1$ in the OLS fit of $Y$ or $\\tilde{Y}$ on $\\tilde{X}_1$, where\n$\\tilde{Y}$ is the residual vector from the OLS fit of $Y$ on $\\left(X_2, \\ldots, X_p\\right)$ $\\tilde{X}_1$ is the residual from the OLS fit of $X_1$ on $\\left(X_2, \\ldots, X_p\\right)$. $$$$\nBased on the FWL theorem, we can obtain $\\tau_e$ in two steps:\nfirst, we obtain the residual $\\tilde{Z}$ from the OLS fit of $Z$ on ${1, e(X)}$; then, we obtain $\\tau_e$ from the OLS fit of $Y$ on $\\tilde{Z}$. Proof of Theorem 14.1 The coefficient of $e(X)$ in the OLS fit of $Z$ on $\\{1, e(X)\\}$ is $$ \\begin{aligned} \\frac{\\operatorname{cov}\\{Z, e(X)\\}}{\\operatorname{var}\\{e(X)\\}} \u0026amp; =\\frac{E[\\operatorname{cov}\\{Z, e(X) \\mid X\\}]+\\operatorname{cov}\\{E(Z \\mid X), e(X)\\}}{\\operatorname{var}\\{e(X)\\}} \\\\ \u0026amp;=\\frac{0+\\operatorname{var}\\{e(X)\\}}{\\operatorname{var}\\{e(X)\\}}=1, \\end{aligned} $$\nthe intercept is $E(Z)-E\\{e(X)\\}=0$ the residual is $\\tilde{Z}=Z-e(X)$ (This makes sense since $Z-e(X)$ is uncorrelated with any function of $X$). Therefore, we can obtain $\\tau_e$ from the univariate OLS fit of $Y$ on $Z-e(X)$ : $$\\small{\\tau_e=\\frac{\\operatorname{cov}\\{Z-e(X), Y\\}}{\\operatorname{var}\\{Z-e(X)\\}}}$$ The denominator simplifies to $$ \\begin{aligned} \\operatorname{var}\\{Z-e(X)\\} \u0026amp; =E\\{Z-e(X)\\}^2 =e(X)+e(X)^2-2 e(X)^2=h_{\\mathrm{O}}(X) \\end{aligned} $$\nProof of Theorem 14.1 The numerator simplifies to $$ \\begin{aligned} \u0026amp; \\operatorname{cov}\\{Z-e(X), Y\\} \\\\ = \u0026amp; E[\\{Z-e(X)\\} Y] \\\\ = \u0026amp; E[\\{Z-e(X)\\} Z Y(1)]+E[\\{Z-e(X)\\}(1-Z) Y(0)] \\\\ \u0026amp; \\quad \\quad \\quad{\\color{red}(\\text { since } Y=Z Y(1)+(1-Z) Y(0))} \\\\ = \u0026amp; E[\\{Z-Z e(X)\\} Y(1)]-E[e(X)(1-Z) Y(0)] \\\\ = \u0026amp; E[Z\\{1-e(X)\\} …","date":1698537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698537600,"objectID":"c9dd82d5ccaec073de3ceacb1b43d0f9","permalink":"https://ikerlz.github.io/slides/causalinferencechapter14/","publishdate":"2023-10-29T00:00:00Z","relpermalink":"/slides/causalinferencechapter14/","section":"slides","summary":"Chapter 14 in \"A First Course in Causal Inference\".","tags":[],"title":"Using the Propensity Score in Regressions for Causal Effects","type":"slides"},{"authors":["Shihao Wu","Zhe Li","Xuening Zhu"],"categories":null,"content":" ","date":1685145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685145600,"objectID":"d036fcb022a3b191f278fccae5a46f36","permalink":"https://ikerlz.github.io/publication/dcd/","publishdate":"2023-07-19T00:00:00Z","relpermalink":"/publication/dcd/","section":"publication","summary":"Community detection for large scale networks is of great importance in modern data analysis. In this work, we develop a distributed spectral clustering algorithm to handle this task. Specifically, we distribute a certain number of pilot network nodes on the master server and the others on worker servers. A spectral clustering algorithm is first conducted on the master to select pseudo centers. Next, the indexes of the pseudo centers are broadcasted to workers to complete the distributed community detection task using an SVD (singular value decomposition) type algorithm. The proposed distributed algorithm has three advantages. First, the communication cost is low, since only the indexes of pseudo centers are communicated. Second, no further iterative algorithm is needed on workers while a “one-shot” computation suffices. Third, both the computational complexity and the storage requirements are much lower compared to using the whole adjacency matrix. We develop a Python package DCD (The Python package is provided in [https://github.com/Ikerlz/dcd](https://github.com/Ikerlz/dcd).) to implement the distributed algorithm on a Spark system and establish theoretical properties with respect to the estimation accuracy and mis-clustering rates under the stochastic block model. Experiments on a variety of synthetic and empirical datasets are carried out to further illustrate the advantages of the methodology.","tags":null,"title":"A Distributed Community Detection Algorithm for Large Scale Networks Under Stochastic Block Models","type":"publication"}]