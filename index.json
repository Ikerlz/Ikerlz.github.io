
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Welcome! I am currently a PhD Student in School of Data Science at Fudan University, under the supervision of Prof. Xuening Zhu. My research interests include econometrics, distributed statistical modelling \u0026amp; inference, network data modelling.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Welcome! I am currently a PhD Student in School of Data Science at Fudan University, under the supervision of Prof. Xuening Zhu. My research interests include econometrics, distributed statistical modelling \u0026 inference, network data modelling.","tags":null,"title":"Li Zhe","type":"authors"},{"authors":["Zhe Li"],"categories":["Classical Statistics"],"content":" Table of Contents 1、简介 2、parallel包 2.1 mclapply 2.2 parLapply 3、foreach包与doParallel包 4、future包 5 、furrr包 6、RcppParallel包 7 、GPU 加速 8 、性能比较 9 、结论 1、简介 R语言提供了多种并行计算的方法，可以显著提高计算密集型任务的执行速度。本文章只简单介绍几种常用的并行计算方法，并提供示例代码，最后对比几种方法。\n2、parallel包 parallel包是R中最基本和广泛使用的并行计算包。它是R基础安装的一部分，无需额外安装。\n2.1 mclapply mclapply 函数适用于类 Unix 系统（Mac 、Linux等），使用 fork 机制创建子进程。\nlibrary(parallel) # 创建一个耗时的函数 time_consuming_function \u0026lt;- function(x) { Sys.sleep(1) # 模拟耗时操作 return(x^2) } # 设置核心数 num_cores \u0026lt;- detectCores() - 1 # 使用 mclapply system.time({ result \u0026lt;- mclapply(1:10, time_consuming_function, mc.cores = num_cores) }) 优点：\n简单易用，语法类似于 lapply 在 Unix-like 系统上效率高 缺点：\n不支持 Windows 系统 不适合需要共享大量数据的任务 2.2 parLapply parLapply 函数适用于所有操作系统，包括 Windows。\n# 创建集群 cl \u0026lt;- makeCluster(num_cores) # 使用 parLapply system.time({ result \u0026lt;- parLapply(cl, 1:10, time_consuming_function) }) # 停止集群 stopCluster(cl) 优点：\n跨平台兼容（包括 Windows） 适合需要在节点间共享大量数据的任务 缺点：\n相比 mclapply，设置稍微复杂 在某些情况下可能比 mclapply 慢 3、foreach包与doParallel包 foreach包提供了一个更直观的并行循环接口。doParallel包为foreach提供了一个并行后端。二者经常同时使用达到并行的目的。\nlibrary(foreach) library(doParallel) # 注册并行后端 registerDoParallel(cores = num_cores) # 使用 foreach system.time({ result \u0026lt;- foreach(i = 1:10, .combine = c) %dopar% { time_consuming_function(i) } }) # 停止并行后端 stopImplicitCluster() 优点：\n语法直观，类似于常规的 for 循环 灵活，可以轻松组合结果 与多种并行后端兼容 缺点：\n需要额外安装包 对于简单任务可能有些过于复杂 4、future包 future包提供了一个统一的并行和分布式处理框架，设计理念先进。\nlibrary(future) library(future.apply) # 设置并行策略 plan(multisession, workers = num_cores) # 使用 future_lapply system.time({ result \u0026lt;- future_lapply(1:10, time_consuming_function) }) 优点：\n提供统一的接口for并行和序列化计算 非常灵活，支持多种并行策略 可以轻松切换不同的并行后端 缺点：\n学习曲线可能较陡 对于简单任务可能显得过于复杂 5 、furrr包 furrr包结合了purrr的函数式编程和future的并行能力。\nlibrary(furrr) # 设置并行策略 plan(multisession, workers = num_cores) # 使用 future_map system.time({ result \u0026lt;- future_map(1:10, time_consuming_function) }) 优点：\n与tidyverse生态系统完美集成 语法简洁，易于使用 保持了purrr的一致性 缺点：\n需要熟悉purrr和函数式编程 对于不使用tidyverse的项目可能不是最佳选择 6、RcppParallel包 RcppParallel包提供了C++级别的并行计算能力。当然，C++本身已经足够快，如果会C++推荐直接写C++。\nlibrary(Rcpp) library(RcppParallel) # 定义 C++ 函数 cppFunction(depends = \u0026#34;RcppParallel\u0026#34;, code = \u0026#39; #include \u0026lt;RcppParallel.h\u0026gt; using namespace RcppParallel; struct SquareWorker : public Worker { const RVector\u0026lt;double\u0026gt; input; RVector\u0026lt;double\u0026gt; output; SquareWorker(const NumericVector input, NumericVector output) : input(input), output(output) {} void operator()(std::size_t begin, std::size_t end) { std::transform(input.begin() + begin, input.begin() + end, output.begin() + begin, [](double x) { return x * x; }); } }; // [[Rcpp::export]] NumericVector parallelSquare(NumericVector x) { NumericVector output(x.size()); SquareWorker worker(x, output); parallelFor(0, x.size(), worker); return output; } \u0026#39;) # 使用 C++ 并行函数 large_vector \u0026lt;- runif(1e7) system.time({ result \u0026lt;- parallelSquare(large_vector) }) 优点：\n性能极高，特别适合计算密集型任务 可以充分利用多核 CPU 缺点：\n需要C++编程知识 开发和调试可能较为复杂 7 、GPU 加速 对于特定类型的计算，可以使用 GPU 加速。例如，使用gpuR包：\nlibrary(gpuR) # 创建 GPU 矩阵 A \u0026lt;- gpuMatrix(rnorm(1000*1000), nrow=1000, ncol=1000) B \u0026lt;- gpuMatrix(rnorm(1000*1000), nrow=1000, ncol=1000) # GPU 矩阵乘法 system.time({ C \u0026lt;- A %*% B }) 优点：\n对于某些类型的计算（如矩阵运算）性能极高 可以处理大规模数据 缺点：\n需要特定的硬件（GPU） 编程模型与 CPU 不同，可能需要学习新的技能 不是所有类型的计算都适合 GPU 8 、性能比较 下面比较了为了比较不同方法的性能，我们可以使用一个统一的测试函数：\n# 安装必要的包（如果尚未安装） if (!requireNamespace(\u0026#34;parallel\u0026#34;, quietly = TRUE)) install.packages(\u0026#34;parallel\u0026#34;) if (!requireNamespace(\u0026#34;foreach\u0026#34;, quietly = TRUE)) install.packages(\u0026#34;foreach\u0026#34;) if (!requireNamespace(\u0026#34;doParallel\u0026#34;, quietly = TRUE)) install.packages(\u0026#34;doParallel\u0026#34;) if (!requireNamespace(\u0026#34;future\u0026#34;, quietly = TRUE)) install.packages(\u0026#34;future\u0026#34;) if (!requireNamespace(\u0026#34;future.apply\u0026#34;, quietly = TRUE)) install.packages(\u0026#34;future.apply\u0026#34;) if (!requireNamespace(\u0026#34;furrr\u0026#34;, quietly = TRUE)) install.packages(\u0026#34;furrr\u0026#34;) if (!requireNamespace(\u0026#34;microbenchmark\u0026#34;, quietly = TRUE)) install.packages(\u0026#34;microbenchmark\u0026#34;) if (!requireNamespace(\u0026#34;ggplot2\u0026#34;, quietly = TRUE)) install.packages(\u0026#34;ggplot2\u0026#34;) # 加载必要的库 library(parallel) library(foreach) library(doParallel) library(future) library(future.apply) library(furrr) library(microbenchmark) library(ggplot2) # 设置随机种子以确保结果可重现 set.seed(123) # 创建一个计算密集型函数 intensive_function \u0026lt;- function(x) { result \u0026lt;- sum(sapply(1:1e5, function(i) sin(x * i))) return(result) } # 创建测试数据 test_data \u0026lt;- runif(100, 0, 1) # 设置核心数，减少到一个较小的值以避免连接限制 num_cores \u0026lt;- min(detectCores() - 1, 20) # 使用最多20个核心 # 定义测试函数 run_test \u0026lt;- function() { # 1. 串行计算（基准） serial \u0026lt;- lapply(test_data, intensive_function) # 2. mclapply mc \u0026lt;- mclapply(test_data, intensive_function, mc.cores = num_cores) # 3. parLapply cl \u0026lt;- makeCluster(num_cores) par \u0026lt;- parLapply(cl, test_data, intensive_function) stopCluster(cl) # 4. foreach cl \u0026lt;- makeCluster(num_cores) registerDoParallel(cl) foreach \u0026lt;- foreach(x = test_data, .combine = c) %dopar% { intensive_function(x) } stopCluster(cl) # 5. future.apply plan(multisession, workers = num_cores) future \u0026lt;- future_lapply(test_data, intensive_function) # 6. furrr plan(multisession, workers = num_cores) furrr \u0026lt;- future_map(test_data, intensive_function) # 检查结果是否一致 all_equal \u0026lt;- …","date":1726763967,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695141567,"objectID":"117388eb247b8d7cd6b8c1619372ded1","permalink":"https://ikerlz.github.io/post/rparallel/","publishdate":"2024-09-20T00:39:27+08:00","relpermalink":"/post/rparallel/","section":"post","summary":"Table of Contents 1、简介 2、parallel包 2.1 mclapply 2.2 parLapply 3、foreach包与doParallel包 4、future包 5 、furrr包 6、RcppParallel包 7 、GPU 加速 8 、性能比较 9 、结论 1、简介 R语言提供了多种并行计算的方法，可以显著提高计算密集型任务的执行速度。本文章只简单介绍几种常用的并行计算方法，并提供示例代码，最后对比几种方法。\n2、parallel包 parallel包是R中最基本和广泛使用的并行计算包。它是R基础安装的一部分，无需额外安装。\n2.1 mclapply mclapply 函数适用于类 Unix 系统（Mac 、Linux等），使用 fork 机制创建子进程。","tags":["Code","并行","R"],"title":"R并行方法简述","type":"post"},{"authors":["Zhe Li"],"categories":["Neural Network"],"content":" Table of Contents 0、MLP 一、Kolmogorov–Arnold Networks 1. Kolmogorov-Arnold Representation theorem 2. KAN Architecture 3. KAN’s Approximation Abilities and Scaling Laws 4. For Interpretability: Simplifying KANs and Making them interactive 二、KAN 的一些问题 参考文献 0、MLP 感知机最早由Rosenblatt于1957年提出，由于其简单的结构而得到快速的发展，下图是一个MLP的示意图 我们可以把上面这个 MLP 表示为： $$ f_{\\mathrm{MLP}}(\\mathbf{x})=\\mathbf{W}_4 \\boldsymbol{\\sigma}\\left(\\mathbf{W}_3 \\boldsymbol{\\sigma}\\left(\\mathbf{W}_2 \\boldsymbol{\\sigma}\\left(\\mathbf{W}_1 \\mathbf{x}+\\mathbf{b}_1\\right)+\\mathbf{b}_2\\right)+\\mathbf{b}_3\\right)+\\mathbf{b}_4, $$ 可以看出，在 MLP 中，激活函数$\\boldsymbol{\\sigma}(\\cdot)$是作用在节点（node）上的，而边（edge）的连接没有附带任何信息，唯一的作用就是把两层中的所有节点连接起来。\nMLP具有很强的拟合能力，常见的连续非线性函数都可以用MLP来近似 但是，我们是否可以把非线性的激活函数放到边上呢？这就是 KAN 的基本思想\n一、Kolmogorov–Arnold Networks 1. Kolmogorov-Arnold Representation theorem Vladimir Arnold与Andrey Kolmogorov证明：如果$f: [0,1]^n \\rightarrow \\mathbb{R}$是一个多元连续函数，则$f$可以被写成单变量连续函数和加法的有限组合。具体而言， $$ f(\\mathbf{x})=f\\left(x_1, \\ldots, x_n\\right)=\\sum_{q=1}^{2 n+1} \\Phi_q\\left(\\sum_{p=1}^n \\phi_{q, p}\\left(x_p\\right)\\right) . $$ 其中，$\\phi_{q, p}:[0,1] \\rightarrow \\mathbb{R}$，$\\Phi_q: \\mathbb{R} \\rightarrow \\mathbb{R}$。\n似乎这个理论告诉我们学习一个高维函数归结为学习多项式个一维函数。然而，这些一维函数可能是非光滑甚至是分形的，因此在实践中可能无法学习到。由于这种病态行为，KA表示定理在机器学习中基本上被判了死刑，被认为在理论上是正确的但在实践中无用\n2. KAN Architecture 根据KA表示定理，我们只需要找到合适的$\\phi_{p,q}$和$\\Phi_q$函数，因此可以设计上图这样的一个神经网络，其中$\\phi_{p,q}$和$\\Phi_q$均为B样条函数。\n所以本质上，KA表示定理其实就是一个两层的KAN，但是如何才能像MLP一样把KAN也做深一些呢？\n我们首先用$n_i$表示第$i$层的节点数量，定义第$l$层的第$i$个节点的激活值为$x_{l,i}$，在第$l$层与$l+1$层共有$n_ln_{l+1}$个激活函数，我们定义连接$(l,i)$这个节点与$(l+1,j)$这个节点的激活函数为 $$ \\phi_{l, j, i}, \\quad l=0, \\cdots, L-1, \\quad i=1, \\cdots, n_l, \\quad j=1, \\cdots, n_{l+1} . $$ 这样就有 $$ \\mathbf{x}_{l+1}\\in\\mathbb{R}^{n_{l+1}}=\\underbrace{\\left(\\begin{array}{cccc} \\phi_{l, 1,1}(\\cdot) \u0026amp; \\phi_{l, 1,2}(\\cdot) \u0026amp; \\cdots \u0026amp; \\phi_{l, 1, n_l}(\\cdot) \\\\ \\phi_{l, 2,1}(\\cdot) \u0026amp; \\phi_{l, 2,2}(\\cdot) \u0026amp; \\cdots \u0026amp; \\phi_{l, 2, n_l}(\\cdot) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ \\phi_{l, n_{l+1}, 1}(\\cdot) \u0026amp; \\phi_{l, n_{l+1}, 2}(\\cdot) \u0026amp; \\cdots \u0026amp; \\phi_{l, n_{l+1}, n_l}(\\cdot) \\end{array}\\right)}_{\\boldsymbol{\\Phi}_l\\in\\mathbb{R}^{n_{l+1}\\times n_l}} \\mathbf{x}_l \\in \\mathbb{R}^{n_l}, $$ $\\boldsymbol{\\Phi}_l$相当于是第$l$层 KAN 的函数矩阵，因此一个$L$层的KAN就可以表示为\n$$ \\operatorname{KAN}(\\mathbf{x})=\\left(\\boldsymbol{\\Phi}_{L-1} \\circ \\boldsymbol{\\Phi}_{L-2} \\circ \\cdots \\circ \\boldsymbol{\\Phi}_1 \\circ \\boldsymbol{\\Phi}_0\\right) \\mathbf{x} $$ 对比 MLP： $$ \\operatorname{MLP}(\\mathbf{x})=\\left(\\mathbf{W}_{L-1} \\circ \\sigma \\circ \\mathbf{W}_{L-2} \\circ \\sigma \\circ \\cdots \\circ \\mathbf{W}_1 \\circ \\sigma \\circ \\mathbf{W}_0\\right) \\mathbf{x} . $$ 可以看出，MLP将线性变换与非线性激活分别用$\\mathbf{W}$和$\\sigma$实现，而KAN直接在$\\boldsymbol{\\Phi}_l$中实现了线性变换和非线性激活\n3. KAN’s Approximation Abilities and Scaling Laws Theorem 2.1 (Approximation theory, KAT). Let $\\mathbf{x}=\\left(x_1, x_2, \\cdots, x_n\\right)$. Suppose that a function $f(\\mathbf{x})$ admits a representation $$ f=\\left(\\boldsymbol{\\Phi}_{L-1} \\circ \\boldsymbol{\\Phi}_{L-2} \\circ \\cdots \\circ \\boldsymbol{\\Phi}_1 \\circ \\boldsymbol{\\Phi}_0\\right) \\mathbf{x} $$ as in Eq. (2.7), where each one of the $\\Phi_{l, i, j}$ are $(k+1)$-times continuously differentiable. Then there exists a constant $C$ depending on $f$ and its representation, such that we have the following approximation bound in terms of the grid size $G$ : there exist $k$-th order $B$-spline functions $\\Phi_{l, i, j}^G$ such that for any $0 \\leq m \\leq k$, we have the bound $$ \\left\\|f-\\left(\\boldsymbol{\\Phi}_{L-1}^G \\circ \\boldsymbol{\\Phi}_{L-2}^G \\circ \\cdots \\circ \\boldsymbol{\\Phi}_1^G \\circ \\boldsymbol{\\Phi}_0^G\\right) \\mathbf{x}\\right\\|_{C^m} \\leq C G^{-k-1+m} . $$\nHere we adopt the notation of $C^m$-norm measuring the magnitude of derivatives up to order $m$ : $$ \\|g\\|_{C^m}=\\max _{|\\beta| \\leq m} \\sup _{x \\in[0,1]^n}\\left|D^\\beta g(x)\\right| . $$\n从结果来看，KAN的 upper bound 与维度$n$无关，破除了“维数诅咒”，从统计的角度开本质上就是引入了``可加结构’’，使得每个节点类似于一维的速度\nNeural scaling laws: comparison to other theories：神经缩放定律描述了测试损失随着模型参数增加而减少的现象，即 $\\ell \\propto N^{-\\alpha}$，其中 $\\ell$ 表示测试均方根误差 (RMSE)，$N$ 为参数数量，$\\alpha$ 为缩放指数。较大的 $\\alpha$意味着通过简单地扩展模型可以获得更多的改进 Sharma 和 Kaplan 提出$\\alpha$与输入流形维度$d$相关，如果模型函数类是阶数为$k$的分段多项式（对于 ReLU，$k=1$），那么标准逼近理论暗示$\\alpha = \\frac{k+1}{d}$。这一界限受限于维度诅咒。Michaud 等人考虑了仅涉及一元（例如，平方、正弦、指数）和二元（加法和乘法）运算的计算图，发现 $\\alpha = \\frac{k+1}{d^*} = \\frac{k+1}{2}$，其中 $d^* = 2$ 是最大元数。\nKAN假设存在平滑的 Kolmogorov-Arnold 表示，将高维函数分解为若干一维函数，得出 $\\alpha = k+1$（其中 $k$ 是分段多项式样条的阶数）。如果选择 $k=3$ 的三次样条，因此 $\\alpha=4$。\nComparison between KAT and UAT：全连接神经网络的强大功能由普适逼近定理（Universal Approximation Theorem, UAT）证明，该定理表明，给定一个函数和误差容忍度 $\\epsilon \u0026gt; 0$，一个具有 $k \u0026gt; N(\\epsilon)$ 个神经元的两层网络可以在误差 $\\epsilon$ 范围内逼近该函数。然而，UAT 并未保证 $N(\\epsilon)$ 如何随 $\\epsilon$ 缩放的界限。实际上，它受制于维度诅咒（COD），并且在某些情况下 $N$ 被证明会随着 $d$ 指数级增长 [15]。KAT 和 UAT 之间的区别在于，KANs 利用了函数的内在低维表示，而 MLPs 则没有。 4. For Interpretability: Simplifying KANs and Making them interactive 如果我们需要拟合函数$f(x,y)=\\exp\\big(\\sin(\\pi x)+y^2\\big)$，那么用$[2,1,1]$这个结构的 KAN 是最合适的，但是我们无法提前知道函数的形式，应该如何与 KAN 交互得到我们想要的结构呢？\n首先定义一个有$N_p$输入的激活函数$\\phi$的$L_1$范数：\n$$ …","date":1715445567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715445567,"objectID":"dd7d7c36dceb499b7158fb43a2455d93","permalink":"https://ikerlz.github.io/post/kan/","publishdate":"2024-05-12T00:39:27+08:00","relpermalink":"/post/kan/","section":"post","summary":"Table of Contents 0、MLP 一、Kolmogorov–Arnold Networks 1. Kolmogorov-Arnold Representation theorem 2. KAN Architecture 3. KAN’s Approximation Abilities and Scaling Laws 4. For Interpretability: Simplifying KANs and Making them interactive 二、KAN 的一些问题 参考文献 0、MLP 感知机最早由Rosenblatt于1957年提出，由于其简单的结构而得到快速的发展，下图是一个MLP的示意图 我们可以把上面这个 MLP 表示为： $$ f_{\\mathrm{MLP}}(\\mathbf{x})=\\mathbf{W}_4 \\boldsymbol{\\sigma}\\left(\\mathbf{W}_3 \\boldsymbol{\\sigma}\\left(\\mathbf{W}_2 \\boldsymbol{\\sigma}\\left(\\mathbf{W}_1 \\mathbf{x}+\\mathbf{b}_1\\right)+\\mathbf{b}_2\\right)+\\mathbf{b}_3\\right)+\\mathbf{b}_4, $$ 可以看出，在 MLP 中，激活函数$\\boldsymbol{\\sigma}(\\cdot)$是作用在节点（node）上的，而边（edge）的连接没有附带任何信息，唯一的作用就是把两层中的所有节点连接起来。","tags":["B样条","Lagrange插值","贝塞尔曲线"],"title":"KAN: Kolmogorov–Arnold Networks","type":"post"},{"authors":["Zhe Li"],"categories":["Deep Learning"],"content":" 主要针对扩散模型的一些经典文章写一些个人理解，博采众长，参考了很多博客、文章，详细信息见参考文献\nTable of Contents 0、文生图片模型 1、几种生成模型的对比 2、扩散模型（DDPM） 3 、基于分数的生成模型（Score-based generative models） 参考文献 0、文生图片模型 DALL·E 3 扩散模型的大火始于2020年所提出的DDPM（“Denoising Diffusion Probabilistic Models”）。当前最先进的两个文本生成图像——OpenAI的DALL·E 3和Google的Imagen 2，都是基于扩散模型来完成的。\n1、几种生成模型的对比 GAN（生成对抗网络）：GAN是由两部分组成，一个生成器和一个判别器。生成器的目标是创建足够真实的数据，以至于判别器不能区分生成的数据和真实数据。判别器的目标是正确区分真实数据和生成器生成的假数据。这两部分在训练过程中相互竞争，推动彼此的进步，因此称为对抗网络。GAN在图像生成方面尤其出色。\nVAE（变分自编码器）：VAE采用不同的方法来生成数据。它通过编码器将数据映射到一个分布上，并从这个分布中采样来构造一个解码器用于数据重建。它是一种通过概率方法生成新数据的模型，通常用于生成遵循特定统计分布的图片。\nFlow-based Models（基于流的模型）：这类模型使用可逆变换来学习数据的分布，这意味着它们可以精确地计算生成数据的概率。它们可以生成高质量的数据，并且给定新数据，也可以确定其概率。这种特性在密度估计和无损压缩方面特别有用。\nDiffusion Models（扩散模型）：Diffusion Models 的灵感来自non-equilibrium thermodynamics （非平衡热力学）。理论首先定义扩散步骤的马尔可夫链，以缓慢地将随机噪声添加到数据中，然后学习逆向扩散过程以从噪声中构造所需的数据样本。与 VAE 或流模型不同，扩散模型是通过固定过程学习，并且隐空间具有比较高的维度。\n2、扩散模型（DDPM） 主要参考B站：李宏毅老师的 Diffusion Model 讲解\n考虑下图瑞士卷形状的二维联合概率分布$p(x,y)$，扩散过程$q$非常直观，本来集中有序的样本点，受到噪声的扰动，向外扩散，最终变成一个完全无序的噪声分布 而diffusion model其实是图上的这个逆过程，将一个噪声分布$N(0,I)$逐步地去噪以映射到$p_{data}$，有了这样的映射，我们从噪声分布中采样，最终可以得到一张想要的图像。 2.1 Diffusion Model原理 Diffusion Models由正向过程（或扩散过程）和反向过程（或逆扩散过程）组成：\n逆向过程（Inverse Process） 正向过程（Forward Process） 原文算法框\nDenoise Module 如何训练 如何从文本生成图片 2.2 DDPM 数学推导 我们可以从MLE的角度来理解图片生成模型，即我们希望随机抽样一个$z$后，经过一个“参数模型”（神经网络）输出的$P_\\theta(x)$这个分布与真实的数据分布$P_{data}(x)$越近越好，因此我们希望找到一个$\\theta$，使其对于从$P_{data}(x)$中抽样出来的$m$个样本$\\{x^1,x^2,\\ldots,x^m\\}$满足： $$\\theta^\\star=\\arg \\max _\\theta \\prod_{i=1}^m P_\\theta\\left(x^i\\right)$$ 因此，本质上就是最大化似然。进一步，我们可以得到： $$ \\begin{aligned} \\theta^\\star= \u0026amp; \\arg \\max _\\theta \\prod_{i=1}^m P_\\theta\\left(x^i\\right)=\\arg \\max _\\theta \\sum_{i=1}^m \\log P_\\theta\\left(x^i\\right)\\\\ \u0026amp; \\approx \\arg \\max _\\theta E_{x \\sim P_{\\text {data }}}\\left[\\log P_\\theta(x)\\right] \\\\ \u0026amp;=\\arg \\max _\\theta \\int_x P_{\\text {data }}(x) \\log P_\\theta(x) d x\\\\ \u0026amp; ={\\small \\arg \\max _\\theta \\left\\{\\int_x P_{\\text {data }}(x) \\log P_\\theta(x) d x-{\\color{blue}\\int_x P_{\\text {data }}(x) \\log P_{\\text {data }}(x) d x}\\right\\}} \\\\ \u0026amp; =\\arg \\max _\\theta \\int_x P_{\\text {data }}(x) \\log \\frac{P_\\theta(x)}{P_{\\text {data }}(x)} d x\\\\ \u0026amp; = \\arg\\min_{\\theta} {\\color{blue}KL (P_{\\text {data }}\\| P_\\theta)} \\end{aligned} $$ 所以最大化似然等价于最小化真实数据分布与生成数据分布的KL散度。但是一般说来，直接计算$P_\\theta(x)$是非常困难的，在VAE我们是通过计算$\\log\\{P(x)\\}$的下界来优化网络的，具体而言，给定任意一个分布$q(z|x)$，我们有 $$ \\begin{aligned} \u0026amp; \\log P(x)\\\\ =\u0026amp;\\int_z q(z \\mid x) \\log P(x) d z \\\\ =\u0026amp;\\int_z q(z \\mid x) \\log \\left(\\frac{P(z, x)}{P(z \\mid x)}\\right) d z\\\\ =\u0026amp;\\int_z q(z \\mid x) \\log \\left(\\frac{P(z, x)}{{\\color{blue}q(z \\mid x)}} \\frac{{\\color{blue}q(z \\mid x)}}{P(z \\mid x)}\\right) d z \\\\ =\u0026amp; \\int_z q(z \\mid x) \\log \\left(\\frac{P(z, x)}{q(z \\mid x)}\\right) d z+\\underbrace{\\int_z q(z \\mid x) \\log \\left(\\frac{q(z \\mid x)}{P(z \\mid x)}\\right) d z}_{{\\color{red}K L(q(z \\mid x) | P(z \\mid x))\\ge 0}} \\\\ \\geq \u0026amp; \\int_z q(z \\mid x) \\log \\left(\\frac{P(z, x)}{q(z \\mid x)}\\right) d z\\\\ =\u0026amp;\\mathrm{E}_{q(z \\mid x)}\\left[\\log \\left(\\frac{P(x, z)}{q(z \\mid x)}\\right)\\right] \\end{aligned} $$ 这里的$\\mathrm{E}_{q(z \\mid x)}\\left[\\log \\left(\\frac{P(x, z)}{q(z \\mid x)}\\right)\\right]$就是$\\log P(x)$的下界（ELBO(evidence lower-bound)），在VAE中，我们本质上是在Maximize这个下界，这里$q(z\\mid x)$实际上就是VAE中的Encoder。至于$P(x,z)$这个联合分布，我们可以借助公式$P(x,z)=P(x \\mid z)P(z)$来计算，$P(z)$通常假设为高斯分布。至于$P(x \\mid z)$的计算，如下图所示，随机生成一个$z$后，输入网络中得到$G(z)$，我们希望$G(z)$与$x$越近越好，因此，最直接的想法是： $$ P_\\theta(x \\mid z)= \\begin{cases}1, \u0026amp; G(z)=x \\\\ 0, \u0026amp; G(z) \\neq x\\end{cases} $$ 但是这样可能会导致$P_\\theta(x \\mid z)$几乎都是0。因此我们可以把假设$P_\\theta(x\\mid z)$服从一个均值为$G(z)$的高斯分布，这样$P_\\theta(x\\mid z)\\propto \\exp \\left(-\\|G(z)-x\\|_2^2\\right)$，就进行训练（当然，这里只是为了解释$P_\\theta(x\\mid z)$的计算，VAE实际操作中还涉及到重参数化的技巧等等，这里就不再赘述） 根据相同的思路，我们就可以计算这个DDPM中的$P_\\theta(x_0)$： 如上图所示，我们可以把$P_\\theta(x_{t-1}\\mid x_t)$看作是服从均值为$G(x_t)$的高斯分布，因此$P_\\theta(x_{t-1}\\mid x_t)\\propto \\exp \\left(-\\|G(x_t)-x_{t-1}\\|_2^2\\right)$，同时我们假设一阶马尔可夫条件，最终得到： $$ {\\small P_\\theta\\left(x_0\\right)=\\int_{x_1: x_T} P\\left(x_T\\right) P_\\theta\\left(x_{T-1} \\mid x_T\\right) \\ldots P_\\theta\\left(x_{t-1} \\mid x_t\\right) \\ldots P_\\theta\\left(x_0 \\mid x_1\\right) d x_1: x_T} $$ 如下图所示，如果我们对比VAE和DDPM，两者在形式上是非常相似的： 这里的$q\\left(x_1: x_T \\mid x_0\\right)$我们仍然可以在假设一阶马尔可夫的条件下拆分为： $$q\\left(x_1: x_T \\mid x_0\\right)=q\\left(x_1 \\mid x_0\\right) q\\left(x_2 \\mid x_1\\right) \\ldots q\\left(x_T \\mid x_{T-1}\\right)$$ 那么，这里的$q(x_t\\mid x_{t-1})$应该怎么计算呢？ 由于在扩散过程（Diffusion Process）中，$x_t$满足$x_t=\\sqrt{1-\\beta_t}x_{t-1}+\\sqrt{\\beta_t}\\epsilon_{t-1}$，其中$\\epsilon_{t-1}\\sim N(0,I)$，因此，我们$x_t$服从均值为$\\sqrt{1-\\beta_t}x_{t-1}$，方差为$\\beta_t$的高斯分布，从而可以很好地进行计算。另外，对于$q(x_t\\mid x_0)$，我们并不需要通过$q(x_t\\mid x_0)=q(x_t\\mid x_{t-1})q(x_{t-1}\\mid x_{t-2})\\cdots q(x_1\\mid x_0)$这个序列每次抽样一个高斯分布，逐一计算，然后乘起来，我们可以直接计算$q(x_t\\mid x_0)$，原因如下图所示： 由于两次抽样是相互独立的，因此我可以只抽样一次，然后修改系数即可： 以此类推，我们有 $$ \\begin{aligned} x_t \u0026amp;= \\sqrt{(1-\\beta_t)(1-\\beta_{t-1})\\ldots(1-\\beta_1)}x_0\\\\ \u0026amp;~~~+ \\sqrt{1-(1-\\beta_t)(1-\\beta_{t-1})\\ldots(1-\\beta_1)}\\ \\epsilon \\end{aligned} $$ 这里的$\\beta_1,\\beta_2,\\ldots,\\beta_T$都是在训练之前人为设定的参数，如果我们定义$\\alpha_t=1-\\beta$，$\\bar\\alpha_t=\\alpha_1\\alpha_2\\ldots \\alpha_t$，我们就得到了 …","date":1712248767,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712248767,"objectID":"92dbfd3e0804cc6aac84af0a6933bc2d","permalink":"https://ikerlz.github.io/post/ddpm/","publishdate":"2024-04-05T00:39:27+08:00","relpermalink":"/post/ddpm/","section":"post","summary":"主要针对扩散模型的一些经典文章写一些个人理解，博采众长，参考了很多博客、文章，详细信息见参考文献\nTable of Contents 0、文生图片模型 1、几种生成模型的对比 2、扩散模型（DDPM） 3 、基于分数的生成模型（Score-based generative models） 参考文献 0、文生图片模型 DALL·E 3 扩散模型的大火始于2020年所提出的DDPM（“Denoising Diffusion Probabilistic Models”）。当前最先进的两个文本生成图像——OpenAI的DALL·E 3和Google的Imagen 2，都是基于扩散模型来完成的。\n1、几种生成模型的对比 GAN（生成对抗网络）：GAN是由两部分组成，一个生成器和一个判别器。生成器的目标是创建足够真实的数据，以至于判别器不能区分生成的数据和真实数据。判别器的目标是正确区分真实数据和生成器生成的假数据。这两部分在训练过程中相互竞争，推动彼此的进步，因此称为对抗网络。GAN在图像生成方面尤其出色。\nVAE（变分自编码器）：VAE采用不同的方法来生成数据。它通过编码器将数据映射到一个分布上，并从这个分布中采样来构造一个解码器用于数据重建。它是一种通过概率方法生成新数据的模型，通常用于生成遵循特定统计分布的图片。\nFlow-based Models（基于流的模型）：这类模型使用可逆变换来学习数据的分布，这意味着它们可以精确地计算生成数据的概率。它们可以生成高质量的数据，并且给定新数据，也可以确定其概率。这种特性在密度估计和无损压缩方面特别有用。\nDiffusion Models（扩散模型）：Diffusion Models 的灵感来自non-equilibrium thermodynamics （非平衡热力学）。理论首先定义扩散步骤的马尔可夫链，以缓慢地将随机噪声添加到数据中，然后学习逆向扩散过程以从噪声中构造所需的数据样本。与 VAE 或流模型不同，扩散模型是通过固定过程学习，并且隐空间具有比较高的维度。","tags":["DDPM","Diffusion Model"],"title":"扩散模型（Diffusion Model）","type":"post"},{"authors":["Zhe Li"],"categories":["Classical Statistics"],"content":" Table of Contents 一、Lagrange插值法 二、（Bezier）贝塞尔曲线与B-Splines 1、（Bezier）贝塞尔曲线 2、B-Splines 三、样条估计 四、拟合样条对深度学习中的双下降（Double Decent）现象的解释 一、Lagrange插值法 已知若干点，如何得到光滑曲线？是否可以通过在原有数据点上进行点的填充生成曲线？\n首先，可以考虑两个点的插值： 考虑$P_0$和$P_1$之间的任意一点$P_x$，可表示为： $$P_x=P_0+\\left(P_1-P_0\\right) t=(1-t) P_0+t P_1$$ 其中$t={(P_0 P_x)}/{(P_0 P_1)}$。直观上，我们可以把$P_0$和$P_1$视为控制点，$(1-t)$和$t$视作基函数。【思考：两点如何推广到多个点？】\n如果知道三个点: $P_0, P_1, P_2$, 如何确定一条曲线 ? 想法: 将$P_0, P_1$ 进行连接，然后将$P_1, P_2$ 进行连接,。但是这样的一个曲线并不光滑 注意到，直线可以由2个点确定， 而二次曲线由三个点即可确定，推广到一般情况， $n-1$ 阶曲线可以由$n$个点确定 这本质上就是Lagrange插值法的思想 (必须经过所有点) 一般来说，如果我们有 $n$ 个点 $\\left(x_1, y_1\\right), \\ldots,\\left(x_n, y_n\\right)$ ，各 $x_i$ 互不相同。对于 1 到 $\\mathrm{n}$ 之间的每个 $k$, 定义 $n-1$ 次多项式 $$ L_k(x)=\\frac{\\left(x-x_1\\right) \\ldots\\left(x-x_{k-1}\\right)\\left(x-x_{k+1}\\right) \\ldots\\left(x-x_n\\right)}{\\left(x_k-x_1\\right) \\ldots\\left(x_k-x_{k-1}\\right)\\left(x_k-x_{k+1}\\right) \\ldots\\left(x_k-x_n\\right)} $$ $L_k(x)$ 具有有趣的性质: $L_k\\left(x_k\\right)=1, L_k\\left(x_j\\right)=0, j \\neq k$. 然后定义一个 $n-1$ 次多项式 $$ P_{n-1}(x)=y_1 L_1(x)+\\ldots+y_n L_n(x)=\\sum_{i=1}^ny_iL_i(x) . $$ 这样的多项式 $P_{n-1}(x)$ 满足 $P_{n-1}\\left(x_i\\right)=y_i, i=1,2, \\ldots, n$. 因此必过控制点$\\{(x_i,y_i)\\mid i=1,\\ldots,n\\}$；这就是著名的拉格朗日插值多项式！ 我们可以把$L_1(x),\\ldots,L_n(x)$看作基函数，而Lagrange插值本质上就是一组基的线性组合！ Lagrange插值法例子 已知区间 $[-1,1]$ 上函数 $f(x)=[1+(5 x)^2]^{-1}$。取等距节点 $$ x_i=-1+\\frac{i}{5}, \\quad i=0,1,2, \\cdots, 10 $$ 作Lagrange插值多项式 $$ P_{10}(x)=\\sum_{i=0}^{10} f\\left(x_i\\right) L_i(x) . $$ 从图中可以看出, 在 $x=0$ 附近, $P_{10}(x)$ 能较好地逼近 $f(x)$, 但在有些地方, 如在 $[-1,-0.8]$ 和 $[0.8,1]$ 之间, $P_{10}(x)$ 与 $f(x)$ 差异很大，这种现象被称为Runge现象。 Runge现象出现的原因是在因为在进行Lagrange插值时要求必须要过控制点，因此可以考虑不过控制点进行插值，这样就能避免Runge现象\n插值法因为要求经过所有节点, 所以导致这种结果, 因此在此基础上提出了拟合的概念: 依据原有数据点，通过参数调整设置，使得生成曲线与原有点差距最小 (最小二乘), 因此曲线未必会经过原有数据点 样条曲线 (Spline curves): 是给定一系列控制点而得到的一条曲线，曲线形状由这些控制点控制。一般分为插值样条和拟合样条。 二、（Bezier）贝塞尔曲线与B-Splines 均匀节点意义下的一元 B 样条 (B-splines, Basis Splines 缩写)是在 1946 年由 1.J.Schoenberg 提出 1962 年, 法国数学家 Pierre Bezier 研究了一种曲线, 即 Bezier 曲线 1972 年, de Boor 与 cox 分别独立提出了计算 B 样条基函数的公式这个公式对 B 样条作为计算机辅助几何设计 (CAGD)重要工具起到了至关重要的作用，称之为 de Boor-Cox 公式，即著名的 de Boor 算法 1、（Bezier）贝塞尔曲线 不过 $P_1$ : $$ \\begin{gathered} P_i\u0026amp;=(1-t) P_0+t P_1 ;\\\\ P_j\u0026amp;=(1-t) P_1+t P_2 ; \\\\ P_x\u0026amp;=(1-t) P_i+t P_j. \\end{gathered} $$ 其中， $$ \\frac{P_0 P_i}{P_0 P_1}=\\frac{P_1 P_j}{P_1 P_2}=\\frac{P_j P_x}{P_i P_j}=t $$ 因此我们可以得到：$P_x=\\left(1-t^2\\right) P_0+2 t(1-t) P_1+t^2 P_2$ 我们可以把$P_0,P_1,P_2$看作控制点，把$(1-t^2)$，$2t(1-t)$和$t^2$看作是基函数 推广到一般情况，假设一共有 $n+1$ 个点, 就可以确定了$n$次的贝塞尔曲线 $$ B(t)=\\sum_{i=0}^n C_n^i(1-t)^{n-i} t^i P_i, \\quad t \\in[0,1] $$ 或者写成这样 $$ B(t)=W_{t, n}^0 P_0+W_{t, n}^1 P_1+\\cdots+W_{t, n}^n P_n $$ 可以理解为以$W$为基, $P$为系数的线性组合；其中$W_i=C_n^i(1-t)^{n-i} t^i$ 注：当有$n+1$个点, 有$n+1$个基函数, 确定$n$阶函数曲线\n$W_{t, n}^i$为$P_i$的系数, 是最高幂次为$n$的关于$t$的多项式。当 $t$确定后, 该值就为定值。\n因此整个式子可以理解为$B(t)$插值点是这$n+1$个点施加各自的权重$W$后累加得到的。这也是为什么改变其中一个控制点, 整个贝塞尔曲线都会受到影响。\n其实对于样条曲线的生成，本质上就对于各个控制点施加权重\n$n$阶贝塞尔曲线$B^n(t)$可以由前$n$个点决定的$n-1$次贝塞尔曲线$B^{n-1}\\left(t \\mid P_0, \\cdots, P_{n-1}\\right)$与后$n$个点决定的$n-1$次贝塞尔曲线$B^{n-1}\\left(t \\mid P_1, \\cdots, P_n\\right)$线性组合递推而来，即 $$ {\\color{red} \\begin{aligned} \u0026amp; B^n\\left(t \\mid P_0, P_1, \\cdots, P_n\\right)= \\\\ \u0026amp; (1-t) B^{n-1}\\left(t \\mid P_0, P_1, \\cdots, P_{n-1}\\right)+t B^{n-1}\\left(t \\mid P_1, P_2, \\cdots, P_n\\right) \\end{aligned} } $$\n2、B-Splines 我们比较好奇的是对于B样条，怎么得到各控制点前的权重（基函数） B-Splines的一些重要定义： 控制点: 控制曲线的点, 等价于贝塞尔函数的控制点, 通过控制点可以控制曲线形状。假设有 $n+1$ 个控制点 $P_0, P_1, \\ldots, P_n$ 。 节点: 与控制点无关，是人为地将目标曲线分为若干个部分,其目的就是尽量使得各个部分有所影响但也有一定独立性,这也是为什么 $\\mathrm{B}$ 样条中, 有时一个控制点的改变, 不会很大影响到整条曲线，而只影响到局部的原因，这区别于贝塞尔曲线。 节点划分影响权重计算，假设我们划分 $m+1$ 个节点 $t_0, t_1, \\ldots, t_m$ ，将曲线分成了 $m$ 段。 次 (degree) 与阶 (order): 次的概念是贝塞尔中次的概念, 即权重中$t$的最高幂次。阶 (order) $=$ 次 (degree) +1 。通常我们用 $k$ 表示次。 注意到： $$ B(t)=\\sum_{i=0}^n W_i P_i=\\sum_{i=0}^n B_{i, k}(t) P_i $$ 我们需要获得$W_i$即可。$W_i$是关于$t$的函数, 最高幂次为$k$。$B$样条中通常记为$B_{i, k}(t)$, 即表示第$i$点的权重, 是关于$t$的函数，且最高幂次为$k$。而这个权重函数$B_{i, k}(t)$，在 B样条里叫做$k$次B样条基函数 $B_{i, k}(t)$ 满足如下递推式 (de Boor 递推式) $$ {\\color{blue} \\begin{gathered} k=0, \\quad B_{i, 0}(t)= \\begin{cases}1, \u0026amp; t \\in\\left[t_i, t_{i+1}\\right] \\\\ 0, \u0026amp; \\text { Otherwise }\\end{cases} \\\\ k\u0026gt;0, \\quad B_{i, k}(t)=\\frac{t-t_i}{t_{i+k}-t_i} B_{i, k-1}(t)+\\frac{t_{i+k+1}-t}{t_{i+k+1}-t_{i+1}} B_{i+1, k-1}(t) \\end{gathered} } $$ 节点数、控制点数与次数的关系 控制点有$n+1=5$个, $n=4$ ，即 $P_0, P_1, P_2, P_3, P_4$ 节点规定为 $m+1=10$ 个, $m=9$ ，即 $t_0, t_1, \\cdots, t_9$ ，该节点将要生成的目标曲线分为了 9 份， 这里的 $t$ 取值一般为 $0-1$的一系列非递减数。 $t_0, t_1, \\cdots, t_9$ 组成的序列，叫做节点表，如等分的节点表 $\\{0, \\frac{1}{9}, \\frac{2}{9}, \\frac{3}{9}, \\frac{4}{9}, \\frac{5}{9}, \\frac{6}{9}, \\frac{7}{9}, \\frac{8}{9}, 1\\}$ 次为$k$。 三者有个必须要满足的关系式为 $$m=n+k+1$$ 为什么满足$m=n+k+1$？ 实例计算 节点设置： 节点向量: $x=0,0.25,0.5,0.75,1$ 节点数: $m+1=5(m=4)$ 节点: $x_0=0, x_1=0.25, x_2=0.5, x_3=0.75, x_4=1$ 节点区间： $\\left[x_0, x_1\\right),\\left[x_1, x_2\\right),\\left[x_2, x_3\\right),\\left[x_3, x_4\\right)$ 0 次 (degree) 基函数为: $$ \\begin{aligned} \u0026amp; B_{0,0}(x)=\\left\\{\\begin{array}{lcc} 1 \u0026amp; \\text { if } \u0026amp; x_0 \\leq x\u0026lt;x_1 \\\\ 0 \u0026amp; \u0026amp; \\text{ other } \\end{array}\\right. \\\\ \u0026amp; B_{1,0}(x)=\\left\\{\\begin{array}{ccc} 1 \u0026amp; \\text { if } \u0026amp; x_1 \\leq x\u0026lt;x_2 \\\\ 0 \u0026amp; …","date":1709570367,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709570367,"objectID":"4b9a037dd38b01138183d35b6350727a","permalink":"https://ikerlz.github.io/post/spline/","publishdate":"2024-03-05T00:39:27+08:00","relpermalink":"/post/spline/","section":"post","summary":"Table of Contents 一、Lagrange插值法 二、（Bezier）贝塞尔曲线与B-Splines 1、（Bezier）贝塞尔曲线 2、B-Splines 三、样条估计 四、拟合样条对深度学习中的双下降（Double Decent）现象的解释 一、Lagrange插值法 已知若干点，如何得到光滑曲线？是否可以通过在原有数据点上进行点的填充生成曲线？\n首先，可以考虑两个点的插值： 考虑$P_0$和$P_1$之间的任意一点$P_x$，可表示为： $$P_x=P_0+\\left(P_1-P_0\\right) t=(1-t) P_0+t P_1$$ 其中$t={(P_0 P_x)}/{(P_0 P_1)}$。直观上，我们可以把$P_0$和$P_1$视为控制点，$(1-t)$和$t$视作基函数。【思考：两点如何推广到多个点？】\n如果知道三个点: $P_0, P_1, P_2$, 如何确定一条曲线 ? 想法: 将$P_0, P_1$ 进行连接，然后将$P_1, P_2$ 进行连接,。但是这样的一个曲线并不光滑 注意到，直线可以由2个点确定， 而二次曲线由三个点即可确定，推广到一般情况， $n-1$ 阶曲线可以由$n$个点确定 这本质上就是Lagrange插值法的思想 (必须经过所有点) 一般来说，如果我们有 $n$ 个点 $\\left(x_1, y_1\\right), \\ldots,\\left(x_n, y_n\\right)$ ，各 $x_i$ 互不相同。对于 1 到 $\\mathrm{n}$ 之间的每个 $k$, 定义 $n-1$ 次多项式 $$ L_k(x)=\\frac{\\left(x-x_1\\right) \\ldots\\left(x-x_{k-1}\\right)\\left(x-x_{k+1}\\right) \\ldots\\left(x-x_n\\right)}{\\left(x_k-x_1\\right) \\ldots\\left(x_k-x_{k-1}\\right)\\left(x_k-x_{k+1}\\right) \\ldots\\left(x_k-x_n\\right)} $$ $L_k(x)$ 具有有趣的性质: $L_k\\left(x_k\\right)=1, L_k\\left(x_j\\right)=0, j \\neq k$.","tags":["B样条","Lagrange插值","贝塞尔曲线"],"title":"B样条（B-Splines)","type":"post"},{"authors":["Yimeng Ren","Zhe Li","Xuening Zhu","Yuan Gao","Hansheng Wang"],"categories":null,"content":" ","date":1706313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706313600,"objectID":"94e8403afddfc5052342001cd845ce2c","permalink":"https://ikerlz.github.io/publication/dsar/","publishdate":"2023-11-27T00:00:00Z","relpermalink":"/publication/dsar/","section":"publication","summary":"The rapid growth of online network platforms generates large-scale network data and it poses great challenges for statistical analysis using the spatial autoregression (SAR) model. In this work, we develop a novel distributed estimation and statistical inference framework for the SAR model on a distributed system. We first propose a distributed network least squares approximation (DNLSA) method. This enables us to obtain a one-step estimator by taking a weighted average of local estimators on each worker. Afterwards, a refined two-step estimation is designed to further reduce the estimation bias. For statistical inference, we utilize a random projection method to reduce the expensive communication cost. Theoretically, we show the consistency and asymptotic normality of both the one-step and two-step estimators. In addition, we provide theoretical guarantee of the distributed statistical inference procedure. The theoretical findings and computational advantages are validated by several numerical simulations implemented on the Spark system. Lastly, an experiment on the Yelp dataset further illustrates the usefulness of the proposed methodology.","tags":null,"title":"Distributed Estimation and Inference for Spatial Autoregression Model with Large Scale Networks","type":"publication"},{"authors":["Zhe Li"],"categories":["Causal Inference"],"content":" Disentangle Mixture Distributions and Instrumental Variable Inequalities $$ \\begin{aligned} \\ \\end{aligned} $$\nLi Zhe $$$$\nSchool of Data Science, Fudan University $$$$\nJanuary 3, 2024 Introduction The IV model in last chapter imposes three assumptions\n(randomization) $Z \\perp\\!\\!\\!\\perp\\{D(1), D(0), Y(1), Y(0)\\}$ (monotonicity) $\\operatorname{pr}(U=\\mathrm{d})=0$ or $D_i(1)\\geq D_i(0)$ (exclusionrestriction) $Y(1)=Y(0) \\text { for } U=\\mathrm{a} \\text { or } \\mathrm{n}$ $$ {\\tiny U_i= \\begin{cases} a, \u0026amp; \\text { if } D_i(1)=1 \\text { and } D_i(0)=1 \\\\ c, \u0026amp; \\text { if } D_i(1)=1 \\text { and } D_i(0)=0 \\\\ d, \u0026amp; \\text { if } D_i(1)=0 \\text { and } D_i(0)=1 \\\\ n, \u0026amp; \\text { if } D_i(1)=0 \\text { and } D_i(0)=0 \\end{cases} } $$\nObserved groups and latent groups under the assumptions $$ {\\small \\begin{array}{cccl} Z=1 \u0026amp; D=1 \u0026amp; D(1)=1 \u0026amp; U=\\mathrm{c} \\text { or a } \\\\ Z=1 \u0026amp; D=0 \u0026amp; D(1)=0 \u0026amp; U=\\mathrm{n} \\\\ Z=0 \u0026amp; D=1 \u0026amp; D(0)=1 \u0026amp; U=\\mathrm{a} \\\\ Z=0 \u0026amp; D=0 \u0026amp; D(0)=0 \u0026amp; U=\\mathrm{c} \\text { or } \\mathrm{n} \\end{array} } $$\nInterestingly, the assumptions have some testable implications. Balke and Pearl (1997) called them the instrumental variable inequalities. Outline Disentangle Mixture Distributions and Instrumental Variable Inequalities\nTestable implications\nExamples\nDisentangle Mixture Distributions and IV Inequalities Recall $\\pi_u$ as the proportion of type $U=u$, and define $$ \\mu_{z u}=E\\{Y(z) \\mid U=u\\}, \\quad(z=0,1 ; u=\\mathrm{a}, \\mathrm{n}, \\mathrm{c}) . $$\nTheorem 22.1 Under the three assumptions, we can identify the proportions of the latent types by $$ \\begin{aligned} \u0026amp; \\pi_{\\mathrm{n}}=\\operatorname{pr}(D=0 \\mid Z=1), \\\\ \u0026amp; \\pi_{\\mathrm{a}}=\\operatorname{pr}(D=1 \\mid Z=0), \\\\ \u0026amp; \\pi_{\\mathrm{c}}=E(D \\mid Z=1)-E(D \\mid Z=0), \\end{aligned} $$ and the type-specific means of the potential outcomes by $$ \\begin{aligned} \\mu_{1 \\mathrm{n}}=\\mu_{0 \\mathrm{n}} \\equiv \\mu_{\\mathrm{n}} \u0026amp; =E(Y \\mid Z=1, D=0) \\\\ \\mu_{1 \\mathrm{a}}=\\mu_{0 \\mathrm{a}} \\equiv \\mu_{\\mathrm{a}} \u0026amp; =E(Y \\mid Z=0, D=1) \\\\ \\mu_{1 \\mathrm{c}} \u0026amp; =\\pi_{\\mathrm{c}}^{-1}\\{E(D Y \\mid Z=1)-E(D Y \\mid Z=0)\\} \\\\ \\mu_{0 \\mathrm{c}} \u0026amp; =\\pi_{\\mathrm{c}}^{-1}[E\\{(1-D) Y \\mid Z=0\\}-E\\{(1-D) Y \\mid Z=1\\}] \\end{aligned} $$\nProof of Theorem 22.1 (Part I) We can identify the proportion of the never takers by $$ \\begin{aligned} \\operatorname{pr}(D=0 \\mid Z=1) \u0026amp; =\\operatorname{pr}(U=\\mathrm{n} \\mid Z=1)=\\operatorname{pr}(U=\\mathrm{n})=\\pi_{\\mathrm{n}}, \\end{aligned} $$\nthe proportion of the always takers by $$ \\begin{aligned} \\operatorname{pr}(D=1 \\mid Z=0) \u0026amp; =\\operatorname{pr}(U=\\mathrm{a} \\mid Z=0)=\\operatorname{pr}(U=\\mathrm{a})=\\pi_{\\mathrm{a}} . \\end{aligned} $$\nthe proportion of compliers is $$ \\begin{aligned} \\pi_{\\mathrm{c}} \u0026amp; =\\operatorname{pr}(U=\\mathrm{c})=1-\\pi_{\\mathrm{n}}-\\pi_{\\mathrm{a}} \\\\ \u0026amp; =1-\\operatorname{pr}(D=0 \\mid Z=1)-\\operatorname{pr}(D=1 \\mid Z=0) \\\\ \u0026amp; =E(D \\mid Z=1)-E(D \\mid Z=0)=\\tau_D, \\end{aligned} $$\nRemark: Although we do not know individual latent compliance types for all units, we can identify the proportions of never takers, always takers, and compliers.\nProof of Theorem 22.1 (Part II) Under the three assumptions, we have\n$$ \\mu_{1 \\mathrm{a}}=\\mu_{0 \\mathrm{a}} \\equiv \\mu_{\\mathrm{a}}, \\quad \\mu_{1 \\mathrm{n}}=\\mu_{0 \\mathrm{n}} \\equiv \\mu_{\\mathrm{n}} . $$\nThe observed group $(Z=1, D=0)$ only has never takers, so $$ E(Y \\mid Z=1, D=0)=E\\{Y(1) \\mid Z=1, U=\\mathrm{n}\\}=E\\{Y(1) \\mid U=\\mathrm{n}\\}=\\mu_{\\mathrm{n}} . $$\nThe observed group $(Z=0, D=1)$ only has always takers, so $$ E(Y \\mid Z=0, D=1)=E\\{Y(0) \\mid Z=0, U=\\mathrm{a}\\}=E\\{Y(0) \\mid U=\\mathrm{a}\\}=\\mu_{\\mathrm{a}} $$\n$$~~$$\nHow about the observed groups $(Z=1, D=1)$ and $(Z=0, D=0)$ ?\nThe Observed Group $(Z=1, D=1)$ The observed group $(Z=1, D=1)$ has both compliers and always takers, so $$ \\small \\begin{aligned} E(Y \\mid Z=1, D=1)= \u0026amp; E\\{Y(1) \\mid Z=1, D(1)=1\\} \\\\ = \u0026amp; E\\{Y(1) \\mid D(1)=1\\} \\\\ = \u0026amp; \\operatorname{pr}\\{D(0)=1 \\mid D(1)=1\\} E\\{Y(1) \\mid D(1)=1, D(0)=1\\} \\\\ \u0026amp; +\\operatorname{pr}\\{D(0)=0 \\mid D(1)=1\\} E\\{Y(1) \\mid D(1)=1, D(0)=0\\} \\\\ = \u0026amp; \\frac{\\pi_{\\mathrm{c}}}{\\pi_{\\mathrm{c}}+\\pi_{\\mathrm{a}}} \\mu_{1 \\mathrm{c}}+\\frac{\\pi_{\\mathrm{a}}}{\\pi_{\\mathrm{c}}+\\pi_{\\mathrm{a}}} \\mu_{\\mathrm{a}} . \\end{aligned} $$\nSolve the linear equation above to obtain $$ \\small \\begin{aligned} \\mu_{1 \\mathrm{c}}= \u0026amp; \\pi_{\\mathrm{c}}^{-1}\\left\\{\\left(\\pi_{\\mathrm{c}}+\\pi_{\\mathrm{a}}\\right) E(Y \\mid Z=1, D=1)-\\pi_{\\mathrm{a}} E(Y \\mid Z=0, D=1)\\right\\} \\\\ = \u0026amp; \\pi_{\\mathrm{c}}^{-1}\\{\\operatorname{pr}(D=1 \\mid Z=1) E(Y \\mid Z=1, D=1) \\\\ \u0026amp; \\quad-\\operatorname{pr}(D=1 \\mid Z=0) E(Y \\mid Z=0, D=1)\\} \\\\ = \u0026amp; \\pi_{\\mathrm{c}}^{-1}\\{E(D Y \\mid Z=1)-E(D Y \\mid Z=0)\\} . \\end{aligned} $$\nBased on the formulas of $\\mu_{1 \\mathrm{c}}$ and $\\mu_{0 \\mathrm{c}}$ in Theorem 22.1, we have\n$$ \\tau_{\\mathrm{c}}=\\mu_{1 \\mathrm{c}}-\\mu_{0 \\mathrm{c}}=\\{E(Y \\mid Z=1)-E(Y \\mid Z=0)\\} / \\pi_{\\mathrm{c}}. $$\nTestable implications Is there any additional value of the this detour for deriving the formula of $\\tau_c$ …","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"eabdaabaf1469b7f03581e40e166b6e7","permalink":"https://ikerlz.github.io/slides/causalinferencechapter22/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/slides/causalinferencechapter22/","section":"slides","summary":"Chapter 22 in \"A First Course in Causal Inference\".","tags":[],"title":"Disentangle Mixture Distributions and Instrumental Variable Inequalities","type":"slides"},{"authors":["Zhe Li"],"categories":["Factor Model"],"content":" On factor models with random missing: EM estimation, inference, and cross validation $$ \\begin{aligned} \\ \\end{aligned} $$\nLi Zhe $$$$\nSchool of Data Science, Fudan University $$$$\nNovember 30, 2023 Outline Background: factor model Motivation Factor models with random missing: EM estimator Asymptotic properties Determining the number of factors Simulation Empirical application Background: factor model $$ \\begin{aligned} x_{i t} \u0026amp; =\\lambda_i f_t+e_{i t} \\\\ \\boldsymbol{X} \u0026amp;= \\boldsymbol{\\Lambda} \\boldsymbol{F}^\\top + \\boldsymbol{E} \\end{aligned} $$\n$\\boldsymbol{X}=(\\boldsymbol{x}_{\\cdot 1},\\ldots,\\boldsymbol{x}_{\\cdot T})\\in\\mathbb{R}^{N\\times T}$\n$\\boldsymbol{\\Lambda}\\in\\mathbb{R}^{N\\times R}$: factor loadings\n$\\boldsymbol{F}\\in\\mathbb{R}^{T\\times R}$: common factors (latent, unobserved)\n$\\boldsymbol{E}\\in\\mathbb{R}^{N\\times T}$: idiosyncratic (or error) component\n${e_{it}}$ can exhibit both cross-sectional and temporal dependence. Given the factor number $k$, we can estimate the factors and factor loadings by $$ \\Big\\{\\widehat{\\boldsymbol{\\Lambda}}^k, \\widehat{\\boldsymbol{F}}^k\\Big\\}=\\arg\\min_{\\boldsymbol{\\Lambda}^k,\\boldsymbol{F}^k}\\frac{1}{NT}\\Big\\|\\boldsymbol{X}- \\boldsymbol{\\Lambda}^k{\\boldsymbol{F}^k}^\\top\\Big\\|_F^2 $$ where $\\boldsymbol{\\Lambda}^k\\in\\mathbb{R}^{N\\times k}$ and $\\boldsymbol{F}^k\\in\\mathbb{R}^{T\\times k}$.\nMotivation Factor model in balanced panel has been thoroughly investigated. $$$$ How to handle the missing data problem in factor models? $$$$ the expectation–maximization (EM) algorithm $$$$ the Kalman filter (KF) $$$$ There is no formal study of the asymptotic properties for the EM estimators of the factors and factor loadings for the PC estimation with missing observations Notations consider the factor model $$ \\boldsymbol{X} = \\boldsymbol{F}\\boldsymbol{\\Lambda}^\\top + \\varepsilon $$\n$\\boldsymbol{X}=(X_1,\\ldots,X_N)$ where $X_i \\equiv\\left(X_{i 1}, \\ldots, X_{i T}\\right)^{\\prime}$ and $X_{it}$ are missing at random $\\varepsilon=\\left(\\varepsilon_1, \\ldots, \\varepsilon_N\\right)$ and $\\varepsilon_i \\equiv\\left(\\varepsilon_{i 1}, \\ldots, \\varepsilon_{i T}\\right)^{\\prime}$ for $i=1, \\ldots, N$. $F=\\left(F_1, \\ldots, F_T\\right)^{\\prime}$ and $\\Lambda=\\left(\\lambda_1, \\ldots, \\lambda_N\\right)^{\\prime}$ where $F_t$ and $\\lambda_i$ are $R \\times 1$ vectors of factors and factor loadings $F^0=\\left(F_1^0, \\ldots, F_T^0\\right)^{\\prime}$ and $\\Lambda^0=\\left(\\lambda_1^0, \\ldots, \\lambda_N^0\\right)^{\\prime}$ are the true values of $F$ and $\\Lambda$\n$\\Omega \\subset[N] \\times[T]$ be the index set of the observations that are observed. That is, $$ \\Omega=\\Big\\{(i, t) \\in[N] \\times[T]: X_{i t} \\text { is observed }\\Big\\}. $$\nLet $G$ denote a $T \\times N$ matrix with $(t, i)$ th element given by $g_{i t}=\\mathbf{1}\\{(i, t) \\in \\Omega\\}$ and is independent of $X, F^0, \\Lambda^0$ and $\\varepsilon$\nThe initial estimates Let $\\tilde{X}=X \\circ G$ and $\\tilde{X}_{i t}=X_{i t} g_{i t}$.\nThe common component $C^0 \\equiv F^0 \\Lambda^0$ is a low rank matrix $\\Rightarrow$ it is possible to recover $C^0$ even when a large proportion of elements in $X$ are missing at random.\nUnder the standard condition that $E\\left(\\varepsilon_{i t} \\mid F_t^0, \\lambda_i^0\\right)=0$, we can verify that $E\\left(\\frac{1}{q} \\tilde{X} \\mid F^0, \\Lambda^0\\right)=F^0 \\Lambda^{0 \\prime}$ $\\Rightarrow$ consider the following least squares objective function $$ \\mathcal{L}_{N T}^0(F, \\Lambda) \\equiv \\frac{1}{N T} \\operatorname{tr}\\left[\\left(\\frac{1}{\\tilde{q}} \\tilde{X}-F \\Lambda^{\\prime}\\right)\\left(\\frac{1}{\\tilde{q}} \\tilde{X}-F \\Lambda^{\\prime}\\right)^{\\prime}\\right] $$ identification restrictions: $F^{\\prime} F / T=I_R$ and $\\Lambda^{\\prime} \\Lambda$ is a diagonal matrix.\nBy concentrating out $\\Lambda$ and using the normalization that $F^{\\prime} F / T=I_R$ $\\Rightarrow$ identical to maximizing $\\tilde{q}^{-2} \\operatorname{tr}\\Big\\{F^{\\prime} \\tilde{X} \\tilde{X}^{\\prime} F\\Big\\}$\nThe initial estimates The estimated factor matrix, denoted by $\\hat{F}^{(0)}$ is $\\sqrt{T}$ times the eigenvectors corresponding to the $R$ largest eigenvalues of the $T \\times T$ matrix $\\frac{1}{N T \\tilde{q}^2} \\tilde{X} \\tilde{X}^{\\prime}:$ $$ \\frac{1}{N T \\tilde{q}^2} \\tilde{X} \\tilde{X}^{\\prime} \\hat{F}^{(0)}=\\hat{F}^{(0)} \\hat{D}^{(0)}, $$ $\\hat{D}^{(0)}$ is an $R \\times R$ diagonal matrix consisting of the $R$ largest eigenvalues of $\\left(N T \\tilde{q}^2\\right)^{-1} \\tilde{X} \\tilde{X}^{\\prime}$, arranged in descending order along its diagonal line. The estimator of $\\Lambda^{\\prime}$ is given by $$ \\hat{\\Lambda}^{(0) \\prime}=\\frac{1}{\\tilde{q}}\\left(\\hat{F}^{(0) \\prime} \\hat{F}^{(0)}\\right)^{-1} \\hat{F}^{(0) \\prime} \\tilde{X}=\\frac{1}{T \\tilde{q}} \\hat{F}^{(0) \\prime} \\tilde{X} . $$ We can obtain an initial estimate of the $(t, i)$ th element, $C_{i t}^0$, of $C^0$ by $\\hat{C}_{i t}^{(0)}=\\hat{\\lambda}_i^{(0)\\prime} \\hat{F}_t^{(0)}$. The initial estimators $\\hat{F}_t^{(0)}, \\hat{\\lambda}_i^{(0)}$ and $\\hat{C}_{i t}^{(0)}$ are consistent and …","date":1701043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701043200,"objectID":"b3e16f3b496f99be8b9b023efab776da","permalink":"https://ikerlz.github.io/slides/factormissing/","publishdate":"2023-11-27T00:00:00Z","relpermalink":"/slides/factormissing/","section":"slides","summary":"Notes on \"On factor models with random missing： EM estimation, inference, and cross validation\".","tags":[],"title":"On factor models with random missing： EM estimation, inference, and cross validation","type":"slides"},{"authors":[],"categories":["factor-model"],"content":" Table of Contents 1. Introduction 2. The three-pass regression filter 2.1. The estimator 2.2. Assumptions 2.3. Consistency 2.4. Asymptotic distributions 2.5. Proxy selection 3. Related procedures 3.1. Constrained least squares 3.2. Partial least squares 4. Empirical evidence 4.1. Forecasting macroeconomic aggregates 4.2. Forecasting market returns 1. Introduction How to use the vast predictive information to forecast important economic aggregates like national product or stock market value.\nIf the predictors number near or more than the number of observations, the standard OLS forecaster is known to be poorly behaved or nonexistent (Huber, 1973) A economic view: data are generated from a model in which latent factors drive the systematic variation of both the forecast target, $\\boldsymbol{y}$, and the matrix of predictors, $\\boldsymbol{X}$ $\\Rightarrow$ the best prediction of $\\boldsymbol{y}$ is infeasible since the factors are unobserved $\\Rightarrow$ require a factor estimation step A benchmark method: extract factors that are significant drivers of variation in $\\boldsymbol{X}$ and then uses these to forecast $\\boldsymbol{y}$. Motivations\nthe factors that are relevant to $\\boldsymbol{y}$ may be a strict subset of all the factors driving $\\boldsymbol{X}$ The method, called the three-pass regression filter (3PRF), selectively identifies only the subset of factors that influence the forecast target while discarding factors that are irrelevant for the target but that may be pervasive among predictors The 3PRF has the advantage of being expressed in closed form and virtually instantaneous to compute. Contributions\ndevelop asymptotic theory for the 3PRF verify the finite sample accuracy of the asymptotic theory compare the 3PRF to other methods provide empirical support for the 3PRF’s strong forecasting performance 2. The three-pass regression filter 2.1. The estimator The environment for 3PRF There is a target variable which we wish to forecast. There exist many predictors which may contain information useful for predicting the target variable. The number of predictors $N$ may be large and number near or more than the available time series observations $T$, which makes OLS problematic. Therefore we look to reduce the dimension of predictive information $\\Rightarrow$ assume the data can be described by an approximate factor model. In order to make forecasts, the 3PRF uses proxies: These are variables, driven by the factors (and as we emphasize below, driven by target-relevant factors in particular), which we show are always available from the target and predictors themselves the econometrician on the basis of economic theory. The target is a linear function of a subset of the latent factors plus some unforecastable noise. The optimal forecast therefore comes from a regression on the true underlying relevant factors. However, since these factors are unobservable, we call this the infeasible best forecast. $$ \\boldsymbol{y} = \\boldsymbol{Z} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} $$\n$\\boldsymbol{y}\\in\\mathbb{R}^{T \\times 1}$: the target variable time series from $2,3, \\ldots, T+1$ $\\boldsymbol{X}\\in\\mathbb{R}^{T \\times N}$: the predictors that have been standardized to have unit time series variance. Temporal dimension: $\\boldsymbol{X}=\\left(\\boldsymbol{x}_1^{\\prime}, \\boldsymbol{x}_2^{\\prime}, \\ldots, \\boldsymbol{x}_T^{\\prime}\\right)^{\\prime}$ Cross section: $\\boldsymbol{X}=\\left(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N\\right)$ $\\boldsymbol{Z}\\in\\mathbb{R}^{T \\times L}$: stacks period-by-period proxy data as $\\boldsymbol{Z}=\\left(\\boldsymbol{z}_1^{\\prime}, \\boldsymbol{z}_2^{\\prime}, \\ldots, \\boldsymbol{z}_T^{\\prime}\\right)^{\\prime}$ Make no assumption on the relationship between $N$ and $T$ but assume $L \\ll \\min (N, T)$ Construct the 3PRF\nPass Description 1. Run time series regression of $\\mathbf{x}_i$ on $\\boldsymbol{Z}$ for $i=1, \\ldots, N$, $x_{i, t}=\\phi_{0, i}+\\boldsymbol{z}^{\\prime} \\boldsymbol{\\phi}_i+\\epsilon_{i t}$, retain slope estimate $\\hat{\\boldsymbol{\\phi}}_i$. 2. Run cross section regression of $\\boldsymbol{x}_t$ on $\\hat{\\boldsymbol{\\phi}}_i$ for $t=1, \\ldots, T$, $x_{i, t}=\\phi_{0, t}+\\hat{\\boldsymbol{\\phi}}^{\\prime} \\boldsymbol{F}_t+\\varepsilon_{i t}$, retain slope estimate $\\hat{\\boldsymbol{F}}_t$. 3. Run time series regression of $y_{t+1}$ on predictive factors $\\hat{\\boldsymbol{F}}_t$, $y_{t+1}=\\beta_0+\\hat{\\boldsymbol{F}}^{\\prime} \\boldsymbol{\\beta}+\\eta_{t+1}$, delivers forecast $\\hat{y}_{t+1}$. Pass 1 : the estimated coefficients describe the sensitivity of the predictor to factors represented by the proxies. Pass 2 : first-stage coefficient estimates map the cross-sectional distribution of predictors to the latent factors. Second-stage cross section regressions use this map to back out estimates of the factors at each point in time. Pass 3 : This is a single time series forecasting regression of the target variable $y_{t +1}$ on the second-pass estimated predictive factors …","date":1700489805,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700489805,"objectID":"cb83c79cd79d6d0dabc1fa683061dca9","permalink":"https://ikerlz.github.io/notes/3prf/","publishdate":"2023-11-20T22:16:45+08:00","relpermalink":"/notes/3prf/","section":"notes","summary":"Table of Contents 1. Introduction 2. The three-pass regression filter 2.1. The estimator 2.2. Assumptions 2.3. Consistency 2.4. Asymptotic distributions 2.5. Proxy selection 3. Related procedures 3.1. Constrained least squares 3.","tags":[],"title":"Notes on ''The three-pass regression filter: A new approach to forecasting using many predictors''","type":"notes"},{"authors":[],"categories":["factor-model"],"content":" Table of Contents 0. Background 1. Introduction 2. Large dimensional factor models with random missing 2.1. EM estimation 3. Determining the number of factors via cross validation 4. Monte Carlo simulations 5. Empirical application: Forecasting macroeconomic variables 0. Background 1. Introduction Factor model in balanced panel has been thoroughly investigated. How to handle the missing data problem in factor models ? the expectation–maximization (EM) algorithm the Kalman filter (KF) There is no formal study of the asymptotic properties for the EM estimators of the factors and factor loadings for the PC estimation with missing observations 2. Large dimensional factor models with random missing 2.1. EM estimation 3. Determining the number of factors via cross validation 4. Monte Carlo simulations 5. Empirical application: Forecasting macroeconomic variables ","date":1699971405,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699971405,"objectID":"b7b894a0969bba15a9144064f9b5af4d","permalink":"https://ikerlz.github.io/notes/factor-model/","publishdate":"2023-11-14T22:16:45+08:00","relpermalink":"/notes/factor-model/","section":"notes","summary":"Table of Contents 0. Background 1. Introduction 2. Large dimensional factor models with random missing 2.1. EM estimation 3. Determining the number of factors via cross validation 4. Monte Carlo simulations 5.","tags":[],"title":"Notes on ''On factor models with random missing: EM estimation, inference, and cross validation''","type":"notes"},{"authors":[],"categories":["causal-inference"],"content":" You can locate the original blog post on observablehq.com, written by Herb Susmann.\nTable of Contents 1. Functional 2. Pathwise Derivatives 3. One-step Estimator 4. Pathwise differentiability 5. Influence function 5.1 The Gâteaux derivative 5.2 The influence function References 1. Functional A functional (you could also call it a parameter) is any function that takes in a probability distribution and returns a number (or multiple numbers). For example, the mean is a functional: given a probability distribution, you compute $\\mathbb{E}(X)=\\int x p(x) d x$ and get back a number. Mathematically, we can define the functional as $T(P):\\mathscr{P} \\rightarrow \\mathbb{R}^q$, where ${\\color{red}\\mathscr{P}=\\{P\\}}$ is the family of the distributions.\nA toy example $$ T(P)=\\int p(x)^2 d x\\qquad(\\star) $$\nThis functional doesn’t usually have any practical significance Other functionals might be the average treatment effect in causal inference the probability of survival after a certain time point in survival analysis. For the true data generating distribution $P$ (here, we can use $N(0,1)$), the value of this squared density parameter is $T(P)=0.282$. However, we don’t know the true data generating distribution and we need to estimate it. Once we have an estimate of the distribution, we can “plug it in” to the formula for the functional to get an estimate for the parameter.\nTake 100 samples from the true data generating distribution and use a kernel density estimator to obtain the estimated distribution $\\tilde{P}$, with a corresponding density $\\tilde{p}$. The following FIGURE shows how the estimated $\\tilde{P}$ compares to the truth, $P$.\nThe difference between $\\tilde{P}$ and the truth $P$ The estimate of the functional $(\\star)$ using this estimated distribution is $T(\\tilde P)=0.282$ (off by 1.5% compared to the true value).\nHow would our estimate of the functional change if we could nudge our estimated distribution towards the true distribution ?\nWe define a family of distributions $P_\\epsilon$ which are mixtures of the estimated distribution and the true distribution. The density of $p_\\epsilon$ is given by $$ p_\\epsilon=(1-\\epsilon) p+\\epsilon \\tilde{p} $$ where $\\epsilon\\in[0,1]$. For every value of $\\epsilon$ we can compute a corresponding value of the functional: $T\\left(P_\\epsilon\\right)$. These values trace out a trajectory as they move from the initual guess, $T(\\tilde{P})$, to the true value, $T(P)$, as shown in FIGURE below.\nThe difference between $\\tilde{P}$ and the truth $P$ 2. Pathwise Derivatives A pathwise derivative is the derivative of $T\\left(P_\\epsilon\\right)$ with respect to $\\epsilon$.\nIt’s useful to think of a derivative as a direction: for each value of $\\epsilon$, the pathwise derivative tells us which direction the functional $T\\left(P_\\epsilon\\right)$ is moving.\nThe pathwise derivative at $\\epsilon=1$ When $\\epsilon=1, \\tilde{P}$ is the estimated distribution. The pathwise derivative at that point tells us how our estimate of $T(\\tilde{P})$ would change if we nudged our estimate in the right direction, towards the true distribution.\n3. One-step Estimator We can use this to form a better estimate of the functional by using this pathwise derivative to approximate the trajectory as a linear function (shown in the following FIGURE). This is called a “one-step” estimator, because it’s like performing a single step of Newton’s method.\nThe one-step estimate In any practical setting, we would need to use the observed data to estimate the pathwise derivative. One-step estimators don’t respect any bounds on the target functional, which can lead to nonsensical results. An alternative family of estimators, Targeted Maximum Likelihood Estimators (TMLE), provide one way around this problem 4. Pathwise differentiability A functional is called pathwise differentiable if it’s possible to compute it’s pathwise derivative.\nThis is a desirable property:\nit allows for the creation of one-step estimators it also allows for other techniques like Targeted Maximum Likelihood Estimation (TMLE). Fortunately, many parameters of real-world interest are pathwise differentiable: for example:\nthe average treatment effect for a binary intervention is a pathwise differentiable parameter.\n5. Influence function The term influence function comes from its origins in robust statistics, where it was developed to quantify how much an estimator will change when its input changes (in other words, how much influence each input data point has on the outcome). In robust statistics, they’re mostly concerned with what happens when your data is perturbed in a bad direction, so they can understand how things like outliers will impact an estimator.\nIn our case, we are more interested in how estimators change when our estimates are nudged in the right direction, towards the true data distribution. Notes of Statistical functionals and influence functions\nIs the plug-in estimator $T(\\tilde F)$ a good estimator?\nThe Glivenko-Cantelli …","date":1699193805,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699193805,"objectID":"dc5bcf9e36aac07100e67e43a919dc45","permalink":"https://ikerlz.github.io/notes/path-dev/","publishdate":"2023-11-05T22:16:45+08:00","relpermalink":"/notes/path-dev/","section":"notes","summary":"You can locate the original blog post on observablehq.com, written by Herb Susmann.\nTable of Contents 1. Functional 2. Pathwise Derivatives 3. One-step Estimator 4. Pathwise differentiability 5. Influence function 5.","tags":[],"title":"Notes on ''One-step Estimators and Pathwise Derivatives''","type":"notes"},{"authors":[],"categories":["causal-inference"],"content":" Table of Contents 0. Background: Semiparametric Model 1. Introduction 2. The Pathwise Derivative Formula For the Asymptotic Variance Regular path \u0026amp; regular estimator The pathwise derivative asymptotic variance formula 3. Semiparametric $M$-estimators When will the adjustment term be zero ? A more direct condition 4. Functions of Mean-square Projection and Densities 5. Regularity Conditions 6. Series Estimation of Projection Functionals 7. Power Series Estimators For Semiparametric Individual Effects and Average Derivatives References 0. Background: Semiparametric Model Reference: Lecture notes from Prof. Bodhisattva Sen (Columbia University)\nA semiparametric model is a statistical model that involves both parametric and nonparametric (infinite-dimensional) components. However, we are mostly interested in estimation and inference of a finite-dimensional parameter in the model.\nExample 1 (population mean) Suppose that $X_1, \\ldots, X_n$ are i.i.d. $P$ belonging to the class $\\mathcal{P}$ of distribution. Let $\\psi(P) \\equiv \\mathbb{E}_P\\left[X_1\\right]$, the mean of the distribution, be the parameter of interest.\nQuestion:\nSuppose that $\\mathcal{P}$ is the class of all distributions that have a finite variance. What is the most efficient estimator of $\\psi(P)$, i.e., what is the estimator with the best asymptotic performance? A model $\\mathcal{P}$ is simply a collection of probability distributions for the data we observe. Example 2 (partial linear regression model) Suppose that we observe i.i.d. data $\\left\\{X_i \\equiv\\left(Y_i, Z_i, V_i\\right): i=1, \\ldots, n\\right\\}$ from the following partial linear regression model: $$ Y_i=Z_i^{\\top} \\beta+g\\left(V_i\\right)+\\epsilon_i $$\n$Y_i$ is the scalar response variable $Z_i$ and $V_i$ are vectors of predictors $g(\\cdot)$ is the unknown (nonparametric) function $\\epsilon_i$ is the unobserved error. For simplicity and to focus on the semiparametric nature of the problem:\nAssume that $\\left(Z_i, V_i\\right) \\sim f(\\cdot, \\cdot)$, where we assume that the density $f(\\cdot, \\cdot)$ is known, is independent of $\\epsilon_i \\sim N\\left(0, \\sigma^2\\right)$ (with $\\sigma^2$ known). The model, under these assumptions, has a parametric component $\\beta$ and a nonparametric component $g(\\cdot)$ “Separated semiparametric model”: We say that the model $\\mathcal{P}=\\left\\{P_{\\nu, \\eta}\\right\\}$ is a “separated semiparametric model”, where $\\nu$ is a “Euclidean parameter” and $\\eta$ runs through a nonparametric class of distributions (or some infinite-dimensional set). This gives a semiparametric model in the strict sense, in which we aim at estimating $\\nu$ and consider $\\eta$ as a nuisance parameter.\n“Frequent questions for semiparametric model”: consider the estimation of a parameter of interest $\\nu=\\nu(P)$, where the data has distribution $P \\in \\mathcal{P}$:\n(Q1) How well can we estimate $\\nu=\\nu(P)$ ? What is our “gold standard”? (Q2) Can we compare absolute “in principle” standards for estimation of $\\nu$ in a model $\\mathcal{P}$ with estimation of $\\nu$ in a submodel $\\mathcal{P}_0 \\subset \\mathcal{P}$ ? What is the effect of not knowing $\\eta$ on estimation of $\\nu$ when $\\mathcal{P}=\\left\\{P_\\theta: \\theta \\equiv(\\nu, \\eta) \\in \\Theta\\right\\}$ ? (Q3) How do we construct efficient estimators of $\\nu(P)$ ? A model $\\mathcal{P}$ is simply a collection of probability distributions for the data we observe. 1. Introduction Develop a general form for the asymptotic variance of semiparametric estimators that depend on nonparametric estimators of functions. The formula is often straightforward to derive, requiring only some calculus. Although the formula is not based on primitive conditions, it should be useful for semiparametric estimators, just as analogous formulae are for parametric estimators. The formula gives the form of remainder terms, which facilitates specification of primitive conditions. It can also be used to make asymptotic efficiency comparisons and to find an efficient estimator in some class. - - Derive the formula: **Section 2** - Propositions about semiparametric estimator - **Section 3** - **Section 4** - High-level regularity conditions: **Section 5** - Conditions for $\\sqrt{n}$-consistency and asymptotic normality: **Section 6** - Primitive conditions for the examples: **Section 7** 2. The Pathwise Derivative Formula For the Asymptotic Variance Preliminary: one-step estimators and pathwise derivatives\nThe formula is based on the observation that $\\sqrt{n}$-consistent nonparametric estimators are often efficient.\nFor example, the sample mean is known to be an efficient estimator of the population mean in a nonparametric model where no restrictions, other than regularity conditions (e.g. existence of the second moment) are placed on the distribution of the data.\nIdea\nCalculate the asymptotic variance of a semiparametric estimator as the variance bound for the functional that it nonparametrically estimates. In other words, the formula is the variance bound for …","date":1699193805,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699193805,"objectID":"a6e05984cc9ab3ef73cd3c315289272a","permalink":"https://ikerlz.github.io/notes/var-semiparam/","publishdate":"2023-11-05T22:16:45+08:00","relpermalink":"/notes/var-semiparam/","section":"notes","summary":"Table of Contents 0. Background: Semiparametric Model 1. Introduction 2. The Pathwise Derivative Formula For the Asymptotic Variance Regular path \u0026 regular estimator The pathwise derivative asymptotic variance formula 3.","tags":[],"title":"Notes on ''The Asymptotic Variance of Semiparametric Estimators''","type":"notes"},{"authors":["Zhe Li"],"categories":["Classical Statistics"],"content":" Table of Contents 一、生成混合高斯数据 1、原理 2、Python实现 3、R实现 二、EM算法 1、原理 2、Python实现 3、R实现 4、第三方工具库 三、真实数据 1、Python实现 2、R实现 四、后记 一、生成混合高斯数据 1、原理 利用混合高斯模型的公式：\n$$ P(y \\mid \\theta)=\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y \\mid \\theta_{k}\\right) $$\n只需要给定：\n各组分布的均值向量及协方差矩阵$\\theta_k = \\left(\\mu_k, \\Sigma_k \\right)$ 需要生成的数据总数$N$以及各组权重$\\alpha = (\\alpha_1,\\ldots,\\alpha_K)$ 即可生成GMM数据 注意：$\\Sigma_k$需要是一个对称正定的矩阵\n两种思路：\n思路一 利用权重$\\alpha = (\\alpha_1,\\ldots,\\alpha_K)$进行带权重的抽样，从$[1,2,\\ldots,K]$中抽样$N$次得到一个长度为$N$的列表（向量），记为group_list，第$i$个元素表示数据应该从第$i$个分模型中生成 遍历列表（向量）group_list中的每个元素，若第$i$次遍历中，group_list[i]=k，则表示从$\\mathcal{N}(\\mu_k,\\Sigma_k)$中生成一个数据$X_i$ 将$X_1,\\ldots,X_N$排列合并得到一个矩阵$X=(X_1,\\ldots,X_N)$ 思路二 利用$\\alpha = (\\alpha_1,\\ldots,\\alpha_K)$，乘上总数$N$，得到每个类别大致的数量，记为group_num_list，第$k$个元素表示有group_num_list[k]个数据来源于第$k$个分模型 遍历列表（向量）group_num_list，若在第$k$次遍历中，group_num_list[k]=$n_k$，则表示从$\\mathcal{N}(\\mu_k,\\Sigma_k)$中独立生成$n_k$个数据$X^k=(X_1,\\ldots,X_{n_k})$ 将$X^1,\\ldots,X^k$合并得到矩阵$X=(X_1,\\ldots,X_N)$ 注意：思路一的方法是严格按照GMM的定理来生成的，复杂度为$O(N)$；思路二严格来说与理论有少许差异，但是时间复杂度会低一些，为$O(K)$\n2、Python实现 def generate_gmm_data(self, shuffle=True): \u0026#34;\u0026#34;\u0026#34; :return: 返回一个自变量矩阵X_mat以及一个因变量向量y_vec \u0026#34;\u0026#34;\u0026#34; assert len(self.weight_vec) == self.K # 判断类别数量是否等于参数的数量 num_list = [int(self.total_num * self.weight_vec[k]) for k in range(self.K)] if sum(num_list) != self.total_num: num_list[self.K] += self.total_num - sum(num_list) # 取整后可能存在加和与总数不等的情况，考虑在最后一个组上做处理 assert sum(num_list) == self.total_num for k in range(self.K): if len(self.mean_list[k]) \u0026gt; 1: X_mat_k = np.random.multivariate_normal(self.mean_list[k], self.cov_mat_list[k], num_list[k]) else: X_mat_k = np.random.normal(self.mean_list[k], self.cov_mat_list[k], num_list[k]).reshape( (num_list[k], -1)) y_vec_k = np.array([k for _ in range(num_list[k])]) if not k: X_mat = X_mat_k y_vec = y_vec_k else: X_mat = np.vstack((X_mat, X_mat_k)) y_vec = np.hstack((y_vec, y_vec_k)) if shuffle: shuffle_index = [x for x in range(self.total_num)] random.shuffle(shuffle_index) X_mat = X_mat[shuffle_index] # if len(X_mat.shape) == 3: # X_mat = X_mat[0] # 维度为1的情况会升维 y_vec = y_vec[shuffle_index] self.data = X_mat self.y_vec = y_vec 3、R实现 generate_gmm_data \u0026lt;- function (params_list=NULL, shuffle=TRUE) { # params_list是一个list类型，应当包含以下几个数据 # total_num: 需要生成数据的数量 # weight_vec: 各类别的权重 # mean_list: 均值列表 # cov_mat_list: 协方差矩阵列表 if (is.list(params_list)) { weight_vec \u0026lt;- params_list$weight_vec total_num \u0026lt;- params_list$total_num mean_list \u0026lt;- params_list$mean_list cov_mat_list \u0026lt;- params_list$cov_mat_list cluster_num \u0026lt;- length(weight_vec) } else { # 如果没有给定参数列表，则使用默认设置 weight_vec \u0026lt;- c(0.3, 0.3, 0.4) total_num \u0026lt;- 100 mean_list \u0026lt;- list(c(-0.5), c(0.5), c(0)) cov_mat_list \u0026lt;- list(matrix(1), matrix(1), matrix(1)) cluster_num \u0026lt;- 3 print(\u0026#34;hhh\u0026#34;) } # 判断cluster数量与mean_list和cov_mat_list中元素个数是否相等 if (length(weight_vec) != cluster_num) { stop(\u0026#34;The length of weight vector is not equal to number of cluster\u0026#34;) } if (length(mean_list) != cluster_num) { stop(\u0026#34;The length of mean list is not equal to number of cluster\u0026#34;) } if (length(cov_mat_list) != cluster_num) { stop(\u0026#34;The length of covariance matrix list is not equal to number of cluster\u0026#34;) } # 各类别的数量 num_before_k \u0026lt;- 0 for (k in 1:cluster_num) { if (k \u0026lt; cluster_num) { num_k \u0026lt;- floor(weight_vec[k] * total_num) # 采用向下取整的策略，并在最后一组做处理 num_before_k \u0026lt;- num_before_k + num_k } else { num_k \u0026lt;- total_num - num_before_k } X_mat_k \u0026lt;- rmvnorm(num_k, mean_list[[k]], cov_mat_list[[k]]) # 生成一个num_k * p 维的matrix y_vec_k \u0026lt;- rep(k, num_k) if (k == 1) { X_mat \u0026lt;- X_mat_k y_vec \u0026lt;- y_vec_k } else { # 纵向合并 X_mat \u0026lt;- rbind(X_mat, X_mat_k) y_vec \u0026lt;- c(y_vec, y_vec_k) } } # shuffle if (shuffle) { shuffle_index \u0026lt;- sample(total_num) X_mat \u0026lt;- X_mat[shuffle_index, ] y_vec \u0026lt;- y_vec[shuffle_index] } # 构造返回结果 res \u0026lt;- list(X_mat=as.matrix(X_mat), y_vec=y_vec) return(res) } 由于GMM类似于聚类模型，因此可以使用Python中Sklearn库中的make_blobs函数快速生成数据\n(X, y) = make_blobs(n_samples=[100,300,250,400], n_features=2, centers=[[100,150],[250,400], [600,100],[300,500]], cluster_std=50, random_state=1) plt.style.use(\u0026#34;ggplot\u0026#34;) plt.figure() plt.title(\u0026#34;Data\u0026#34;) plt.scatter(X[:, 0], X[:,1], marker=\u0026#34;o\u0026#34;, c=np.squeeze(y), s=30) 利用Sklearn生成的聚类数据 二、EM算法 1、原理 GMM的EM实现使用的是传统的EM算法框架，Python实现中，主要使用了Numpy做矩阵运算，R实现中，其自带的矩阵运算已经能满足要求，不需要另外导包 由于EM算法本身是一个迭代求解算法，因此需要给出终止条件，在本文的实现中，使用了两个终止条件： 最大迭代次数：max_iter 对数似然更新阈值：eps print_log参数用于控制是否输出每次迭代的信息 思路 首先进行初始化，即初始化$\\alpha_k$、$\\mu_k$、$\\Sigma_k$；可以采取多种初始化方法：\nkmeans方法（第三方包常用） $\\alpha_k$初始化为$\\frac{1}{K}$，$\\mu_k$选择$K$个数据点的值作为初始化的值，$\\Sigma_k$可以选择整体的协方差矩阵作为初始化值 E-Step：利用下式更新$\\Gamma\\in \\mathbb{R}^{N\\times K}$\n$$ \\widehat{\\gamma}_{j k}=\\frac{\\alpha_{k} \\phi\\left(y_{j} \\mid \\theta_{k}\\right)}{\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y_{j} \\mid \\theta_{k}\\right)} $$\nM-Step：利用下式更新待估计的参数 $$ \\widehat{\\mu}_{k}=\\frac{\\sum_{j=1}^{N} \\widehat{\\gamma}_{j k} y_{j}}{\\sum_{j=1}^{N} \\widehat{y}_{j k}}\\quad \\widehat{\\sigma}_{k}^{2}=\\frac{\\sum_{j=1}^{N} \\widehat{\\gamma}_{j k}\\left(y_{j}-\\mu_{k}\\right)^{2}}{\\sum_{j=1}^{N} \\widehat{\\gamma}_{j k}}\\quad \\widehat{\\alpha}_{k}=\\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{j …","date":1698856767,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698856767,"objectID":"a90f6c4f605f68e7632803e124f38604","permalink":"https://ikerlz.github.io/post/gmm/","publishdate":"2023-11-02T00:39:27+08:00","relpermalink":"/post/gmm/","section":"post","summary":"Table of Contents 一、生成混合高斯数据 1、原理 2、Python实现 3、R实现 二、EM算法 1、原理 2、Python实现 3、R实现 4、第三方工具库 三、真实数据 1、Python实现 2、R实现 四、后记 一、生成混合高斯数据 1、原理 利用混合高斯模型的公式：\n$$ P(y \\mid \\theta)=\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y \\mid \\theta_{k}\\right) $$\n只需要给定：","tags":["Code","GMM","Python","R"],"title":"混合高斯模型的代码实现","type":"post"},{"authors":["Zhe Li"],"categories":["Causal Inference"],"content":" Using the Propensity Score in Regressions for Causal Effects $$ \\begin{aligned} \\ \\end{aligned} $$\nLi Zhe $$$$\nSchool of Data Science, Fudan University $$$$\nNovember 1, 2023 Introduction This chapter discusses two simple methods to use the propensity score:\nthe propensity score as a covariate in regressions running regressions weighted by the inverse of the propensity score Reasons:\nthey are easy to implement, which involve only standard statistical software packages for regressions; their properties are comparable to many more complex methods; they can be easily extended to allow for flexible statistical models including machine learning algorithms. Outline Regressions with the propensity score as a covariate Theorem 14.1 Proposition 14.1 $$$$\nRegressions weighted by the inverse of the propensity score Average causal effect Theorem 14.2 Average causal effect on the treated units Table 14.1 Proposition 14.2 Theorem 14.3 Regressions with the propensity score as a covariate $$ \\text { Theorem 11.1 If } Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid X \\text {, then } {\\color{red}Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid e(X)} \\text {. } $$\nBy Theorem 11.1, if unconfoundedness holds conditioning on $X$, then it also holds conditioning on $e(X)$: $\\color{red}{Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid e(X) }.$ Analogous to (10.5), $\\tau$ is also nonparametrically identified by $$ \\tau=E[E\\{Y \\mid Z=1, e(X)\\}-E\\{Y \\mid Z=0, e(X)\\}], $$ $\\Rightarrow$ The simplest regression specification is the OLS fit of $Y$ on $\\{1, Z, e(X)\\}$, with the coefficient of $Z$ as an estimator, denoted by $\\tau_e$: $$ \\arg \\min _{a, b, c} E\\{Y-a-b Z-c e(X)\\}^2 $$ $\\tau_e$ defined as the coefficient of $Z$. It is consistent for $\\tau$ if have a correct propensity score model the outcome model is indeed linear in $Z$ and $e(X)$ $\\tau_e$ estimates $\\tau_{\\mathrm{O}}$ if we have a correct propensity score model even if the outcome model is completely misspecified Regressions with the propensity score as a covariate Theorem 14.1 If $Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid X$, then the coefficient of $Z$ in the OLS fit of $Y$ on $\\{1, Z, e(X)\\}$ equals $$ \\tau_e=\\tau_{\\mathrm{O}}=\\frac{E\\left\\{h_{\\mathrm{O}}(X) \\tau(X)\\right\\}}{E\\left\\{h_{\\mathrm{O}}(X)\\right\\}}, $$ recalling that $h_{\\mathrm{O}}(X)=e(X)\\{1-e(X)\\}$ and $\\tau(X)=E\\{Y(1)-Y(0) \\mid X\\}$.\n$$ \\begin{aligned} \\ \\ \\end{aligned} $$\nCorollary 14.1 If $Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid X$, then\nthe coefficient of $Z-e(X)$ in the OLS fit of $Y$ on $Z-e(X)$ or $\\{1, Z-e(X)\\}$ equals $\\tau_{\\mathrm{O}}$; the coefficient of $Z$ in the OLS fit of $Y$ on $\\{1, Z, e(X), X\\}$ equals $\\tau_{\\mathrm{O}}$. Regressions with the propensity score as a covariate An unusual feature of Theorem 14.1 is that the overlap condition ($0 \u0026lt; e(x) \u0026lt; 1$) is not needed any more. Even if some units have propensity score $e(X)$ equaling 0 or 1, their associate weight $e(X)\\{1-e(X)\\}$ is zero so that they do not contribute anything to the final parameter $\\tau_O$. Frisch–Waugh–Lovell Theorem The Frisch–Waugh–Lovell (FWL) theorem reduces multivariate OLS to univariate OLS and therefore facilitate the understanding and calculation of the OLS coefficients.\nTheorem A2.2 (sample FWL) With data $\\left(Y, X_1, X_2, \\ldots, X_p\\right)$ containing column vectors, the coefficient of $X_1$ equals the coefficient of $\\tilde{X}_1$ in the OLS fit of $Y$ or $\\tilde{Y}$ on $\\tilde{X}_1$, where\n$\\tilde{Y}$ is the residual vector from the OLS fit of $Y$ on $\\left(X_2, \\ldots, X_p\\right)$ $\\tilde{X}_1$ is the residual from the OLS fit of $X_1$ on $\\left(X_2, \\ldots, X_p\\right)$. $$$$\nBased on the FWL theorem, we can obtain $\\tau_e$ in two steps:\nfirst, we obtain the residual $\\tilde{Z}$ from the OLS fit of $Z$ on ${1, e(X)}$; then, we obtain $\\tau_e$ from the OLS fit of $Y$ on $\\tilde{Z}$. Proof of Theorem 14.1 The coefficient of $e(X)$ in the OLS fit of $Z$ on $\\{1, e(X)\\}$ is $$ \\begin{aligned} \\frac{\\operatorname{cov}\\{Z, e(X)\\}}{\\operatorname{var}\\{e(X)\\}} \u0026amp; =\\frac{E[\\operatorname{cov}\\{Z, e(X) \\mid X\\}]+\\operatorname{cov}\\{E(Z \\mid X), e(X)\\}}{\\operatorname{var}\\{e(X)\\}} \\\\ \u0026amp;=\\frac{0+\\operatorname{var}\\{e(X)\\}}{\\operatorname{var}\\{e(X)\\}}=1, \\end{aligned} $$\nthe intercept is $E(Z)-E\\{e(X)\\}=0$ the residual is $\\tilde{Z}=Z-e(X)$ (This makes sense since $Z-e(X)$ is uncorrelated with any function of $X$). Therefore, we can obtain $\\tau_e$ from the univariate OLS fit of $Y$ on $Z-e(X)$ : $$\\small{\\tau_e=\\frac{\\operatorname{cov}\\{Z-e(X), Y\\}}{\\operatorname{var}\\{Z-e(X)\\}}}$$ The denominator simplifies to $$ \\begin{aligned} \\operatorname{var}\\{Z-e(X)\\} \u0026amp; =E\\{Z-e(X)\\}^2 =e(X)+e(X)^2-2 e(X)^2=h_{\\mathrm{O}}(X) \\end{aligned} $$\nProof of Theorem 14.1 The numerator simplifies to $$ \\begin{aligned} \u0026amp; \\operatorname{cov}\\{Z-e(X), Y\\} \\\\ = \u0026amp; E[\\{Z-e(X)\\} Y] \\\\ = \u0026amp; E[\\{Z-e(X)\\} Z Y(1)]+E[\\{Z-e(X)\\}(1-Z) Y(0)] \\\\ \u0026amp; \\quad \\quad \\quad{\\color{red}(\\text { since } Y=Z Y(1)+(1-Z) Y(0))} \\\\ = \u0026amp; E[\\{Z-Z e(X)\\} Y(1)]-E[e(X)(1-Z) Y(0)] \\\\ = \u0026amp; E[Z\\{1-e(X)\\} …","date":1698537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698537600,"objectID":"c9dd82d5ccaec073de3ceacb1b43d0f9","permalink":"https://ikerlz.github.io/slides/causalinferencechapter14/","publishdate":"2023-10-29T00:00:00Z","relpermalink":"/slides/causalinferencechapter14/","section":"slides","summary":"Chapter 14 in \"A First Course in Causal Inference\".","tags":[],"title":"Using the Propensity Score in Regressions for Causal Effects","type":"slides"},{"authors":["Shihao Wu","Zhe Li","Xuening Zhu"],"categories":null,"content":" ","date":1685145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685145600,"objectID":"d036fcb022a3b191f278fccae5a46f36","permalink":"https://ikerlz.github.io/publication/dcd/","publishdate":"2023-07-19T00:00:00Z","relpermalink":"/publication/dcd/","section":"publication","summary":"Community detection for large scale networks is of great importance in modern data analysis. In this work, we develop a distributed spectral clustering algorithm to handle this task. Specifically, we distribute a certain number of pilot network nodes on the master server and the others on worker servers. A spectral clustering algorithm is first conducted on the master to select pseudo centers. Next, the indexes of the pseudo centers are broadcasted to workers to complete the distributed community detection task using an SVD (singular value decomposition) type algorithm. The proposed distributed algorithm has three advantages. First, the communication cost is low, since only the indexes of pseudo centers are communicated. Second, no further iterative algorithm is needed on workers while a “one-shot” computation suffices. Third, both the computational complexity and the storage requirements are much lower compared to using the whole adjacency matrix. We develop a Python package DCD (The Python package is provided in [https://github.com/Ikerlz/dcd](https://github.com/Ikerlz/dcd).) to implement the distributed algorithm on a Spark system and establish theoretical properties with respect to the estimation accuracy and mis-clustering rates under the stochastic block model. Experiments on a variety of synthetic and empirical datasets are carried out to further illustrate the advantages of the methodology.","tags":null,"title":"A Distributed Community Detection Algorithm for Large Scale Networks Under Stochastic Block Models","type":"publication"}]