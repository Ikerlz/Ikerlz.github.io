
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Welcome! I am currently a PhD Student in School of Data Science at Fudan University, under the supervision of Prof. Xuening Zhu. My research interests include econometrics, distributed statistical modelling \u0026amp; inference, network data modelling.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Welcome! I am currently a PhD Student in School of Data Science at Fudan University, under the supervision of Prof. Xuening Zhu. My research interests include econometrics, distributed statistical modelling \u0026 inference, network data modelling.","tags":null,"title":"Li Zhe","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":[],"categories":["causal-inference"],"content":" You can locate the original blog post on observablehq.com, written by Herb Susmann.\nTable of Contents 1. Functional 2. Pathwise Derivatives 3. One-step Estimator 4. Pathwise differentiability 5. Influence function 5.1 The Gâteaux derivative 5.2 The influence function References 1. Functional A functional (you could also call it a parameter) is any function that takes in a probability distribution and returns a number (or multiple numbers). For example, the mean is a functional: given a probability distribution, you compute $\\mathbb{E}(X)=\\int x p(x) d x$ and get back a number. Mathematically, we can define the functional as $T(P):\\mathscr{P} \\rightarrow \\mathbb{R}^q$, where ${\\color{red}\\mathscr{P}=\\{P\\}}$ is the family of the distributions.\nA toy example $$ T(P)=\\int p(x)^2 d x\\qquad(\\star) $$\nThis functional doesn’t usually have any practical significance Other functionals might be the average treatment effect in causal inference the probability of survival after a certain time point in survival analysis. For the true data generating distribution $P$ (here, we can use $N(0,1)$), the value of this squared density parameter is $T(P)=0.282$. However, we don’t know the true data generating distribution and we need to estimate it. Once we have an estimate of the distribution, we can “plug it in” to the formula for the functional to get an estimate for the parameter.\nTake 100 samples from the true data generating distribution and use a kernel density estimator to obtain the estimated distribution $\\tilde{P}$, with a corresponding density $\\tilde{p}$. The following FIGURE shows how the estimated $\\tilde{P}$ compares to the truth, $P$.\nThe difference between $\\tilde{P}$ and the truth $P$ The estimate of the functional $(\\star)$ using this estimated distribution is $T(\\tilde P)=0.282$ (off by 1.5% compared to the true value).\nHow would our estimate of the functional change if we could nudge our estimated distribution towards the true distribution ?\nWe define a family of distributions $P_\\epsilon$ which are mixtures of the estimated distribution and the true distribution. The density of $p_\\epsilon$ is given by $$ p_\\epsilon=(1-\\epsilon) p+\\epsilon \\tilde{p} $$ where $\\epsilon\\in[0,1]$. For every value of $\\epsilon$ we can compute a corresponding value of the functional: $T\\left(P_\\epsilon\\right)$. These values trace out a trajectory as they move from the initual guess, $T(\\tilde{P})$, to the true value, $T(P)$, as shown in FIGURE below.\nThe difference between $\\tilde{P}$ and the truth $P$ 2. Pathwise Derivatives A pathwise derivative is the derivative of $T\\left(P_\\epsilon\\right)$ with respect to $\\epsilon$.\nIt’s useful to think of a derivative as a direction: for each value of $\\epsilon$, the pathwise derivative tells us which direction the functional $T\\left(P_\\epsilon\\right)$ is moving.\nThe pathwise derivative at $\\epsilon=1$ When $\\epsilon=1, \\tilde{P}$ is the estimated distribution. The pathwise derivative at that point tells us how our estimate of $T(\\tilde{P})$ would change if we nudged our estimate in the right direction, towards the true distribution.\n3. One-step Estimator We can use this to form a better estimate of the functional by using this pathwise derivative to approximate the trajectory as a linear function (shown in the following FIGURE). This is called a “one-step” estimator, because it’s like performing a single step of Newton’s method.\nThe one-step estimate In any practical setting, we would need to use the observed data to estimate the pathwise derivative. One-step estimators don’t respect any bounds on the target functional, which can lead to nonsensical results. An alternative family of estimators, Targeted Maximum Likelihood Estimators (TMLE), provide one way around this problem 4. Pathwise differentiability A functional is called pathwise differentiable if it’s possible to compute it’s pathwise derivative.\nThis is a desirable property:\nit allows for the creation of one-step estimators it also allows for other techniques like Targeted Maximum Likelihood Estimation (TMLE). Fortunately, many parameters of real-world interest are pathwise differentiable: for example:\nthe average treatment effect for a binary intervention is a pathwise differentiable parameter.\n5. Influence function The term influence function comes from its origins in robust statistics, where it was developed to quantify how much an estimator will change when its input changes (in other words, how much influence each input data point has on the outcome). In robust statistics, they’re mostly concerned with what happens when your data is perturbed in a bad direction, so they can understand how things like outliers will impact an estimator.\nIn our case, we are more interested in how estimators change when our estimates are nudged in the right direction, towards the true data distribution. Notes of Statistical functionals and influence functions\nIs the plug-in estimator $T(\\tilde F)$ a good estimator?\nThe Glivenko-Cantelli …","date":1699193805,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699193805,"objectID":"d3283c0877339ab5213ebc7517c138b3","permalink":"https://example.com/post/path-dev/","publishdate":"2023-11-05T22:16:45+08:00","relpermalink":"/post/path-dev/","section":"post","summary":"You can locate the original blog post on observablehq.com, written by Herb Susmann.\nTable of Contents 1. Functional 2. Pathwise Derivatives 3. One-step Estimator 4. Pathwise differentiability 5. Influence function 5.","tags":[],"title":"Notes on ''One-step Estimators and Pathwise Derivatives''","type":"post"},{"authors":[],"categories":["causal-inference"],"content":" Table of Contents 0. Background: Semiparametric Model 1. Introduction 2. The Pathwise Derivative Formula For the Asymptotic Variance Regular path \u0026amp; regular estimator The pathwise derivative asymptotic variance formula 3. Semiparametric $M$-estimators When will the adjustment term be zero ? A more direct condition 4. Functions of Mean-square Projection and Densities 5. Regularity Conditions 6. Series Estimation of Projection Functionals 7. Power Series Estimators For Semiparametric Individual Effects and Average Derivatives References 0. Background: Semiparametric Model Reference: Lecture notes from Prof. Bodhisattva Sen (Columbia University)\nA semiparametric model is a statistical model that involves both parametric and nonparametric (infinite-dimensional) components. However, we are mostly interested in estimation and inference of a finite-dimensional parameter in the model.\nExample 1 (population mean) Suppose that $X_1, \\ldots, X_n$ are i.i.d. $P$ belonging to the class $\\mathcal{P}$ of distribution. Let $\\psi(P) \\equiv \\mathbb{E}_P\\left[X_1\\right]$, the mean of the distribution, be the parameter of interest.\nQuestion:\nSuppose that $\\mathcal{P}$ is the class of all distributions that have a finite variance. What is the most efficient estimator of $\\psi(P)$, i.e., what is the estimator with the best asymptotic performance? A model $\\mathcal{P}$ is simply a collection of probability distributions for the data we observe. Example 2 (partial linear regression model) Suppose that we observe i.i.d. data $\\left\\{X_i \\equiv\\left(Y_i, Z_i, V_i\\right): i=1, \\ldots, n\\right\\}$ from the following partial linear regression model: $$ Y_i=Z_i^{\\top} \\beta+g\\left(V_i\\right)+\\epsilon_i $$\n$Y_i$ is the scalar response variable $Z_i$ and $V_i$ are vectors of predictors $g(\\cdot)$ is the unknown (nonparametric) function $\\epsilon_i$ is the unobserved error. For simplicity and to focus on the semiparametric nature of the problem:\nAssume that $\\left(Z_i, V_i\\right) \\sim f(\\cdot, \\cdot)$, where we assume that the density $f(\\cdot, \\cdot)$ is known, is independent of $\\epsilon_i \\sim N\\left(0, \\sigma^2\\right)$ (with $\\sigma^2$ known). The model, under these assumptions, has a parametric component $\\beta$ and a nonparametric component $g(\\cdot)$ “Separated semiparametric model”: We say that the model $\\mathcal{P}=\\left\\{P_{\\nu, \\eta}\\right\\}$ is a “separated semiparametric model”, where $\\nu$ is a “Euclidean parameter” and $\\eta$ runs through a nonparametric class of distributions (or some infinite-dimensional set). This gives a semiparametric model in the strict sense, in which we aim at estimating $\\nu$ and consider $\\eta$ as a nuisance parameter.\n“Frequent questions for semiparametric model”: consider the estimation of a parameter of interest $\\nu=\\nu(P)$, where the data has distribution $P \\in \\mathcal{P}$:\n(Q1) How well can we estimate $\\nu=\\nu(P)$ ? What is our “gold standard”? (Q2) Can we compare absolute “in principle” standards for estimation of $\\nu$ in a model $\\mathcal{P}$ with estimation of $\\nu$ in a submodel $\\mathcal{P}_0 \\subset \\mathcal{P}$ ? What is the effect of not knowing $\\eta$ on estimation of $\\nu$ when $\\mathcal{P}=\\left\\{P_\\theta: \\theta \\equiv(\\nu, \\eta) \\in \\Theta\\right\\}$ ? (Q3) How do we construct efficient estimators of $\\nu(P)$ ? A model $\\mathcal{P}$ is simply a collection of probability distributions for the data we observe. 1. Introduction Develop a general form for the asymptotic variance of semiparametric estimators that depend on nonparametric estimators of functions. The formula is often straightforward to derive, requiring only some calculus. Although the formula is not based on primitive conditions, it should be useful for semiparametric estimators, just as analogous formulae are for parametric estimators. The formula gives the form of remainder terms, which facilitates specification of primitive conditions. It can also be used to make asymptotic efficiency comparisons and to find an efficient estimator in some class. - - Derive the formula: **Section 2** - Propositions about semiparametric estimator - **Section 3** - **Section 4** - High-level regularity conditions: **Section 5** - Conditions for $\\sqrt{n}$-consistency and asymptotic normality: **Section 6** - Primitive conditions for the examples: **Section 7** 2. The Pathwise Derivative Formula For the Asymptotic Variance Preliminary: one-step estimators and pathwise derivatives\nThe formula is based on the observation that $\\sqrt{n}$-consistent nonparametric estimators are often efficient.\nFor example, the sample mean is known to be an efficient estimator of the population mean in a nonparametric model where no restrictions, other than regularity conditions (e.g. existence of the second moment) are placed on the distribution of the data.\nIdea\nCalculate the asymptotic variance of a semiparametric estimator as the variance bound for the functional that it nonparametrically estimates. In other words, the formula is the variance bound for …","date":1699193805,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699193805,"objectID":"12600186049f853d85a45f7f946b1deb","permalink":"https://example.com/post/var-semiparam/","publishdate":"2023-11-05T22:16:45+08:00","relpermalink":"/post/var-semiparam/","section":"post","summary":"Table of Contents 0. Background: Semiparametric Model 1. Introduction 2. The Pathwise Derivative Formula For the Asymptotic Variance Regular path \u0026 regular estimator The pathwise derivative asymptotic variance formula 3.","tags":[],"title":"Notes on ''The Asymptotic Variance of Semiparametric Estimators''","type":"post"},{"authors":["Zhe Li"],"categories":["Classical Statistics"],"content":" Table of Contents 一、生成混合高斯数据 1、原理 2、Python实现 3、R实现 二、EM算法 1、原理 2、Python实现 3、R实现 4、第三方工具库 三、真实数据 1、Python实现 2、R实现 四、后记 一、生成混合高斯数据 1、原理 利用混合高斯模型的公式：\n$$ P(y \\mid \\theta)=\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y \\mid \\theta_{k}\\right) $$\n只需要给定：\n各组分布的均值向量及协方差矩阵$\\theta_k = \\left(\\mu_k, \\Sigma_k \\right)$ 需要生成的数据总数$N$以及各组权重$\\alpha = (\\alpha_1,\\ldots,\\alpha_K)$ 即可生成GMM数据 注意：$\\Sigma_k$需要是一个对称正定的矩阵\n两种思路：\n思路一 利用权重$\\alpha = (\\alpha_1,\\ldots,\\alpha_K)$进行带权重的抽样，从$[1,2,\\ldots,K]$中抽样$N$次得到一个长度为$N$的列表（向量），记为group_list，第$i$个元素表示数据应该从第$i$个分模型中生成 遍历列表（向量）group_list中的每个元素，若第$i$次遍历中，group_list[i]=k，则表示从$\\mathcal{N}(\\mu_k,\\Sigma_k)$中生成一个数据$X_i$ 将$X_1,\\ldots,X_N$排列合并得到一个矩阵$X=(X_1,\\ldots,X_N)$ 思路二 利用$\\alpha = (\\alpha_1,\\ldots,\\alpha_K)$，乘上总数$N$，得到每个类别大致的数量，记为group_num_list，第$k$个元素表示有group_num_list[k]个数据来源于第$k$个分模型 遍历列表（向量）group_num_list，若在第$k$次遍历中，group_num_list[k]=$n_k$，则表示从$\\mathcal{N}(\\mu_k,\\Sigma_k)$中独立生成$n_k$个数据$X^k=(X_1,\\ldots,X_{n_k})$ 将$X^1,\\ldots,X^k$合并得到矩阵$X=(X_1,\\ldots,X_N)$ 注意：思路一的方法是严格按照GMM的定理来生成的，复杂度为$O(N)$；思路二严格来说与理论有少许差异，但是时间复杂度会低一些，为$O(K)$\n2、Python实现 def generate_gmm_data(self, shuffle=True): \u0026#34;\u0026#34;\u0026#34; :return: 返回一个自变量矩阵X_mat以及一个因变量向量y_vec \u0026#34;\u0026#34;\u0026#34; assert len(self.weight_vec) == self.K # 判断类别数量是否等于参数的数量 num_list = [int(self.total_num * self.weight_vec[k]) for k in range(self.K)] if sum(num_list) != self.total_num: num_list[self.K] += self.total_num - sum(num_list) # 取整后可能存在加和与总数不等的情况，考虑在最后一个组上做处理 assert sum(num_list) == self.total_num for k in range(self.K): if len(self.mean_list[k]) \u0026gt; 1: X_mat_k = np.random.multivariate_normal(self.mean_list[k], self.cov_mat_list[k], num_list[k]) else: X_mat_k = np.random.normal(self.mean_list[k], self.cov_mat_list[k], num_list[k]).reshape( (num_list[k], -1)) y_vec_k = np.array([k for _ in range(num_list[k])]) if not k: X_mat = X_mat_k y_vec = y_vec_k else: X_mat = np.vstack((X_mat, X_mat_k)) y_vec = np.hstack((y_vec, y_vec_k)) if shuffle: shuffle_index = [x for x in range(self.total_num)] random.shuffle(shuffle_index) X_mat = X_mat[shuffle_index] # if len(X_mat.shape) == 3: # X_mat = X_mat[0] # 维度为1的情况会升维 y_vec = y_vec[shuffle_index] self.data = X_mat self.y_vec = y_vec 3、R实现 generate_gmm_data \u0026lt;- function (params_list=NULL, shuffle=TRUE) { # params_list是一个list类型，应当包含以下几个数据 # total_num: 需要生成数据的数量 # weight_vec: 各类别的权重 # mean_list: 均值列表 # cov_mat_list: 协方差矩阵列表 if (is.list(params_list)) { weight_vec \u0026lt;- params_list$weight_vec total_num \u0026lt;- params_list$total_num mean_list \u0026lt;- params_list$mean_list cov_mat_list \u0026lt;- params_list$cov_mat_list cluster_num \u0026lt;- length(weight_vec) } else { # 如果没有给定参数列表，则使用默认设置 weight_vec \u0026lt;- c(0.3, 0.3, 0.4) total_num \u0026lt;- 100 mean_list \u0026lt;- list(c(-0.5), c(0.5), c(0)) cov_mat_list \u0026lt;- list(matrix(1), matrix(1), matrix(1)) cluster_num \u0026lt;- 3 print(\u0026#34;hhh\u0026#34;) } # 判断cluster数量与mean_list和cov_mat_list中元素个数是否相等 if (length(weight_vec) != cluster_num) { stop(\u0026#34;The length of weight vector is not equal to number of cluster\u0026#34;) } if (length(mean_list) != cluster_num) { stop(\u0026#34;The length of mean list is not equal to number of cluster\u0026#34;) } if (length(cov_mat_list) != cluster_num) { stop(\u0026#34;The length of covariance matrix list is not equal to number of cluster\u0026#34;) } # 各类别的数量 num_before_k \u0026lt;- 0 for (k in 1:cluster_num) { if (k \u0026lt; cluster_num) { num_k \u0026lt;- floor(weight_vec[k] * total_num) # 采用向下取整的策略，并在最后一组做处理 num_before_k \u0026lt;- num_before_k + num_k } else { num_k \u0026lt;- total_num - num_before_k } X_mat_k \u0026lt;- rmvnorm(num_k, mean_list[[k]], cov_mat_list[[k]]) # 生成一个num_k * p 维的matrix y_vec_k \u0026lt;- rep(k, num_k) if (k == 1) { X_mat \u0026lt;- X_mat_k y_vec \u0026lt;- y_vec_k } else { # 纵向合并 X_mat \u0026lt;- rbind(X_mat, X_mat_k) y_vec \u0026lt;- c(y_vec, y_vec_k) } } # shuffle if (shuffle) { shuffle_index \u0026lt;- sample(total_num) X_mat \u0026lt;- X_mat[shuffle_index, ] y_vec \u0026lt;- y_vec[shuffle_index] } # 构造返回结果 res \u0026lt;- list(X_mat=as.matrix(X_mat), y_vec=y_vec) return(res) } 由于GMM类似于聚类模型，因此可以使用Python中Sklearn库中的make_blobs函数快速生成数据\n(X, y) = make_blobs(n_samples=[100,300,250,400], n_features=2, centers=[[100,150],[250,400], [600,100],[300,500]], cluster_std=50, random_state=1) plt.style.use(\u0026#34;ggplot\u0026#34;) plt.figure() plt.title(\u0026#34;Data\u0026#34;) plt.scatter(X[:, 0], X[:,1], marker=\u0026#34;o\u0026#34;, c=np.squeeze(y), s=30) 利用Sklearn生成的聚类数据 二、EM算法 1、原理 GMM的EM实现使用的是传统的EM算法框架，Python实现中，主要使用了Numpy做矩阵运算，R实现中，其自带的矩阵运算已经能满足要求，不需要另外导包 由于EM算法本身是一个迭代求解算法，因此需要给出终止条件，在本文的实现中，使用了两个终止条件： 最大迭代次数：max_iter 对数似然更新阈值：eps print_log参数用于控制是否输出每次迭代的信息 思路 首先进行初始化，即初始化$\\alpha_k$、$\\mu_k$、$\\Sigma_k$；可以采取多种初始化方法：\nkmeans方法（第三方包常用） $\\alpha_k$初始化为$\\frac{1}{K}$，$\\mu_k$选择$K$个数据点的值作为初始化的值，$\\Sigma_k$可以选择整体的协方差矩阵作为初始化值 E-Step：利用下式更新$\\Gamma\\in \\mathbb{R}^{N\\times K}$\n$$ \\widehat{\\gamma}_{j k}=\\frac{\\alpha_{k} \\phi\\left(y_{j} \\mid \\theta_{k}\\right)}{\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y_{j} \\mid \\theta_{k}\\right)} $$\nM-Step：利用下式更新待估计的参数 $$ \\widehat{\\mu}_{k}=\\frac{\\sum_{j=1}^{N} \\widehat{\\gamma}_{j k} y_{j}}{\\sum_{j=1}^{N} \\widehat{y}_{j k}}\\quad \\widehat{\\sigma}_{k}^{2}=\\frac{\\sum_{j=1}^{N} \\widehat{\\gamma}_{j k}\\left(y_{j}-\\mu_{k}\\right)^{2}}{\\sum_{j=1}^{N} \\widehat{\\gamma}_{j k}}\\quad \\widehat{\\alpha}_{k}=\\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{j …","date":1698856767,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698856767,"objectID":"a90f6c4f605f68e7632803e124f38604","permalink":"https://example.com/post/gmm/","publishdate":"2023-11-02T00:39:27+08:00","relpermalink":"/post/gmm/","section":"post","summary":"Table of Contents 一、生成混合高斯数据 1、原理 2、Python实现 3、R实现 二、EM算法 1、原理 2、Python实现 3、R实现 4、第三方工具库 三、真实数据 1、Python实现 2、R实现 四、后记 一、生成混合高斯数据 1、原理 利用混合高斯模型的公式：\n$$ P(y \\mid \\theta)=\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y \\mid \\theta_{k}\\right) $$\n只需要给定：","tags":["Code","GMM","Python","R"],"title":"混合高斯模型的代码实现","type":"post"},{"authors":["Zhe Li"],"categories":["Causal Inference"],"content":" Using the Propensity Score in Regressions for Causal Effects $$ \\begin{aligned} \\ \\end{aligned} $$\nLi Zhe $$$$\nSchool of Data Science, Fudan University $$$$\nNovember 1, 2023 Introduction This chapter discusses two simple methods to use the propensity score:\nthe propensity score as a covariate in regressions running regressions weighted by the inverse of the propensity score Reasons:\nthey are easy to implement, which involve only standard statistical software packages for regressions; their properties are comparable to many more complex methods; they can be easily extended to allow for flexible statistical models including machine learning algorithms. Outline Regressions with the propensity score as a covariate Theorem 14.1 Proposition 14.1 $$$$\nRegressions weighted by the inverse of the propensity score Average causal effect Theorem 14.2 Average causal effect on the treated units Table 14.1 Proposition 14.2 Theorem 14.3 Regressions with the propensity score as a covariate $$ \\text { Theorem 11.1 If } Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid X \\text {, then } {\\color{red}Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid e(X)} \\text {. } $$\nBy Theorem 11.1, if unconfoundedness holds conditioning on $X$, then it also holds conditioning on $e(X)$: $\\color{red}{Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid e(X) }.$ Analogous to (10.5), $\\tau$ is also nonparametrically identified by $$ \\tau=E[E\\{Y \\mid Z=1, e(X)\\}-E\\{Y \\mid Z=0, e(X)\\}], $$ $\\Rightarrow$ The simplest regression specification is the OLS fit of $Y$ on $\\{1, Z, e(X)\\}$, with the coefficient of $Z$ as an estimator, denoted by $\\tau_e$: $$ \\arg \\min _{a, b, c} E\\{Y-a-b Z-c e(X)\\}^2 $$ $\\tau_e$ defined as the coefficient of $Z$. It is consistent for $\\tau$ if have a correct propensity score model the outcome model is indeed linear in $Z$ and $e(X)$ $\\tau_e$ estimates $\\tau_{\\mathrm{O}}$ if we have a correct propensity score model even if the outcome model is completely misspecified Regressions with the propensity score as a covariate Theorem 14.1 If $Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid X$, then the coefficient of $Z$ in the OLS fit of $Y$ on $\\{1, Z, e(X)\\}$ equals $$ \\tau_e=\\tau_{\\mathrm{O}}=\\frac{E\\left\\{h_{\\mathrm{O}}(X) \\tau(X)\\right\\}}{E\\left\\{h_{\\mathrm{O}}(X)\\right\\}}, $$ recalling that $h_{\\mathrm{O}}(X)=e(X)\\{1-e(X)\\}$ and $\\tau(X)=E\\{Y(1)-Y(0) \\mid X\\}$.\n$$ \\begin{aligned} \\ \\ \\end{aligned} $$\nCorollary 14.1 If $Z \\perp\\!\\!\\!\\perp\\{Y(1), Y(0)\\} \\mid X$, then\nthe coefficient of $Z-e(X)$ in the OLS fit of $Y$ on $Z-e(X)$ or $\\{1, Z-e(X)\\}$ equals $\\tau_{\\mathrm{O}}$; the coefficient of $Z$ in the OLS fit of $Y$ on $\\{1, Z, e(X), X\\}$ equals $\\tau_{\\mathrm{O}}$. Regressions with the propensity score as a covariate An unusual feature of Theorem 14.1 is that the overlap condition ($0 \u0026lt; e(x) \u0026lt; 1$) is not needed any more. Even if some units have propensity score $e(X)$ equaling 0 or 1, their associate weight $e(X)\\{1-e(X)\\}$ is zero so that they do not contribute anything to the final parameter $\\tau_O$. Frisch–Waugh–Lovell Theorem The Frisch–Waugh–Lovell (FWL) theorem reduces multivariate OLS to univariate OLS and therefore facilitate the understanding and calculation of the OLS coefficients.\nTheorem A2.2 (sample FWL) With data $\\left(Y, X_1, X_2, \\ldots, X_p\\right)$ containing column vectors, the coefficient of $X_1$ equals the coefficient of $\\tilde{X}_1$ in the OLS fit of $Y$ or $\\tilde{Y}$ on $\\tilde{X}_1$, where\n$\\tilde{Y}$ is the residual vector from the OLS fit of $Y$ on $\\left(X_2, \\ldots, X_p\\right)$ $\\tilde{X}_1$ is the residual from the OLS fit of $X_1$ on $\\left(X_2, \\ldots, X_p\\right)$. $$$$\nBased on the FWL theorem, we can obtain $\\tau_e$ in two steps:\nfirst, we obtain the residual $\\tilde{Z}$ from the OLS fit of $Z$ on ${1, e(X)}$; then, we obtain $\\tau_e$ from the OLS fit of $Y$ on $\\tilde{Z}$. Proof of Theorem 14.1 The coefficient of $e(X)$ in the OLS fit of $Z$ on $\\{1, e(X)\\}$ is $$ \\begin{aligned} \\frac{\\operatorname{cov}\\{Z, e(X)\\}}{\\operatorname{var}\\{e(X)\\}} \u0026amp; =\\frac{E[\\operatorname{cov}\\{Z, e(X) \\mid X\\}]+\\operatorname{cov}\\{E(Z \\mid X), e(X)\\}}{\\operatorname{var}\\{e(X)\\}} \\\\ \u0026amp;=\\frac{0+\\operatorname{var}\\{e(X)\\}}{\\operatorname{var}\\{e(X)\\}}=1, \\end{aligned} $$\nthe intercept is $E(Z)-E\\{e(X)\\}=0$ the residual is $\\tilde{Z}=Z-e(X)$ (This makes sense since $Z-e(X)$ is uncorrelated with any function of $X$). Therefore, we can obtain $\\tau_e$ from the univariate OLS fit of $Y$ on $Z-e(X)$ : $$\\small{\\tau_e=\\frac{\\operatorname{cov}\\{Z-e(X), Y\\}}{\\operatorname{var}\\{Z-e(X)\\}}}$$ The denominator simplifies to $$ \\begin{aligned} \\operatorname{var}\\{Z-e(X)\\} \u0026amp; =E\\{Z-e(X)\\}^2 =e(X)+e(X)^2-2 e(X)^2=h_{\\mathrm{O}}(X) \\end{aligned} $$\nProof of Theorem 14.1 The numerator simplifies to $$ \\begin{aligned} \u0026amp; \\operatorname{cov}\\{Z-e(X), Y\\} \\\\ = \u0026amp; E[\\{Z-e(X)\\} Y] \\\\ = \u0026amp; E[\\{Z-e(X)\\} Z Y(1)]+E[\\{Z-e(X)\\}(1-Z) Y(0)] \\\\ \u0026amp; \\quad \\quad \\quad{\\color{red}(\\text { since } Y=Z Y(1)+(1-Z) Y(0))} \\\\ = \u0026amp; E[\\{Z-Z e(X)\\} Y(1)]-E[e(X)(1-Z) Y(0)] \\\\ = \u0026amp; E[Z\\{1-e(X)\\} …","date":1698537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698537600,"objectID":"c9dd82d5ccaec073de3ceacb1b43d0f9","permalink":"https://example.com/slides/causalinferencechapter14/","publishdate":"2023-10-29T00:00:00Z","relpermalink":"/slides/causalinferencechapter14/","section":"slides","summary":"Chapter 14 in \"A First Course in Causal Inference\".","tags":[],"title":"Using the Propensity Score in Regressions for Causal Effects","type":"slides"},{"authors":["Shihao Wu","Zhe Li","Xuening Zhu"],"categories":null,"content":" ","date":1685145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685145600,"objectID":"d036fcb022a3b191f278fccae5a46f36","permalink":"https://example.com/publication/dcd/","publishdate":"2023-07-19T00:00:00Z","relpermalink":"/publication/dcd/","section":"publication","summary":"Community detection for large scale networks is of great importance in modern data analysis. In this work, we develop a distributed spectral clustering algorithm to handle this task. Specifically, we distribute a certain number of pilot network nodes on the master server and the others on worker servers. A spectral clustering algorithm is first conducted on the master to select pseudo centers. Next, the indexes of the pseudo centers are broadcasted to workers to complete the distributed community detection task using an SVD (singular value decomposition) type algorithm. The proposed distributed algorithm has three advantages. First, the communication cost is low, since only the indexes of pseudo centers are communicated. Second, no further iterative algorithm is needed on workers while a “one-shot” computation suffices. Third, both the computational complexity and the storage requirements are much lower compared to using the whole adjacency matrix. We develop a Python package DCD (The Python package is provided in [https://github.com/Ikerlz/dcd](https://github.com/Ikerlz/dcd).) to implement the distributed algorithm on a Spark system and establish theoretical properties with respect to the estimation accuracy and mis-clustering rates under the stochastic block model. Experiments on a variety of synthetic and empirical datasets are carried out to further illustrate the advantages of the methodology.","tags":null,"title":"A Distributed Community Detection Algorithm for Large Scale Networks Under Stochastic Block Models","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://example.com/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://example.com/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"}]