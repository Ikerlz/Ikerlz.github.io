<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zhe Li</title>
    <link>https://ikerlz.github.io/</link>
      <atom:link href="https://ikerlz.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Zhe Li</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ikerlz.github.io/media/icon_hu9ccd2acdcd774e20fa34966445b706a8_6997380_512x512_fill_lanczos_center_3.png</url>
      <title>Zhe Li</title>
      <link>https://ikerlz.github.io/</link>
    </image>
    
    <item>
      <title>扩散模型（Diffusion Model）</title>
      <link>https://ikerlz.github.io/post/ddpm/</link>
      <pubDate>Fri, 05 Apr 2024 00:39:27 +0800</pubDate>
      <guid>https://ikerlz.github.io/post/ddpm/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;主要针对扩散模型的一些经典文章写一些个人理解，博采众长，参考了很多博客、文章，详细信息见&lt;a href=&#34;#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae&#34;&gt;参考文献&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;


&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#0文生图片模型&#34;&gt;0、文生图片模型&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#1几种生成模型的对比&#34;&gt;1、几种生成模型的对比&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2扩散模型ddpm&#34;&gt;2、扩散模型（DDPM）&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-基于分数的生成模型score-based-generative-models&#34;&gt;3 、基于分数的生成模型（Score-based generative models）&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#参考文献&#34;&gt;参考文献&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;h3 id=&#34;0文生图片模型&#34;&gt;0、文生图片模型&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DALL·E 3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig1_hu1e16810cab0fdef8a3c7f055d86628b1_315476_d839474c362ab19b6de2a13f08f9b3fb.webp 400w,
               /media/posts/ddpm/fig1_hu1e16810cab0fdef8a3c7f055d86628b1_315476_3aa66a8c57ca28c59135515ff4051399.webp 760w,
               /media/posts/ddpm/fig1_hu1e16810cab0fdef8a3c7f055d86628b1_315476_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig1_hu1e16810cab0fdef8a3c7f055d86628b1_315476_d839474c362ab19b6de2a13f08f9b3fb.webp&#34;
               width=&#34;760&#34;
               height=&#34;625&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;扩散模型的大火始于2020年所提出的DDPM（&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Denoising Diffusion Probabilistic Models&amp;rdquo;&lt;/a&gt;）。当前最先进的两个文本生成图像——OpenAI的DALL·E 3和Google的Imagen 2，都是基于扩散模型来完成的。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1几种生成模型的对比&#34;&gt;1、几种生成模型的对比&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig2_hucb6f640be08c90d6178259e0eccf4c36_525595_ed85736f85e08b4a3879ed404c3baed6.webp 400w,
               /media/posts/ddpm/fig2_hucb6f640be08c90d6178259e0eccf4c36_525595_ff4c893122c8087b276f842eb842bff1.webp 760w,
               /media/posts/ddpm/fig2_hucb6f640be08c90d6178259e0eccf4c36_525595_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig2_hucb6f640be08c90d6178259e0eccf4c36_525595_ed85736f85e08b4a3879ed404c3baed6.webp&#34;
               width=&#34;760&#34;
               height=&#34;526&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GAN（生成对抗网络）&lt;/strong&gt;：GAN是由两部分组成，一个生成器和一个判别器。生成器的目标是创建足够真实的数据，以至于判别器不能区分生成的数据和真实数据。判别器的目标是正确区分真实数据和生成器生成的假数据。这两部分在训练过程中相互竞争，推动彼此的进步，因此称为对抗网络。GAN在图像生成方面尤其出色。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VAE（变分自编码器）&lt;/strong&gt;：VAE采用不同的方法来生成数据。它通过编码器将数据映射到一个分布上，并从这个分布中采样来构造一个解码器用于数据重建。它是一种通过概率方法生成新数据的模型，通常用于生成遵循特定统计分布的图片。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Flow-based Models（基于流的模型）&lt;/strong&gt;：这类模型使用可逆变换来学习数据的分布，这意味着它们可以精确地计算生成数据的概率。它们可以生成高质量的数据，并且给定新数据，也可以确定其概率。这种特性在密度估计和无损压缩方面特别有用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Diffusion Models（扩散模型）&lt;/strong&gt;：Diffusion Models 的灵感来自non-equilibrium thermodynamics （非平衡热力学）。理论首先定义扩散步骤的马尔可夫链，以缓慢地将随机噪声添加到数据中，然后学习逆向扩散过程以从噪声中构造所需的数据样本。与 VAE 或流模型不同，扩散模型是通过固定过程学习，并且隐空间具有比较高的维度。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2扩散模型ddpm&#34;&gt;2、扩散模型（DDPM）&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;主要参考&lt;a href=&#34;https://www.bilibili.com/video/BV14c411J7f2/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=503c784d065197393158da59fb3ef766&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;B站：李宏毅老师的 Diffusion Model 讲解&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;考虑下图瑞士卷形状的二维联合概率分布$p(x,y)$，扩散过程$q$非常直观，本来集中有序的样本点，受到噪声的扰动，向外扩散，最终变成一个完全无序的噪声分布
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig3_hu821fc73d96bb670162bc2e51d59cab7b_260304_34a2474f2f2ea2383b67148180ef013a.webp 400w,
               /media/posts/ddpm/fig3_hu821fc73d96bb670162bc2e51d59cab7b_260304_4da38fe1b89fcdda19319a92aa3ae4e5.webp 760w,
               /media/posts/ddpm/fig3_hu821fc73d96bb670162bc2e51d59cab7b_260304_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig3_hu821fc73d96bb670162bc2e51d59cab7b_260304_34a2474f2f2ea2383b67148180ef013a.webp&#34;
               width=&#34;760&#34;
               height=&#34;351&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

而diffusion model其实是图上的这个逆过程，将一个噪声分布$N(0,I)$逐步地去噪以映射到$p_{data}$，有了这样的映射，我们从噪声分布中采样，最终可以得到一张想要的图像。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig4_hu26b6188eab507b98056abd87a587f2b1_543246_85386dae27368958589a975cdaf9855b.webp 400w,
               /media/posts/ddpm/fig4_hu26b6188eab507b98056abd87a587f2b1_543246_6984f6ad32b9351b46424051252376ae.webp 760w,
               /media/posts/ddpm/fig4_hu26b6188eab507b98056abd87a587f2b1_543246_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig4_hu26b6188eab507b98056abd87a587f2b1_543246_85386dae27368958589a975cdaf9855b.webp&#34;
               width=&#34;760&#34;
               height=&#34;314&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;21-diffusion-model原理&#34;&gt;2.1 Diffusion Model原理&lt;/h4&gt;
&lt;p&gt;Diffusion Models由正向过程（或扩散过程）和反向过程（或逆扩散过程）组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;逆向过程（Inverse Process）
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig5_hu7aea3f01016767d9da2ee599072df153_1208336_9b25e9a8239fa8496ba565538225f224.webp 400w,
               /media/posts/ddpm/fig5_hu7aea3f01016767d9da2ee599072df153_1208336_094c4a172f0574a17d93022df9adbfc3.webp 760w,
               /media/posts/ddpm/fig5_hu7aea3f01016767d9da2ee599072df153_1208336_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig5_hu7aea3f01016767d9da2ee599072df153_1208336_9b25e9a8239fa8496ba565538225f224.webp&#34;
               width=&#34;760&#34;
               height=&#34;348&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;正向过程（Forward Process）
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig10_hu8bfdc2e6fec0bc662b834b86bb424c05_150998_1fb069f3e54917294e3766e21cf44262.webp 400w,
               /media/posts/ddpm/fig10_hu8bfdc2e6fec0bc662b834b86bb424c05_150998_100888f7b50df7e19e3d9bea49401203.webp 760w,
               /media/posts/ddpm/fig10_hu8bfdc2e6fec0bc662b834b86bb424c05_150998_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig10_hu8bfdc2e6fec0bc662b834b86bb424c05_150998_1fb069f3e54917294e3766e21cf44262.webp&#34;
               width=&#34;760&#34;
               height=&#34;137&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;原文算法框&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig25_hub27644e6e15658d60eb80af72eb8a3ba_82961_a431166f893a36fca0446b0f9a5059df.webp 400w,
               /media/posts/ddpm/fig25_hub27644e6e15658d60eb80af72eb8a3ba_82961_f5cf4e8c3068d0466d3d34b6c726cec0.webp 760w,
               /media/posts/ddpm/fig25_hub27644e6e15658d60eb80af72eb8a3ba_82961_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig25_hub27644e6e15658d60eb80af72eb8a3ba_82961_a431166f893a36fca0446b0f9a5059df.webp&#34;
               width=&#34;760&#34;
               height=&#34;182&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Denoise Module&lt;/strong&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig6_hu0e91f38c452b01d88480c874d04bb6e9_434502_0a8f4e55e2da1824e098648a7f1576a0.webp 400w,
               /media/posts/ddpm/fig6_hu0e91f38c452b01d88480c874d04bb6e9_434502_4ff309f3f21cd1832811e7716496d175.webp 760w,
               /media/posts/ddpm/fig6_hu0e91f38c452b01d88480c874d04bb6e9_434502_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig6_hu0e91f38c452b01d88480c874d04bb6e9_434502_0a8f4e55e2da1824e098648a7f1576a0.webp&#34;
               width=&#34;760&#34;
               height=&#34;336&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如何训练&lt;/strong&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig7_hue760546b5339ff73f2e1c0885d259cd7_2140973_54aa961493db2ced61dbbe6ebdd2e34a.webp 400w,
               /media/posts/ddpm/fig7_hue760546b5339ff73f2e1c0885d259cd7_2140973_d7f707455c1f6e5260a220b5f55ae850.webp 760w,
               /media/posts/ddpm/fig7_hue760546b5339ff73f2e1c0885d259cd7_2140973_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig7_hue760546b5339ff73f2e1c0885d259cd7_2140973_54aa961493db2ced61dbbe6ebdd2e34a.webp&#34;
               width=&#34;760&#34;
               height=&#34;386&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如何从文本生成图片&lt;/strong&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig8_hu07a283b9f2fe30b8f42b861660e27d35_1239135_d0dca45ce64a4972fe2e9672fd298e17.webp 400w,
               /media/posts/ddpm/fig8_hu07a283b9f2fe30b8f42b861660e27d35_1239135_b1adb5541035b0da3c2cd1cb0cc6bc41.webp 760w,
               /media/posts/ddpm/fig8_hu07a283b9f2fe30b8f42b861660e27d35_1239135_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig8_hu07a283b9f2fe30b8f42b861660e27d35_1239135_d0dca45ce64a4972fe2e9672fd298e17.webp&#34;
               width=&#34;760&#34;
               height=&#34;322&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig9_hu77b70742483f8616b3f219bbbb608587_777210_f86ec8143aa20281536c1411adf6d0f3.webp 400w,
               /media/posts/ddpm/fig9_hu77b70742483f8616b3f219bbbb608587_777210_c7b154285229bab0b4b15a1845b4a4a2.webp 760w,
               /media/posts/ddpm/fig9_hu77b70742483f8616b3f219bbbb608587_777210_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig9_hu77b70742483f8616b3f219bbbb608587_777210_f86ec8143aa20281536c1411adf6d0f3.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;22-ddpm-数学推导&#34;&gt;2.2 DDPM 数学推导&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig11_huf2fa0849d9c18b9d3f38f919f7dbbefd_89863_0647271d198440857855456cd0887d31.webp 400w,
               /media/posts/ddpm/fig11_huf2fa0849d9c18b9d3f38f919f7dbbefd_89863_4f758a954266206f53c913a1a258e674.webp 760w,
               /media/posts/ddpm/fig11_huf2fa0849d9c18b9d3f38f919f7dbbefd_89863_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig11_huf2fa0849d9c18b9d3f38f919f7dbbefd_89863_0647271d198440857855456cd0887d31.webp&#34;
               width=&#34;760&#34;
               height=&#34;355&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

我们可以从MLE的角度来理解图片生成模型，即我们希望随机抽样一个$z$后，经过一个“参数模型”（神经网络）输出的$P_\theta(x)$这个分布与真实的数据分布$P_{data}(x)$越近越好，因此我们希望找到一个$\theta$，使其对于从$P_{data}(x)$中抽样出来的$m$个样本$\{x^1,x^2,\ldots,x^m\}$满足：
$$\theta^\star=\arg \max _\theta \prod_{i=1}^m P_\theta\left(x^i\right)$$
因此，本质上就是最大化似然。进一步，我们可以得到：
$$
\begin{aligned}
\theta^\star= &amp;amp; \arg \max _\theta \prod_{i=1}^m P_\theta\left(x^i\right)=\arg \max _\theta \sum_{i=1}^m \log P_\theta\left(x^i\right)\\
&amp;amp; \approx \arg \max _\theta E_{x \sim P_{\text {data }}}\left[\log P_\theta(x)\right] \\
&amp;amp;=\arg \max _\theta \int_x P_{\text {data }}(x) \log P_\theta(x) d x\\
&amp;amp; ={\small \arg \max _\theta \left\{\int_x P_{\text {data }}(x) \log P_\theta(x) d x-{\color{blue}\int_x P_{\text {data }}(x) \log P_{\text {data }}(x) d x}\right\}} \\
&amp;amp; =\arg \max _\theta \int_x P_{\text {data }}(x) \log \frac{P_\theta(x)}{P_{\text {data }}(x)} d x\\
&amp;amp; = \arg\min_{\theta} {\color{blue}KL (P_{\text {data }}\| P_\theta)}
\end{aligned}
$$
所以最大化似然等价于最小化真实数据分布与生成数据分布的KL散度。但是一般说来，直接计算$P_\theta(x)$是非常困难的，在VAE我们是通过计算$\log\{P(x)\}$的下界来优化网络的，具体而言，给定任意一个分布$q(z|x)$，我们有
$$
\begin{aligned}
&amp;amp; \log P(x)\\
=&amp;amp;\int_z q(z \mid x) \log P(x) d z \\
=&amp;amp;\int_z q(z \mid x) \log \left(\frac{P(z, x)}{P(z \mid x)}\right) d z\\
=&amp;amp;\int_z q(z \mid x) \log \left(\frac{P(z, x)}{{\color{blue}q(z \mid x)}} \frac{{\color{blue}q(z \mid x)}}{P(z \mid x)}\right) d z \\
=&amp;amp; \int_z q(z \mid x) \log \left(\frac{P(z, x)}{q(z \mid x)}\right) d z+\underbrace{\int_z q(z \mid x) \log \left(\frac{q(z \mid x)}{P(z \mid x)}\right) d z}_{{\color{red}K L(q(z \mid x) | P(z \mid x))\ge 0}}  \\
\geq &amp;amp; \int_z q(z \mid x) \log \left(\frac{P(z, x)}{q(z \mid x)}\right) d z\\
=&amp;amp;\mathrm{E}_{q(z \mid x)}\left[\log \left(\frac{P(x, z)}{q(z \mid x)}\right)\right]
\end{aligned}
$$
这里的$\mathrm{E}_{q(z \mid x)}\left[\log \left(\frac{P(x, z)}{q(z \mid x)}\right)\right]$就是$\log P(x)$的下界（ELBO(evidence lower-bound)），在VAE中，我们本质上是在Maximize这个下界，这里$q(z\mid x)$实际上就是VAE中的Encoder。至于$P(x,z)$这个联合分布，我们可以借助公式$P(x,z)=P(x \mid z)P(z)$来计算，$P(z)$通常假设为高斯分布。至于$P(x \mid z)$的计算，如下图所示，随机生成一个$z$后，输入网络中得到$G(z)$，我们希望$G(z)$与$x$越近越好，因此，最直接的想法是：
$$
P_\theta(x \mid z)= \begin{cases}1, &amp;amp; G(z)=x \\ 0, &amp;amp; G(z) \neq x\end{cases}
$$
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig12_hua02dbfdc2ef7699e593e9be777735686_69741_1be4a558ca0f81c17aaff41583c2bc4a.webp 400w,
               /media/posts/ddpm/fig12_hua02dbfdc2ef7699e593e9be777735686_69741_0e109b27b730859f5f94e2ef0edd4901.webp 760w,
               /media/posts/ddpm/fig12_hua02dbfdc2ef7699e593e9be777735686_69741_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig12_hua02dbfdc2ef7699e593e9be777735686_69741_1be4a558ca0f81c17aaff41583c2bc4a.webp&#34;
               width=&#34;632&#34;
               height=&#34;444&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

但是这样可能会导致$P_\theta(x \mid z)$几乎都是0。因此我们可以把假设$P_\theta(x\mid z)$服从一个均值为$G(z)$的高斯分布，这样$P_\theta(x\mid z)\propto \exp \left(-\|G(z)-x\|_2^2\right)$，就进行训练（当然，这里只是为了解释$P_\theta(x\mid z)$的计算，VAE实际操作中还涉及到重参数化的技巧等等，这里就不再赘述）
根据相同的思路，我们就可以计算这个DDPM中的$P_\theta(x_0)$：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig13_hu8be117594e72e0473dfe5bb163641514_199145_f52a73409331a6bd504c815b1fe80a73.webp 400w,
               /media/posts/ddpm/fig13_hu8be117594e72e0473dfe5bb163641514_199145_de895675f0048aed8f97d226b65fd7b5.webp 760w,
               /media/posts/ddpm/fig13_hu8be117594e72e0473dfe5bb163641514_199145_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig13_hu8be117594e72e0473dfe5bb163641514_199145_f52a73409331a6bd504c815b1fe80a73.webp&#34;
               width=&#34;760&#34;
               height=&#34;222&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

如上图所示，我们可以把$P_\theta(x_{t-1}\mid x_t)$看作是服从均值为$G(x_t)$的高斯分布，因此$P_\theta(x_{t-1}\mid x_t)\propto \exp \left(-\|G(x_t)-x_{t-1}\|_2^2\right)$，同时我们假设一阶马尔可夫条件，最终得到：
$$
{\small P_\theta\left(x_0\right)=\int_{x_1: x_T} P\left(x_T\right) P_\theta\left(x_{T-1} \mid x_T\right) \ldots P_\theta\left(x_{t-1} \mid x_t\right) \ldots P_\theta\left(x_0 \mid x_1\right) d x_1: x_T}
$$
如下图所示，如果我们对比VAE和DDPM，两者在形式上是非常相似的：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig14_hu0b21a8c49959ecddac23906f97c2b114_53574_705b8e0e697b6a81819b739f666dffb3.webp 400w,
               /media/posts/ddpm/fig14_hu0b21a8c49959ecddac23906f97c2b114_53574_ddf41f58fe893b5fe8b75a7ca880b23e.webp 760w,
               /media/posts/ddpm/fig14_hu0b21a8c49959ecddac23906f97c2b114_53574_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig14_hu0b21a8c49959ecddac23906f97c2b114_53574_705b8e0e697b6a81819b739f666dffb3.webp&#34;
               width=&#34;760&#34;
               height=&#34;217&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

这里的$q\left(x_1: x_T \mid x_0\right)$我们仍然可以在假设一阶马尔可夫的条件下拆分为：
$$q\left(x_1: x_T \mid x_0\right)=q\left(x_1 \mid x_0\right) q\left(x_2 \mid x_1\right) \ldots q\left(x_T \mid x_{T-1}\right)$$
那么，这里的$q(x_t\mid x_{t-1})$应该怎么计算呢？
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig15_hu876d3a5c98e047e2dadaa9a9ea215024_1046707_11ecf189d0db3c5cffc93f39ae75201a.webp 400w,
               /media/posts/ddpm/fig15_hu876d3a5c98e047e2dadaa9a9ea215024_1046707_325b391a868fff9e997eb549a47c6ebd.webp 760w,
               /media/posts/ddpm/fig15_hu876d3a5c98e047e2dadaa9a9ea215024_1046707_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig15_hu876d3a5c98e047e2dadaa9a9ea215024_1046707_11ecf189d0db3c5cffc93f39ae75201a.webp&#34;
               width=&#34;760&#34;
               height=&#34;168&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

由于在扩散过程（Diffusion Process）中，$x_t$满足$x_t=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon_{t-1}$，其中$\epsilon_{t-1}\sim N(0,I)$，因此，我们$x_t$服从均值为$\sqrt{1-\beta_t}x_{t-1}$，方差为$\beta_t$的高斯分布，从而可以很好地进行计算。另外，对于$q(x_t\mid x_0)$，我们并不需要通过$q(x_t\mid x_0)=q(x_t\mid x_{t-1})q(x_{t-1}\mid x_{t-2})\cdots q(x_1\mid x_0)$这个序列每次抽样一个高斯分布，逐一计算，然后乘起来，我们可以直接计算$q(x_t\mid x_0)$，原因如下图所示：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig16_hub9a88944aad38652fab818f85290f39d_802773_596576daadc60690d83215eac446c5b7.webp 400w,
               /media/posts/ddpm/fig16_hub9a88944aad38652fab818f85290f39d_802773_550d50cc68b71be43be51893914faaf5.webp 760w,
               /media/posts/ddpm/fig16_hub9a88944aad38652fab818f85290f39d_802773_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig16_hub9a88944aad38652fab818f85290f39d_802773_596576daadc60690d83215eac446c5b7.webp&#34;
               width=&#34;760&#34;
               height=&#34;466&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

由于两次抽样是相互独立的，因此我可以只抽样一次，然后修改系数即可：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig17_hucd2b878f290493e25e2d963d4cbcc8c6_960995_38f59c4aab34d15e1ff9397930fce3b7.webp 400w,
               /media/posts/ddpm/fig17_hucd2b878f290493e25e2d963d4cbcc8c6_960995_9664255e64588c51cd8a7553dd2a9e34.webp 760w,
               /media/posts/ddpm/fig17_hucd2b878f290493e25e2d963d4cbcc8c6_960995_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig17_hucd2b878f290493e25e2d963d4cbcc8c6_960995_38f59c4aab34d15e1ff9397930fce3b7.webp&#34;
               width=&#34;760&#34;
               height=&#34;126&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

以此类推，我们有
$$
\begin{aligned}
x_t &amp;amp;= \sqrt{(1-\beta_t)(1-\beta_{t-1})\ldots(1-\beta_1)}x_0\\
&amp;amp;~~~+ \sqrt{1-(1-\beta_t)(1-\beta_{t-1})\ldots(1-\beta_1)}\ \epsilon
\end{aligned}
$$
这里的$\beta_1,\beta_2,\ldots,\beta_T$都是在训练之前人为设定的参数，如果我们定义$\alpha_t=1-\beta$，$\bar\alpha_t=\alpha_1\alpha_2\ldots \alpha_t$，我们就得到了论文中的表达式：
$$
\begin{aligned}
x_t &amp;amp;= \sqrt{\bar\alpha_t}x_0+ \sqrt{1-\bar\alpha_t}\ \epsilon
\end{aligned}
$$
我们回到优化目标：
$$
\operatorname{Maximize}\quad \mathrm{E}_{q\left(x_1: x_T \mid x_0\right)}\left[\log \left(\frac{P\left(x_0: x_T\right)}{q\left(x_1: x_T \mid x_0\right)}\right)\right]
$$
在“&lt;a href=&#34;https://arxiv.org/pdf/2208.11970.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Diffusion Models: A Unified Perspective&lt;/a&gt;”这篇文章中，对上述目标函数进行了推导：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; \log p(\boldsymbol{x}) \geq \mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p\left(\boldsymbol{x}_{0: T}\right)}{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\right] \\
&amp;amp; =\mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p\left(\boldsymbol{x}_T\right) \prod_{t=1}^T p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)}{\prod_{t=1}^T q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}\right)}\right] \\
&amp;amp; =\mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p\left(\boldsymbol{x}_T\right) p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_1\right) \prod_{t=2}^T p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)}{q\left(\boldsymbol{x}_1 \mid \boldsymbol{x}_0\right) \prod_{t=2}^T q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}\right)}\right] \\
&amp;amp; =\mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p\left(\boldsymbol{x}_T\right) p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_1\right) \prod_{t=2}^T p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)}{q\left(\boldsymbol{x}_1 \mid \boldsymbol{x}_0\right) \prod_{t=2}^T q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_0\right)}\right] \\
&amp;amp; =\mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_T\right) p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_1\right)}{q\left(\boldsymbol{x}_1 \mid \boldsymbol{x}_0\right)}+\log \prod_{t=2}^T \frac{p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)}{q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_0\right)}\right] \\
&amp;amp; =\mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p\left(\boldsymbol{x}_T\right) p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_1\right)}{q\left(\boldsymbol{x}_1 \mid \boldsymbol{x}_0\right)}+\log \prod_{t=2}^T \frac{p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)}{\frac{q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right) q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right)}{q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_0\right)}}\right] \\
&amp;amp; =\mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p\left(\boldsymbol{x}_T\right) p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_1\right)}{q\left(\boldsymbol{x}_1 \mid \boldsymbol{x}_0\right)}+\log \prod_{t=2}^T \frac{p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)}{\frac{q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right) q\left(\boldsymbol{x}_t+\boldsymbol{x}_0\right)}{q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_0\right)}}\right] \\
&amp;amp; =\mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p\left(\boldsymbol{x}_T\right) p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_1\right)}{q\left(\boldsymbol{x}_1 \mid \boldsymbol{x}_0\right)}+\log \frac{q\left(\boldsymbol{x}_1 \mid \boldsymbol{x}_0\right)}{q\left(\boldsymbol{x}_T \mid \boldsymbol{x}_0\right)}\right.\\
&amp;amp;~~~~~~~~~~~~~~~~~~~~~~~~+\left.\log \prod_{t=2}^T \frac{p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)}{q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)}\right] \\
&amp;amp; =\mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p\left(\boldsymbol{x}_T\right) p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_1\right)}{q\left(\boldsymbol{x}_T \mid \boldsymbol{x}_0\right)}+\sum_{t=2}^T \log \frac{p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)}{q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)}\right] \\
&amp;amp; =\mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_1\right)\right]+\mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p\left(\boldsymbol{x}_T\right)}{q\left(\boldsymbol{x}_T \mid \boldsymbol{x}_0\right)}\right]\\
&amp;amp; ~~~~~~~~~ \qquad \qquad  +\sum_{t=2}^T \mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)}{q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)}\right] \\
&amp;amp; =\mathbb{E}_{q\left(\boldsymbol{x}_1 \mid \boldsymbol{x}_0\right)}\left[\log p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_1\right)\right]+\mathbb{E}_{q\left(\boldsymbol{x}_T \mid \boldsymbol{x}_0\right)}\left[\log \frac{p\left(\boldsymbol{x}_T\right)}{q\left(\boldsymbol{x}_T \mid \boldsymbol{x}_0\right)}\right]\\
&amp;amp; ~~~~~~~~~ \qquad \qquad  +\sum_{t=2}^T \mathbb{E}_{q\left(\boldsymbol{x}_t, \boldsymbol{x}_{t-1} \mid \boldsymbol{x}_0\right)}\left[\log \frac{p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)}{q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)}\right] \\
&amp;amp; =\underbrace{\mathbb{E}_{q\left(\boldsymbol{x}_1 \mid \boldsymbol{x}_0\right)}\left[\log p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_1\right)\right]}_{\text {reconstruction term }}-\underbrace{D_{\mathrm{KL}}\left(q\left(\boldsymbol{x}_T \mid \boldsymbol{x}_0\right) \| p\left(\boldsymbol{x}_T\right)\right)}_{\text {prior matching term }}\\
&amp;amp; ~~~~~~~~~ \qquad \qquad  -\sum_{t=2}^T \underbrace{\mathbb{E}_{q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right)}\left[D_{\mathrm{KL}}\left(q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right) \| p_{\boldsymbol{\theta}}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)\right)\right]}_{\text {denoising matching term }}
\end{aligned}
$$
因此，我们得到了DDPM中$\log P(x)$的lower bound：
$$
\begin{aligned}
&amp;amp;\mathrm{E}_{q\left(x_1 \mid x_0\right)}\left[\log P\left(x_0 \mid x_1\right)\right]  -K L\left(q\left(x_T \mid x_0\right)|| P\left(x_T\right)\right) \\
&amp;amp; ~~~ \qquad -\sum_{t=2}^T \mathrm{E}_{q\left(x_t \mid x_0\right)}\left[K L\left(q\left(x_{t-1} \mid x_t, x_0\right)\| P\left(x_{t-1} \mid x_t\right)\right)\right]
\end{aligned}
$$
我们并不需要计算$K L\left(q\left(x_T \mid x_0\right)|| P\left(x_T\right)\right)$，因为这里的$P\left(x_T\right)$是从高斯分布中采样，而$q\left(x_T \mid x_0\right)$是我们人为设定的 diffusion process，这两个分布均于神经网络参数$\theta$无关，可以视为一个常数。我们重点关注的应该是最后一项$\sum_{t=2}^T \mathrm{E}_{q\left(x_t \mid x_0\right)}\left[K L\left(q\left(x_{t-1} \mid x_t, x_0\right)\| P\left(x_{t-1} \mid x_t\right)\right)\right]$，至于第一项，优化的思路类似于最后一项，这里不再展开，详细的情况可以参看原论文或&lt;a href=&#34;https://zhuanlan.zhihu.com/p/549623622&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;（Diffusion Models：生成扩散模型-知乎）&lt;/a&gt;。对于$\sum_{t=2}^T \mathrm{E}_{q\left(x_t \mid x_0\right)}\left[K L\left(q\left(x_{t-1} \mid x_t, x_0\right)\| P\left(x_{t-1} \mid x_t\right)\right)\right]$这一项，难点在于计算$q\left(x_{t-1} \mid x_t, x_0\right)$，由于我们可以计算$q\left(x_{t} \mid x_0\right)$, $q\left(x_{t-1} \mid x_0\right)$以及$q\left(x_{t} \mid x_{t-1}\right)$：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig18_hu2ad81eb2a30207fc03d0722a4d476e66_693698_a1a2f9b2302e5bab199d3c17301c9792.webp 400w,
               /media/posts/ddpm/fig18_hu2ad81eb2a30207fc03d0722a4d476e66_693698_439fd90de5246b7a6ce5b8bc313e2349.webp 760w,
               /media/posts/ddpm/fig18_hu2ad81eb2a30207fc03d0722a4d476e66_693698_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig18_hu2ad81eb2a30207fc03d0722a4d476e66_693698_a1a2f9b2302e5bab199d3c17301c9792.webp&#34;
               width=&#34;760&#34;
               height=&#34;265&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

因此，利用条件概率公式得：
$$
\begin{aligned}
q\left(x_{t-1} \mid x_t, x_0\right)&amp;amp;=\frac{q\left(x_{t-1}, x_t, x_0\right)}{q\left(x_t, x_0\right)}=\frac{q\left(x_t \mid x_{t-1}\right) q\left(x_{t-1} \mid x_0\right) q\left(x_0\right)}{q\left(x_t \mid x_0\right) q\left(x_0\right)}\\
&amp;amp;=\frac{q\left(x_t \mid x_{t-1}\right) q\left(x_{t-1} \mid x_0\right)}{q\left(x_t \mid x_0\right)}
\end{aligned}
$$
将$q\left(x_{t} \mid x_0\right)$, $q\left(x_{t-1} \mid x_0\right)$以及$q\left(x_{t} \mid x_{t-1}\right)$这三个的具体表达式代入后，我们可以得到：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig19_hu4002690b8a073c8e34ccfaa35295ed29_158246_50c285e334fca80464909573ea6dc4cb.webp 400w,
               /media/posts/ddpm/fig19_hu4002690b8a073c8e34ccfaa35295ed29_158246_743269468c38b38d2fa64dab9f3d187c.webp 760w,
               /media/posts/ddpm/fig19_hu4002690b8a073c8e34ccfaa35295ed29_158246_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig19_hu4002690b8a073c8e34ccfaa35295ed29_158246_50c285e334fca80464909573ea6dc4cb.webp&#34;
               width=&#34;760&#34;
               height=&#34;721&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

我们发现$q\left(x_{t-1} \mid x_t, x_0\right)$仍然是一个高斯分布，均值和方差分别为：
$$
\begin{aligned}
{\mu_q\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)} &amp;amp;= {\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) \boldsymbol{x}_t+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_t\right) \boldsymbol{x}_0}{1-\bar{\alpha}_t}}\\
\boldsymbol{\Sigma}_q(t) &amp;amp;= {\frac{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t}\mathbf{I}}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;既然$q\left(x_{t-1} \mid x_t, x_0\right)$与$P\left(x_{t-1} \mid x_t\right)$均为高斯分布，那么KL散度是有显示解的，即：
$$
\begin{aligned}
&amp;amp; D_{\mathrm{KL}}\left(\mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{\mu}_x, \boldsymbol{\Sigma}_x\right) | \mathcal{N}\left(\boldsymbol{y} ; \boldsymbol{\mu}_y, \boldsymbol{\Sigma}_y\right)\right)\\
=&amp;amp;\frac{1}{2}\left[\log \frac{\left|\boldsymbol{\Sigma}_y\right|}{\left|\boldsymbol{\Sigma}_x\right|}-d+\operatorname{tr}\left(\boldsymbol{\Sigma}_y^{-1} \boldsymbol{\Sigma}_x\right)+\left(\boldsymbol{\mu}_y-\boldsymbol{\mu}_x\right)^T \boldsymbol{\Sigma}_y^{-1}\left(\boldsymbol{\mu}_y-\boldsymbol{\mu}_x\right)\right]
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;但是我们有更简单的方法来替代KL散度的计算，具体而言，我们注意到$q\left(x_{t-1} \mid x_t, x_0\right)$是一个跟网络参数无关的高斯分布，因此其均值和方差都是固定的，而$P\left(x_{t-1} \mid x_t\right)$中，我们会控制 Denoise 的方差，但是其均值是更网络参数有关的，因此我们只需要让这两个分布的均值接近就可以，这就等价于最小化KL散度
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig20_hu8ace829d78d9a17233afe6a958d29f16_189358_e0907363747884fea102a9ed0297476c.webp 400w,
               /media/posts/ddpm/fig20_hu8ace829d78d9a17233afe6a958d29f16_189358_3115d3e08111d8f466a68ac116a669a7.webp 760w,
               /media/posts/ddpm/fig20_hu8ace829d78d9a17233afe6a958d29f16_189358_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig20_hu8ace829d78d9a17233afe6a958d29f16_189358_e0907363747884fea102a9ed0297476c.webp&#34;
               width=&#34;760&#34;
               height=&#34;261&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

因此，实际训练时，如下图所示，对于样本$x_0$，我们首先构造$x_t=\sqrt{\bar{\alpha}_t} x_0+\sqrt{1-\bar{\alpha}_t} \varepsilon$，然后将$x_t$与$t$作为Denoise网络的输入，希望输出的结果与$q\left(x_{t-1} \mid x_t, x_0\right)$的均值
$$\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t x_0+\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) x_t}{1-\bar{\alpha}_t}\qquad(\star)$$
越接近越好。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig21_hud1a3113e8657a3e31ed4c25722cba978_414492_17dea1a90cd4a421d508b085361fb3be.webp 400w,
               /media/posts/ddpm/fig21_hud1a3113e8657a3e31ed4c25722cba978_414492_3dc3a380cc85c19e83d1abef24e000ba.webp 760w,
               /media/posts/ddpm/fig21_hud1a3113e8657a3e31ed4c25722cba978_414492_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig21_hud1a3113e8657a3e31ed4c25722cba978_414492_17dea1a90cd4a421d508b085361fb3be.webp&#34;
               width=&#34;760&#34;
               height=&#34;259&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

注意到$x_t=\sqrt{\bar{\alpha}_t} x_0+\sqrt{1-\bar{\alpha}_t} \varepsilon$，通过反解$x_0$，代入$(\star)$式中得到
$$
\begin{aligned}
&amp;amp;\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t x_0+\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) x_t}{1-\bar{\alpha}_t}\\
&amp;amp; =\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t\left(\frac{x_t-\sqrt{1-\bar{\alpha}_t} \varepsilon}{\sqrt{\bar{\alpha}_t}}\right)+\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) x_t}{1-\bar{\alpha}_t} \\
&amp;amp; ={\color{red}\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \varepsilon\right)}
\end{aligned}
$$
因此，如下图所示，Denoise模式要做的就是预测出$\varepsilon$，因为$\alpha_t$这些参数都是预先设定好的，跟网络无关。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig22_hua43fd0105d76fafc2612dc6af5cc8746_1716234_b16a240fe91b898049fc1219005795bf.webp 400w,
               /media/posts/ddpm/fig22_hua43fd0105d76fafc2612dc6af5cc8746_1716234_958e01c67abcdea11a34be303a87a9bb.webp 760w,
               /media/posts/ddpm/fig22_hua43fd0105d76fafc2612dc6af5cc8746_1716234_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig22_hua43fd0105d76fafc2612dc6af5cc8746_1716234_b16a240fe91b898049fc1219005795bf.webp&#34;
               width=&#34;760&#34;
               height=&#34;414&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

同时，我们也可以发现这个式子与DDPM论文中Sampling那个式子是一样的。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;最后，我们注意到算法二中Sampling的公式
$$
\mathbf{x}_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta\left(\mathbf{x}_t, t\right)\right)+{\color{red}{\sigma_t \mathbf{z}}}
$$
仍然在每一步引入了随机项，为什么不直接取均值呢？这个问题其实跟文本生成模型中引入随机性是一样的道理，如下图所示，在文本生成模型中，如果每次都取概率最大的，那么生成的文字很有可能出现重复进而导致较差的效果：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig23_hua8c1ba845f0f004c323ccee4fe3e673a_495081_df5d176c32e63415c42782c9851a4200.webp 400w,
               /media/posts/ddpm/fig23_hua8c1ba845f0f004c323ccee4fe3e673a_495081_c581bf530ab1e3d52d3f8685e06e2a30.webp 760w,
               /media/posts/ddpm/fig23_hua8c1ba845f0f004c323ccee4fe3e673a_495081_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig23_hua8c1ba845f0f004c323ccee4fe3e673a_495081_df5d176c32e63415c42782c9851a4200.webp&#34;
               width=&#34;760&#34;
               height=&#34;274&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

下图的结果也确实验证了这个情况
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig24_hue9f0741a07e9627ab62f60e2b9ad96fd_248159_1768d4d92dad2e7cfa9dab2e204dd1c9.webp 400w,
               /media/posts/ddpm/fig24_hue9f0741a07e9627ab62f60e2b9ad96fd_248159_a5e693a5e8d6613bb62a6291e24018bc.webp 760w,
               /media/posts/ddpm/fig24_hue9f0741a07e9627ab62f60e2b9ad96fd_248159_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig24_hue9f0741a07e9627ab62f60e2b9ad96fd_248159_1768d4d92dad2e7cfa9dab2e204dd1c9.webp&#34;
               width=&#34;760&#34;
               height=&#34;232&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-基于分数的生成模型score-based-generative-models&#34;&gt;3 、基于分数的生成模型（Score-based generative models）&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;主要参考：&lt;a href=&#34;https://zhuanlan.zhihu.com/p/597490389&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;知乎：@CW不要無聊的風格&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig26_hu501c0461d78f3f7a70fc142ea2efff13_234389_d71db7717eee8131dab0145c5697c46c.webp 400w,
               /media/posts/ddpm/fig26_hu501c0461d78f3f7a70fc142ea2efff13_234389_a8edb9d058d3254a6f6efa060a2eaadc.webp 760w,
               /media/posts/ddpm/fig26_hu501c0461d78f3f7a70fc142ea2efff13_234389_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig26_hu501c0461d78f3f7a70fc142ea2efff13_234389_d71db7717eee8131dab0145c5697c46c.webp&#34;
               width=&#34;760&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

我们希望生成模型能表示概率分布，从而能够通过其实现采样生成。至于如何表示，不同的模型有不同的方法，但它们大致都可被归纳为两种范式，分别是：对数据的&lt;strong&gt;采样过程&lt;/strong&gt;建模和对&lt;strong&gt;概率密度&lt;/strong&gt;进行建模。前者不纠结于数据分布的概率密度，而是通过其它方法达到表示概率分布的目的，因此也被称为隐式(implicit)生成模型，例如GAN；而后者则直接让模型去估计概率密度，不绕圈子，于是被称为显式(explicit)生成模型，例如VAE 、DDPM。而基于分数的生成模型和这两类都不太一样，该领域的代表作为&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generative Modeling by Estimating Gradients of the Data Distribution&lt;/a&gt;这篇文章。&lt;/p&gt;
&lt;h4 id=&#34;31-朗之万动力学采样&#34;&gt;3.1 朗之万动力学采样&lt;/h4&gt;
&lt;p&gt;朗之万动力学(Langevin dynamics)是物理学中对分子系统进行统计建模(布朗运动)的工具&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34;
           src=&#34;https://ikerlz.github.io/media/posts/ddpm/video1.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;上图模拟了花粉(黄色)收到大量水分子的撞击进行随机的布朗运动的情形。朗之万方程可化简为&lt;/p&gt;
&lt;p&gt;$${X_{t+dt}} = X_t-\frac{dt}{\gamma} \frac{\partial E}{\partial X}+\frac{dt}{\gamma} \epsilon,$$
这里的$-\frac{dt}{\gamma} \frac{\partial E}{\partial X}$表示能量减小得最快的方向，因此该方程刻画了粒子会往能量减小的最快的方向移动。而在2011年的文章&lt;a href=&#34;https://citeseerx.ist.psu.edu/document?repid=rep1&amp;amp;type=pdf&amp;amp;doi=56f89ce43d7e386bface3cba63e674fe748703fc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian learning via stochastic gradient Langevin dynamics&lt;/a&gt;中，提出了随机梯度朗之万动力学，而基于分数的生成模型的一些理论保证就来源于该文章。那我们应该怎么使用这个动力学方程从一个分布$p(x)$进行采样呢？答案是下面这个公式：&lt;/p&gt;
&lt;p&gt;$$
\tilde{\mathbf{x}}_t=\tilde{\mathbf{x}}_{t-1}+\frac{\epsilon}{2} \nabla_{\mathbf{x}} \log p\left(\tilde{\mathbf{x}}_{t-1}\right)+\sqrt{\epsilon} \mathbf{z}_t
$$
具体而言，我们可以从任意的先验分布$\pi(\mathbf{x})$中抽样一个初始样本$\tilde{\mathbf{x}}_0$，然后按照上式更新即可。这里的$\mathbf{z}_t\sim \mathcal{N}(0,I)$。因此我们可以发现这样的采样过程只会涉及到对数似然的梯度$\nabla_{\mathbf{x}} \log p\left(\tilde{\mathbf{x}}_{t-1}\right)$，只要我们能准确地近似这个梯度，那么我们就可以随意采样，随意生成高质量的图片。这个梯度被称之为“分数”（score），这也是该类方法的名字由来。理论上, 在一定约束条件下, 并且当 $\epsilon \rightarrow 0, T \rightarrow \infty$ 时, 最终生成的样本 $x_T$ 将会服从原数据分布 $p_{\text {data }}(x)$ 。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig27_hufc1161ab624693f27e78c03c5b6f41a0_80090_a9967b1b253f9642e60c6c81fbcc92e1.webp 400w,
               /media/posts/ddpm/fig27_hufc1161ab624693f27e78c03c5b6f41a0_80090_e8079b7e03987991a7faf5e1ba12dade.webp 760w,
               /media/posts/ddpm/fig27_hufc1161ab624693f27e78c03c5b6f41a0_80090_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig27_hufc1161ab624693f27e78c03c5b6f41a0_80090_a9967b1b253f9642e60c6c81fbcc92e1.webp&#34;
               width=&#34;609&#34;
               height=&#34;194&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;因此，现在的问题转化为如何利用神经网络得到$\mathbf{s}_\theta(\mathbf{x})$，使其约接近$\nabla_{\mathbf{x}} \log p\left(\tilde{\mathbf{x}}_{t-1}\right)$越好&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34;
           src=&#34;https://ikerlz.github.io/media/posts/ddpm/video2.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;32-分数模型理论&#34;&gt;3.2 分数模型理论&lt;/h4&gt;
&lt;p&gt;根据前面的介绍，我们可以得到分数模型的训练目标
$$
\frac{1}{2} E_{p_{\text {data }}}\left[\left\|s_\theta(x)-\frac{\partial \log \left(p_{\text {data }}(x)\right)}{\partial x}\right\|_2^2\right]\qquad (1)
$$
其中, $\theta$代表网络参数，$s_\theta(x)$是网络估计的分数，其减去的右边那项代表真实的分数，$E_{P_{d a t a}}$表示在整体数据中求期望。但是这显示是无法计算的，因为我们并不知道分布$p_{\text {data }}$，但是我们可以通过&lt;strong&gt;score matching(分数匹配)&lt;/strong&gt; 的方法，可以让我们在不了解概率密度的情况下也可以成功训练出模型来估计分数，即上述目标函数等价于&lt;/p&gt;
&lt;p&gt;$$
\mathbb{E}_{p_{\text {data }}(\mathbf{x})}\left[\operatorname{tr}\left(\nabla_{\mathbf{x}} \mathbf{s}_{\boldsymbol{\theta}}(\mathbf{x})\right)+\frac{1}{2}\left\|\mathbf{s}_{\boldsymbol{\theta}}(\mathbf{x})\right\|_2^2\right]
$$
这里的$\nabla_{\mathbf{x}} \mathbf{s}_{\boldsymbol{\theta}}(\mathbf{x})$其实就是 Jacobian矩阵，但是注意到当$\mathbf{x}$维度很高的时候，$\nabla_{\mathbf{x}} \mathbf{s}_{\boldsymbol{\theta}}(\mathbf{x})$的反向传播是非常麻烦的，因此论文中提到了两种方法来解决这个问题，分别是：sliced score matching和denoising score matching&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sliced Score Matching(切片分数匹配)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;既然这个矩阵的trace很难计算，那我们可以尝试去估计它，利用&lt;a href=&#34;https://zhuanlan.zhihu.com/p/597490389&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hutchinson trace estimator&lt;/a&gt;，对于任意一个矩阵$A$，我们有
$$
\begin{aligned}
\operatorname{tr}(A)&amp;amp;=\operatorname{tr}\left(A \mathbb{E}\left[v v^T\right]\right)=\operatorname{tr}\left(\mathbb{E}\left[A v v^T\right]\right)=\mathbb{E}\left[\operatorname{tr}\left(A v v^T\right)\right]\\
&amp;amp;=\mathbb{E}\left[\operatorname{tr}\left(v^T A v\right)\right]=\mathbb{E}\left[v^T A v\right]
\end{aligned}
$$
其中$v\sim (0,I)$，因此我们可以得到&lt;/p&gt;
&lt;p&gt;$$\operatorname{tr}\left(\frac{\partial s_\theta(x)}{\partial x}\right)=\mathbb{E}_{p_v}\left[v^T \frac{\partial s_\theta(x)}{\partial x} v\right]=\mathbb{E}_{p_v}\left[\frac{\partial\left(v^T s_{\theta(x)}\right)}{\partial x} v\right]$$
这样就很大程度上简化了梯度的计算，训练的目标函数也变为：
$$
E_{p_v} E_{p_{\text {data }}}\left[\frac{\partial\left(v^T s_\theta(x)\right)}{\partial x} v+\frac{1}{2}\left\|s_\theta(x)\right\|_2^2\right]
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Denoising Score Matching(去噪分数匹配)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个方法是原文默认的方法，具体而言，就是在原分布上加噪声，然后利用$(1)$式来训练。记训练样本为 $x$, 加噪后的数据为 $\tilde{x}$, 预定义的分布为 $q_\sigma(\tilde{x} \mid x) \sim N\left(\tilde{x} ; x, \sigma^2 I\right)$, 方差 $\sigma^2$ 预先定义好。加噪后的数据分布表示为: $q_\sigma(\tilde{x})=\int q_\sigma(\tilde{x} \mid x) p_{\text {data }}(x) d x$。&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;\mathbb{E}_{q_\sigma(\tilde{x})}\left[\left\|s_\theta(\tilde{x})-\frac{\partial \log \left(q_\sigma(\bar{x})\right)}{\partial \tilde{x}}\right\|^2\right]\\
=&amp;amp;\mathbb{E}_{q_\sigma(\tilde{x} \mid x) p_{\text {data }}(x)}\left[\left\|s_\theta(\tilde{x})-\frac{\partial \log \left(q_\sigma(\tilde{x} \mid x)\right)}{\partial \tilde{x}}\right\|^2\right]+C
\end{aligned}
$$
其中$C$为一个与模型参数$\theta$不相关的常数项。&lt;/p&gt;
&lt;p&gt;等号左边是加噪后最直接的分数匹配形式（&lt;strong&gt;显式的分数匹配&lt;/strong&gt;(Explicit Score Matching)），而等号右边则是实际使用的&lt;strong&gt;去噪分数匹配形式&lt;/strong&gt;(DSM)，ESM与DSM的等价性证明见（&lt;a href=&#34;https://zhuanlan.zhihu.com/p/597490389&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;知乎&lt;/a&gt;）。于是, 训练目标就转化为:
$$
\frac{1}{2} E_{q_\sigma(\tilde{x} \mid x) p_{\text {data }}(x)}\left[\left\|s_\theta(\tilde{x})-\frac{\partial \log \left(q_\sigma(\tilde{x} \mid x)\right)}{\partial \tilde{x}}\right\|_2^2\right]
$$
在训练模型时，我们只需要将加噪后的样本$\tilde{x}$喂给模型即可计算估计分数，但是另外, 这个分数是对应噪声数据分布$q_\sigma(\tilde{x})$的, 而非原始数据分布$p_{\text {data }}(x)$，因此我们需要让$q_\sigma(\tilde{x})$尽可能与$p_{\text {data }}(x)$ 接近, 这就要求预定义的$\sigma$ 要足够小。否则, 如果对原始数据扰动过大，模型训练完后生成的样本就会严重偏离原数据分布。&lt;/p&gt;
&lt;h4 id=&#34;33-分数模型面临的问题&#34;&gt;3.3 分数模型面临的问题&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;训练时损失不收敛&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基于sliced score matching方式下训练的loss
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig28_hu7286c9ac5bf0aa19bace7cf1b394c7e5_144845_08b5c01d6da1558f6d4006087982f4f3.webp 400w,
               /media/posts/ddpm/fig28_hu7286c9ac5bf0aa19bace7cf1b394c7e5_144845_42fc343f9b0fa297dbac8b5cdd200420.webp 760w,
               /media/posts/ddpm/fig28_hu7286c9ac5bf0aa19bace7cf1b394c7e5_144845_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig28_hu7286c9ac5bf0aa19bace7cf1b394c7e5_144845_08b5c01d6da1558f6d4006087982f4f3.webp&#34;
               width=&#34;760&#34;
               height=&#34;519&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

不收敛的原因可以通过“流形假设”(mainfold hypotheis)来解释。流行假设认为，生活中的真实数据大部分都倾向于分布在低维空间中，而我们的编码空间(通过神经网络等一系列方法)的维度可能很广泛，但是数据经过编码后在许多维度上都存在冗余，实际上用一部分维度就足以表示它，说明实质上它仅分布在整个编码空间中的一部分(低维流形)，并没有“占满”整个编码空间。当我们的数据仅分布在编码空间的低维流形时，分数就会出现“没有定义” (undefined) 的情况。另外，score matching 的训练目标是基于数据“占满”了整个编码空间这个前提下而推出来的。当数据仅分布在低维流形中时，score matching 的方法就不适用了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;分数估计不准&lt;/strong&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig29_hu09258e34174ea97a35a1ef313b2075b4_387253_514056df9b00b083a110f795542957f5.webp 400w,
               /media/posts/ddpm/fig29_hu09258e34174ea97a35a1ef313b2075b4_387253_fe06a8c5e7df64051cc290ac4a700077.webp 760w,
               /media/posts/ddpm/fig29_hu09258e34174ea97a35a1ef313b2075b4_387253_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig29_hu09258e34174ea97a35a1ef313b2075b4_387253_514056df9b00b083a110f795542957f5.webp&#34;
               width=&#34;760&#34;
               height=&#34;384&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

如上图所示，颜色深浅代表数据概率密度的大小(越深越大，对应的样本越可能出现)，箭头代表分数。可以看到，右图的箭头(也就是模型估计的分数)在颜色深的地方基本和左图的箭头(也就是真实的分数)一致，如矩形框出来的部分所示。而在颜色比较浅的部分，特别是左图和右图对角线那部分，两者的箭头差异就比较大，代表模型估计的分数不准确。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;生成结果偏差大&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;原文中，作者发现训练后的模型生成效果不佳，采样生成出来的样本不太符合原数据分布。作者发现当数据分布是由多个分布按一定比例混合而成时，在朗之万动力学采样的玩法下，即使用真实的分数，采样生成出来的结果也可能&lt;strong&gt;不能反应出各个分布之间的比例关系&lt;/strong&gt;，如下图所示：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig30_hu8d7b30ba1bdb4710aadb838d3d066d89_104834_15c44ed3255a034d6f4d8a1792462bfd.webp 400w,
               /media/posts/ddpm/fig30_hu8d7b30ba1bdb4710aadb838d3d066d89_104834_f8685c455fbc8e513429cb053651d9fc.webp 760w,
               /media/posts/ddpm/fig30_hu8d7b30ba1bdb4710aadb838d3d066d89_104834_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig30_hu8d7b30ba1bdb4710aadb838d3d066d89_104834_15c44ed3255a034d6f4d8a1792462bfd.webp&#34;
               width=&#34;760&#34;
               height=&#34;396&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

之所以会出现这种情况，猜测是因为采样时会丢失比例关系，举个简单的粒子，考虑两种分布为$p_1(x)$和$p_2(x)$, 分别以比例$\pi$和$1-\pi$混合而成, 于是整体的数据分布表示为：
$$
p_{\text {data }}(x)=\pi p_1(x)+(1-\pi) p_2(x)
$$&lt;/p&gt;
&lt;p&gt;对于整体数据分布对应的分数, 如果我们分别来看其中两个分布对应的分数, 就会发现其实和它们的混合比例不相关:
$$
\begin{aligned}
\frac{\partial \log \left(p_{\text {data }}(x)\right)}{\partial x}=&amp;amp; \frac{\partial \log \left(\pi p_1(x)\right)}{\partial x}+\frac{\partial \log \left((1-\pi) p_2(x)\right)}{\partial x}\\
=&amp;amp;\frac{\partial \log \left(p_1(x)\right)}{\partial x} +\frac{\partial \log \left(p_2(x)\right)}{\partial x}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;这样就就没有关于$\pi$的信息了。原文说到面对这种混合分布的情况，从理论上来说，朗之万动力学采样是可以生成与原分布相似的结果的，但可能需要很小的步长(step size)以及非常大的步数(steps)才能做到。&lt;/p&gt;
&lt;h4 id=&#34;34-利用ncsn解决三个问题&#34;&gt;3.4 利用NCSN解决三个问题&lt;/h4&gt;
&lt;p&gt;NCSN是Noise Conditional Score Networks的缩写，作者用NCSN 解决了上一节中提到的三个问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;解决Loss不收敛&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NCSN利用了高斯噪声去扰动数据（仅仅需要一个非常小的扰动），而高斯噪声是分布在整个编码空间的，因此，扰动后的数据能够有机会出现在整个编码空间的任意部分，从而避免了分数(score)没有定义(undefined)的情况，同时使得分数匹配公式适用，这就破解了“loss 不收敛”的困局。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;解决分数估计不准&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig32_hu0fc04be8cd1fa824eba1dce915f5ad53_105719_5f588bbbf23c1e26dfe4f3e922cad89a.webp 400w,
               /media/posts/ddpm/fig32_hu0fc04be8cd1fa824eba1dce915f5ad53_105719_288206c3af785323e934def09898f52d.webp 760w,
               /media/posts/ddpm/fig32_hu0fc04be8cd1fa824eba1dce915f5ad53_105719_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig32_hu0fc04be8cd1fa824eba1dce915f5ad53_105719_5f588bbbf23c1e26dfe4f3e922cad89a.webp&#34;
               width=&#34;601&#34;
               height=&#34;198&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig31_hub1b5b87f3c8cf7064650129d5b3a546c_125078_18492912d35ee837883121582620b24c.webp 400w,
               /media/posts/ddpm/fig31_hub1b5b87f3c8cf7064650129d5b3a546c_125078_3bbbf4dc1e5300b8ed559646917705e5.webp 760w,
               /media/posts/ddpm/fig31_hub1b5b87f3c8cf7064650129d5b3a546c_125078_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig31_hub1b5b87f3c8cf7064650129d5b3a546c_125078_18492912d35ee837883121582620b24c.webp&#34;
               width=&#34;613&#34;
               height=&#34;198&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;通过扰动数据的时候，噪声会有更多机会将原来的数据扩散到低概率密度区域，于是原来的低概率密度区域就能获得更多的监督信号，从而提升了分数估计的准确性，这就能破解了低密度趋于“分数估计不准”的问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;解决生成结果偏差大&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig33_hu795e947a51f957dc3b0e2b90fa41ca50_240501_ffd25fc4f9371bb85bd49ee0f7fa9af9.webp 400w,
               /media/posts/ddpm/fig33_hu795e947a51f957dc3b0e2b90fa41ca50_240501_54d0f1d3b396c50702b1007fcc1724df.webp 760w,
               /media/posts/ddpm/fig33_hu795e947a51f957dc3b0e2b90fa41ca50_240501_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig33_hu795e947a51f957dc3b0e2b90fa41ca50_240501_ffd25fc4f9371bb85bd49ee0f7fa9af9.webp&#34;
               width=&#34;760&#34;
               height=&#34;256&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;加噪后，原本“稀疏”的低概率密度区域就能够被“填满”，并且，自然地，两个分布中混合比例高的那个分布所占这部分(“填满”后的区域)的比例会更高，因此扰动后的分数将更多地由混合比例高的那个分布的分数“演变”而来。这就是说，分数的方向将更多地指向混合比例高的那个分布。&lt;/p&gt;
&lt;h4 id=&#34;35-ncsn的技术细节&#34;&gt;3.5 NCSN的技术细节&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;噪声设计原则&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从前一节可以看出，加噪是该模型的精髓，但是具体应该加多大的噪声呢？直觉上为了让扰动后的分布能够更多地“填充”原来的低概率密度区域，我们应该加一个大的噪声，但是，我们需要用加噪后的分数 $\frac{\partial \log \left(q_\sigma(\tilde{x})\right)}{\partial \tilde{x}}$ 来近似原分布的分数 $\frac{\partial \log \left(p_{\text {data }}(x)\right)}{\partial x}$ 的 $(\tilde{x}, x$ 分别代表加噪后的数据与原数据)。若扰动过大, 抹去了原数据大部分信息, 就会造成近似效果不佳，因此噪声强度小能够获得与原数据分布较为近似的效果, 但是却不能够很好地“填充”低概率密度区域。于是，需要做一个 trade-off 。原文中，作者采用了一个噪声序列 $\left\{\sigma_i\right\}_{i=1}^L$, 并满足: $\frac{\sigma_1}{\sigma_2}=\ldots=\frac{\sigma_{L-1}}{\sigma_L}&amp;gt;1$, 同时 $\sigma_1$ 需要足够大, 以能够充分“填充”低概率密度区域; 而 $\sigma_L$ 得足够小, 以获得对原数据分布良好的近似, 避免过度扰动。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;训练方法：去噪分数匹配&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;切片分数匹配(sliced score matching)也可以训练NCSN，但计算更多，训练过程会更慢，因此，作者使用去噪分数匹配进行训练，注意到我们的训练目标位&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{2} E_{q_\sigma(\tilde{x} \mid x) p_{\text {data }}(x)}\left[\left\|s_\theta(\tilde{x})-\frac{\partial \log \left(q_\sigma(\tilde{x} \mid x)\right)}{\partial \tilde{x}}\right\|_2^2\right]
$$&lt;/p&gt;
&lt;p&gt;由于加噪后的样本$\tilde{x}\sim\mathcal{N}(x,\sigma^2 \mathbf{I})$，因此&lt;/p&gt;
&lt;p&gt;$$q_\sigma(\tilde{x} \mid x)=\frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{(\bar{x}-\mathrm{x})^2}{2 \sigma^2}}$$&lt;/p&gt;
&lt;p&gt;将其取对数井对 $\tilde{x}$求导, 就可获得其分数的表示形式:&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial \log \left(q_\sigma(\bar{x} \mid x)\right)}{\partial \tilde{x}}=-\left(\frac{\bar{x}-x}{\sigma^2}\right)
$$&lt;/p&gt;
&lt;p&gt;将以上代入目标函数, 就得到在某个噪声级别 $\sigma$ 对应的损失函数:&lt;/p&gt;
&lt;p&gt;$$
l(\theta ; \sigma) \equiv \frac{1}{2} E_{p_{\text {data }}(x)} E_{\tilde{x} \sim N\left(x, \sigma^2 I\right)}\left[\left|s_\theta(\tilde{x}, \sigma)+\frac{\bar{x}-x}{\sigma^2}\right|_2^2\right]
$$&lt;/p&gt;
&lt;p&gt;其中 $s_\theta(\tilde{x}, \sigma)$ 代表网络在特定噪声级别 $\sigma$ 下估计的分数。
而 NCSN 使用了多个噪声级别, 于是, 分别对它们的损失加权求和后再求均值, 就得到了联合的损失函数，它表示为:&lt;/p&gt;
&lt;p&gt;$$
L\left(\theta ;\left\{\sigma_i\right\}_{i=1}^L\right) \equiv \frac{1}{L} \sum_{i=1}^L \lambda\left(\sigma_i\right) l\left(\theta ; \sigma_i\right)
$$&lt;/p&gt;
&lt;p&gt;其中$\lambda\left(\sigma_i\right)&amp;gt;0$代表不同噪声级别的损失的权重，但是应该怎么设置$\lambda\left(\sigma_i\right)$呢？首先需要保证的是加权后所有噪声级别的损失都在同一量级，不受$\sigma_i$ 的大小影响。然后作者 发现网络训练到收玫时, 估计出的分数的量级 $\left\|s_\theta(x, \sigma)\right\|_2$在 $\frac{1}{\sigma}$ 这个水平, 即: $s_\theta(x, \sigma) \asymp \frac{1}{\sigma}$ 。因此作者将 $\lambda(\sigma)$ 设置为 $\sigma^2$。其实，把$\lambda\left(\sigma_i\right)=\sigma_i^2$ 代入$L\left(\theta ;\left\{\sigma_i\right\}_{i=1}^L\right)$会发现:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\lambda\left(\sigma_i\right) l\left(\theta ; \sigma_i\right)&amp;amp;=\sigma_i^2 l\left(\theta ; \sigma_i\right)=\frac{1}{2} E\left[\left\|\sigma_i s_\theta\left(\tilde{x}, \sigma_i\right)+\sigma_i \frac{\tilde{x}-x}{\sigma_i^2}\right\|_2^2\right] \\
&amp;amp; =\frac{1}{2} E\left[\left\|\sigma_i s_\theta\left(\tilde{x}, \sigma_i\right)+\frac{\tilde{x}-x}{\sigma_i}\right\|_2^2\right]\qquad(2)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;由于 $q_{\sigma_i}(\tilde{x} \mid x) \sim N\left(\tilde{x} ; x, \sigma_i^2 I\right)$, 因此有$\frac{\tilde{x}-x}{\sigma}$ 就是噪声 $\epsilon \sim N\left(0, I^2\right)$,这是与 $\sigma_i$ 无关的量, 即不受后者影响。在这种训练方式下, 只要预先设定好 $\left\{\sigma_i\right\}_{i=1}^L$, 然后在每次迭代时随机选取其中一个噪声级别 $\sigma_i$ , 并采样一个标准高斯噪声 $\epsilon$, 对原始数据样本加噪: $\tilde{x}=x+\sigma_i \epsilon$, 接着将加噪后的样本 $\tilde{x}$ 与对应的噪声级别 $\sigma_i$ 作为网络输入, 待其输出预测的分数 $s_\theta\left(\tilde{x}, \sigma_i\right)$ 后, 就可以使用 $(2)$ 来计算损失了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;采样生成：退火朗之万动力学&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig34_hucdb9a1a081a360198c3e428ced2ae1de_6576438_432c2a94551671b8749890297407a028.webp 400w,
               /media/posts/ddpm/fig34_hucdb9a1a081a360198c3e428ced2ae1de_6576438_7c1d57688336a816ad460a34f6ac7191.webp 760w,
               /media/posts/ddpm/fig34_hucdb9a1a081a360198c3e428ced2ae1de_6576438_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig34_hucdb9a1a081a360198c3e428ced2ae1de_6576438_432c2a94551671b8749890297407a028.webp&#34;
               width=&#34;760&#34;
               height=&#34;325&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在训练好模型后，我们需要进行采样生成图片，原文中作者使用的方法为退火朗之万采样法，这种方法实质上是在噪声强度递减的情况下使用朗之万动力学采样，在每个噪声级别下都有一个朗之万动力学采样生成的过程。首先, 初始样本 $\tilde{x}_0$ 从某种固定的先验分布去采样(作者默认采用均匀分布); 然后, 从最大的噪声级别 $\sigma_1$ 开始使用朗之万动力学采样, 直至最小的噪声级别 $\sigma_L$ 。在每个噪声级别进行采样前, 会先设定步长(step size) $\alpha_i$，接着, 在每个噪声级别下, 基于一定的步数 $T$ 去迭代进行朗之万动力学采样。该噪声级别最后一步采样生成的样本会作为下一个噪声级别的初始样本(见上图 $\tilde{x}_0 \leftarrow x_T$); 最后, 待所有噪声级别的朗之万动力学采样过程均完成时, 就得到了最终的生成结果。这里步长设置的动机就是固定 &amp;ldquo;信噪比&amp;rdquo; (signalto-noise) 的量级，即$\frac{\alpha_i s_\theta\left(x, \sigma_i\right)}{2 \sqrt{\alpha_i} z}$ ( $z$ 是随机噪声)。该信噪比的量级为：&lt;/p&gt;
&lt;p&gt;$$
E\left|\frac{\alpha_i s_\theta\left(x, \sigma_i\right)}{2 \sqrt{\alpha_i z}}\right|_2^2 \approx E\left[\frac{\alpha_i\left\|s_\theta\left(x, \sigma_i\right)\right\|_2^2}{4 z^2}\right] \asymp \frac{1}{4} E\left[\left\|\sigma_i s_\theta\left(x, \sigma_i\right)\right\|_2^2\right]
$$&lt;/p&gt;
&lt;p&gt;由于$s_\theta\left(x, \sigma_i\right) \asymp \frac{1}{\sigma_i}$, 于是这里就有 $\sigma_i s_\theta\left(x, \sigma_i\right) \asymp 1$ 。因此，我们得到 $\frac{1}{4} E\left[\left|\sigma_i s_\theta\left(x, \sigma_i\right)\right|_2^2\right] \asymp \frac{1}{4}$ 。这就说明, 在 $\alpha_i \asymp \sigma_i^2$ 的设置下, 信噪比会固定在常量级, 与噪声级别 $\sigma_i$ 无关。&lt;/p&gt;
&lt;h4 id=&#34;36-ddpm和ncsn在sde下的统一&#34;&gt;3.6 DDPM和NCSN在SDE下的统一&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;主要参考&lt;a href=&#34;https://zhuanlan.zhihu.com/p/662786209&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;知乎：@不许打针&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;宋飏大佬在 2021 年的ICLR的 best paper: &lt;a href=&#34;https://arxiv.org/abs/2011.13456&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Score-Based Generative Modeling through Stochastic Differential Equations&lt;/a&gt;中从 SDE（Stochastic Differential Equations）的角度说明了 NCSN 与 DDPM 在 SDE 的角度是一致的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;随机微分过程&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;许多随机过程（特别是扩散过程）都是随机微分方程 (SDE) 的解。一般来说, SDE具有以下形式:&lt;/p&gt;
&lt;p&gt;$$
\mathrm{d} \boldsymbol{x}=f(\boldsymbol{x}, t) \mathrm{d} t+g(t) \mathrm{d} \boldsymbol{w} \Longleftrightarrow \boldsymbol{x}_{t+\Delta t}=\boldsymbol{x}_t+\underbrace{\boldsymbol{f}_t\left(\boldsymbol{x}_t\right) \Delta t}_{\text {确定部分 }}+\underbrace{g_t \sqrt{\Delta t} \boldsymbol{\varepsilon}}_{\text {随机部分 }}, \quad \boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})
$$&lt;/p&gt;
&lt;p&gt;其中 $f(\cdot)$ 被称为&lt;strong&gt;漂移系数 (drift coefficient)&lt;/strong&gt;, $g(t)$ 被称为&lt;strong&gt;扩散系数 (diffusion coefficient)&lt;/strong&gt;, $\boldsymbol{w}$ 是一个标准的布朗运动, $\mathrm{d} w$ 就可以被看成是一个白噪声。这个随机微分方程的解就是一组连续的随机变量 ${\boldsymbol{x}(t)}_{t \in[0, T]}$ 。&lt;/p&gt;
&lt;p&gt;同时，任何SDE都有相应的反向SDE，其闭合形式由下式给出：&lt;/p&gt;
&lt;p&gt;$$
d \boldsymbol{x}=\left[\boldsymbol{f}_t(\boldsymbol{x})-g_t^2 \nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x})\right] d t+g_t d \boldsymbol{w}
$$&lt;/p&gt;
&lt;p&gt;因此，DDPM与NSCN中加噪和去噪的过程不就可以看出正向SDE与逆向SDE？&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/ddpm/fig35_hu0d963069b4bf10aaf1005687a845965a_243433_081861d959d8af5824b72dcfb178a110.webp 400w,
               /media/posts/ddpm/fig35_hu0d963069b4bf10aaf1005687a845965a_243433_e4be07b8e471170364e700457e5b99c8.webp 760w,
               /media/posts/ddpm/fig35_hu0d963069b4bf10aaf1005687a845965a_243433_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/ddpm/fig35_hu0d963069b4bf10aaf1005687a845965a_243433_081861d959d8af5824b72dcfb178a110.webp&#34;
               width=&#34;720&#34;
               height=&#34;311&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对于 DDPM，前向加噪过程为
$$
x_{t+1}=\sqrt{1-\beta_{t+1}} x_t+\sqrt{\beta_{t+1}} \varepsilon
$$&lt;/p&gt;
&lt;p&gt;当 $x \rightarrow 0,(1-x)^\alpha \approx 1-\alpha x$, 那么
$$
\begin{aligned}
x_{t+\Delta t} &amp;amp; =  \sqrt{1-\beta(t+\Delta t) \Delta t} x_t+\sqrt{\beta(t+\Delta t) \Delta t \varepsilon} \\
&amp;amp; \approx\left(1-\frac{1}{2} \beta(t+\Delta t) \Delta t\right) x_t+\sqrt{\beta(t+\Delta t)} \sqrt{\Delta t} \varepsilon \\
&amp;amp; \approx\left(1-\frac{1}{2} \beta(t) \Delta t\right) x_t+\sqrt{\beta(t)} \sqrt{\Delta t \varepsilon}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;对比SDE形式, 得出
$$
f\left(x_t, t\right)=-\frac{1}{2} \beta(t) x_t ;\qquad g(t)=\sqrt{\beta(t)}
$$&lt;/p&gt;
&lt;p&gt;而对于NCSN，前向加噪过程为
$$
x_{t+\Delta t}=x_t+\sqrt{\sigma_{t+\Delta t}^2-\sigma_t^2} \varepsilon
$$&lt;/p&gt;
&lt;p&gt;那么
$$
\begin{aligned}
x_{t+\Delta t} &amp;amp; =x_t+\sqrt{\sigma_{t+\Delta t}^2-\sigma_t^2} \varepsilon \\
&amp;amp; =x_t+\sqrt{\frac{\sigma_{t+\Delta t}^2-\sigma_t^2}{\Delta t}} \sqrt{\Delta t} \varepsilon \\
&amp;amp; =x_t+\sqrt{\frac{\Delta \sigma_t^2}{\Delta t} \sqrt{\Delta t} \varepsilon}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;对比SDE形式, 得出
$$
f\left(x_t, t\right)=0 ;\qquad g(t)=\frac{d}{d t} \sigma_t^2
$$&lt;/p&gt;
&lt;p&gt;因此，从上面推导可以看出NCSN和DDPM其实只是SDE框架下不同的$f$和$g$。同时，注意到：
对于NCSN我们有
$$
\frac{x_t}{\sqrt{1+\sigma_t^2}}=\frac{x_0}{\sqrt{1+\sigma_t^2}}+\frac{\sigma_t}{\sqrt{1+\sigma_t^2}} \varepsilon
$$&lt;/p&gt;
&lt;p&gt;令
$$
\mathrm{x}_t=\frac{x_t}{\sqrt{1+\sigma_t^2}} ; \sqrt{\bar{\alpha}_t}=\frac{1}{\sqrt{1+\sigma_t^2}}
$$&lt;/p&gt;
&lt;p&gt;可得
$$
\mathrm{x}_t=\sqrt{\bar{\alpha}_t} \mathrm{x}_0+\sqrt{1-\bar{\alpha}_t} \varepsilon
$$&lt;/p&gt;
&lt;p&gt;可以看出这跟DDPM的加噪过程是相同的。最后，在DDPM的前向过程中, 可以表示为
$$
\left.\boldsymbol{x}_t \sim q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right)=\mathcal{N}\left(\boldsymbol{x}_t ; \sqrt{\bar{\alpha}_t} \boldsymbol{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)\right)
$$&lt;/p&gt;
&lt;p&gt;对于一个高斯变量 $z \sim \mathcal{N}\left(z ; \mu_z, \Sigma_z\right)$, 有如下结论:
$$
\mu_z=z+\Sigma_z \nabla \log p(z)
$$&lt;/p&gt;
&lt;p&gt;两式带入一下可得
$$
\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0=\boldsymbol{x}_t+\left(1-\bar{\alpha}_t\right) \nabla \log p\left(\boldsymbol{x}_t\right)
$$
$\boldsymbol{x}_t=\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0+\sqrt{1-\bar{\alpha}_t} \varepsilon_t$, 因此
$$
\begin{aligned}
\boldsymbol{x}_0 &amp;amp; =\frac{\boldsymbol{x}_t+\left(1-\bar{\alpha}_t\right) \nabla \log p\left(\boldsymbol{x}_t\right)}{\sqrt{\bar{\alpha}_t}}=\frac{\boldsymbol{x}_t-\sqrt{1-\bar{\alpha}_t} \varepsilon_t}{\sqrt{\bar{\alpha}_t}} \\
&amp;amp; \Rightarrow \nabla \log p\left(\boldsymbol{x}_t\right)=-\frac{\boldsymbol{\varepsilon}_t}{\sqrt{1-\bar{\alpha}_t}}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;因此DDPM去噪过程中预测的是噪声 $\varepsilon_t$, 而噪声正好就是分数的线性关系且方向相反。去噪的过程也就是朝分数方向移动的过程。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;参考文献&#34;&gt;参考文献&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/662786209&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://zhuanlan.zhihu.com/p/662786209&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yinglinzheng.github.io/diffusion-model-tutorial/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://yinglinzheng.github.io/diffusion-model-tutorial/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/658549707&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://zhuanlan.zhihu.com/p/658549707&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kexue.fm/archives/9119#%E8%AF%A5%E6%80%8E%E4%B9%88%E6%8B%86&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kexue.fm/archives/9119#%E8%AF%A5%E6%80%8E%E4%B9%88%E6%8B%86&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/651528231&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://zhuanlan.zhihu.com/p/651528231&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/595866176&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://zhuanlan.zhihu.com/p/595866176&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ho, J., Jain, A., &amp;amp; Abbeel, P. (2020). &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denoising diffusion probabilistic models&lt;/a&gt;. Advances in neural information processing systems, 33, 6840-6851.&lt;/li&gt;
&lt;li&gt;Song, Y., &amp;amp; Ermon, S. (2019). &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generative modeling by estimating gradients of the data distribution&lt;/a&gt;. Advances in neural information processing systems, 32.&lt;/li&gt;
&lt;li&gt;Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp;amp; Poole, B. (2020). &lt;a href=&#34;https://arxiv.org/pdf/2011.13456&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Score-based generative modeling through stochastic differential equations&lt;/a&gt;. arXiv preprint arXiv:2011.13456.&lt;/li&gt;
&lt;li&gt;Welling, M., &amp;amp; Teh, Y. W. (2011). &lt;a href=&#34;https://citeseerx.ist.psu.edu/document?repid=rep1&amp;amp;type=pdf&amp;amp;doi=56f89ce43d7e386bface3cba63e674fe748703fc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian learning via stochastic gradient Langevin dynamics&lt;/a&gt;. In Proceedings of the 28th international conference on machine learning (ICML-11) (pp. 681-688).&lt;/li&gt;
&lt;li&gt;Luo, C. (2022). &lt;a href=&#34;https://arxiv.org/pdf/2208.11970&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding diffusion models: A unified perspective.&lt;/a&gt; arXiv preprint arXiv:2208.11970.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>B样条（B-Splines)</title>
      <link>https://ikerlz.github.io/post/spline/</link>
      <pubDate>Tue, 05 Mar 2024 00:39:27 +0800</pubDate>
      <guid>https://ikerlz.github.io/post/spline/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#一lagrange插值法&#34;&gt;一、Lagrange插值法&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#二bezier贝塞尔曲线与b-splines&#34;&gt;二、（Bezier）贝塞尔曲线与B-Splines&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1bezier贝塞尔曲线&#34;&gt;1、（Bezier）贝塞尔曲线&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2b-splines&#34;&gt;2、B-Splines&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#三样条估计&#34;&gt;三、样条估计&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#四拟合样条对深度学习中的双下降double-decent现象的解释&#34;&gt;四、拟合样条对深度学习中的双下降（Double Decent）现象的解释&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;h2 id=&#34;一lagrange插值法&#34;&gt;一、Lagrange插值法&lt;/h2&gt;
&lt;p&gt;已知若干点，如何得到&lt;strong&gt;光滑&lt;/strong&gt;曲线？是否可以通过在原有数据点上进行点的填充生成曲线？&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig1_hu56ed4bdc945701bcbc543a02d841351a_5272_0999ba0256e004e76caddbc2a1bb6fe9.webp 400w,
               /media/posts/spline/fig1_hu56ed4bdc945701bcbc543a02d841351a_5272_451087a27e071e66517027cbbe5b8df0.webp 760w,
               /media/posts/spline/fig1_hu56ed4bdc945701bcbc543a02d841351a_5272_1200x1200_fit_q75_h2_lanczos_2.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig1_hu56ed4bdc945701bcbc543a02d841351a_5272_0999ba0256e004e76caddbc2a1bb6fe9.webp&#34;
               width=&#34;695&#34;
               height=&#34;520&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

首先，可以考虑两个点的插值：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig2_hucb27f28971e8e84fa66a03f8e6b729f2_167146_32bfc7313d8dc81e10c4d2f0e73fa1fe.webp 400w,
               /media/posts/spline/fig2_hucb27f28971e8e84fa66a03f8e6b729f2_167146_22543dd06a9ae9464ecb074f3452c76c.webp 760w,
               /media/posts/spline/fig2_hucb27f28971e8e84fa66a03f8e6b729f2_167146_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig2_hucb27f28971e8e84fa66a03f8e6b729f2_167146_32bfc7313d8dc81e10c4d2f0e73fa1fe.webp&#34;
               width=&#34;760&#34;
               height=&#34;459&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

考虑$P_0$和$P_1$之间的任意一点$P_x$，可表示为：
$$P_x=P_0+\left(P_1-P_0\right) t=(1-t) P_0+t P_1$$
其中$t={(P_0 P_x)}/{(P_0 P_1)}$。直观上，我们可以把$P_0$和$P_1$视为&lt;strong&gt;控制点&lt;/strong&gt;，$(1-t)$和$t$视作&lt;strong&gt;基函数&lt;/strong&gt;。【思考：两点如何推广到多个点？】&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果知道三个点: $P_0, P_1, P_2$, 如何确定一条曲线 ?
&lt;ul&gt;
&lt;li&gt;想法: 将$P_0, P_1$ 进行连接，然后将$P_1, P_2$ 进行连接,。但是这样的一个曲线并不光滑&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;注意到，直线可以由2个点确定， 而二次曲线由三个点即可确定，推广到一般情况， $n-1$ 阶曲线可以由$n$个点确定&lt;/li&gt;
&lt;li&gt;这本质上就是Lagrange插值法的思想 (必须经过所有点)&lt;/li&gt;
&lt;li&gt;一般来说，如果我们有 $n$ 个点 $\left(x_1, y_1\right), \ldots,\left(x_n, y_n\right)$ ，各 $x_i$ 互不相同。对于 1 到 $\mathrm{n}$ 之间的每个 $k$, 定义 $n-1$ 次多项式
$$
L_k(x)=\frac{\left(x-x_1\right) \ldots\left(x-x_{k-1}\right)\left(x-x_{k+1}\right) \ldots\left(x-x_n\right)}{\left(x_k-x_1\right) \ldots\left(x_k-x_{k-1}\right)\left(x_k-x_{k+1}\right) \ldots\left(x_k-x_n\right)}
$$&lt;/li&gt;
&lt;li&gt;$L_k(x)$ 具有有趣的性质: $L_k\left(x_k\right)=1, L_k\left(x_j\right)=0, j \neq k$. 然后定义一个 $n-1$ 次多项式
$$
P_{n-1}(x)=y_1 L_1(x)+\ldots+y_n L_n(x)=\sum_{i=1}^ny_iL_i(x) .
$$
这样的多项式 $P_{n-1}(x)$ 满足 $P_{n-1}\left(x_i\right)=y_i, i=1,2, \ldots, n$. 因此必过控制点$\{(x_i,y_i)\mid i=1,\ldots,n\}$；这就是著名的拉格朗日插值多项式！&lt;/li&gt;
&lt;li&gt;我们可以把$L_1(x),\ldots,L_n(x)$看作基函数，而Lagrange插值本质上就是一组基的线性组合！&lt;/li&gt;
&lt;/ul&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Lagrange插值法例子&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;已知区间 $[-1,1]$ 上函数 $f(x)=[1+(5 x)^2]^{-1}$。取等距节点
$$
x_i=-1+\frac{i}{5}, \quad i=0,1,2, \cdots, 10
$$&lt;/li&gt;
&lt;li&gt;作Lagrange插值多项式
$$
P_{10}(x)=\sum_{i=0}^{10} f\left(x_i\right) L_i(x) .
$$
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig3_hu351a4dfac429a8bfc34629108fccb8d0_313529_6629893d1fd8ef7ad6631243ce3143d6.webp 400w,
               /media/posts/spline/fig3_hu351a4dfac429a8bfc34629108fccb8d0_313529_fa8cf5705a5f81531403c612ecdb2c6f.webp 760w,
               /media/posts/spline/fig3_hu351a4dfac429a8bfc34629108fccb8d0_313529_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig3_hu351a4dfac429a8bfc34629108fccb8d0_313529_6629893d1fd8ef7ad6631243ce3143d6.webp&#34;
               width=&#34;760&#34;
               height=&#34;752&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

从图中可以看出, 在 $x=0$ 附近, $P_{10}(x)$ 能较好地逼近 $f(x)$, 但在有些地方, 如在 $[-1,-0.8]$ 和 $[0.8,1]$ 之间, $P_{10}(x)$ 与 $f(x)$ 差异很大，这种现象被称为&lt;a href=&#34;https://en.wikipedia.org/wiki/Runge%27s_phenomenon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Runge现象&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;Runge现象出现的原因是在因为在进行Lagrange插值时要求必须要过控制点，因此可以考虑不过控制点进行插值，这样就能避免Runge现象&lt;/font&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;ul&gt;
&lt;li&gt;插值法因为要求经过所有节点, 所以导致这种结果, 因此在此基础上提出了拟合的概念:
&lt;ul&gt;
&lt;li&gt;依据原有数据点，通过参数调整设置，使得生成曲线与原有点差距最小 (最小二乘), 因此曲线未必会经过原有数据点&lt;/li&gt;
&lt;li&gt;样条曲线 (Spline curves): 是给定一系列控制点而得到的一条曲线，曲线形状由这些控制点控制。一般分为&lt;strong&gt;插值样条&lt;/strong&gt;和&lt;strong&gt;拟合样条&lt;/strong&gt;。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig4_hucbdfa747d9f91d6fc6936cd9674c23cd_415381_75d79890c994575678027044f0672a5a.webp 400w,
               /media/posts/spline/fig4_hucbdfa747d9f91d6fc6936cd9674c23cd_415381_6b4d894436fa7d9c9cf0995c04506441.webp 760w,
               /media/posts/spline/fig4_hucbdfa747d9f91d6fc6936cd9674c23cd_415381_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig4_hucbdfa747d9f91d6fc6936cd9674c23cd_415381_75d79890c994575678027044f0672a5a.webp&#34;
               width=&#34;760&#34;
               height=&#34;269&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;二bezier贝塞尔曲线与b-splines&#34;&gt;二、（Bezier）贝塞尔曲线与B-Splines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;均匀节点意义下的一元 B 样条 (B-splines, Basis Splines 缩写)是在 1946 年由 1.J.Schoenberg 提出&lt;/li&gt;
&lt;li&gt;1962 年, 法国数学家 Pierre Bezier 研究了一种曲线, 即 Bezier 曲线&lt;/li&gt;
&lt;li&gt;1972 年, de Boor 与 cox 分别独立提出了计算 B 样条基函数的公式这个公式对 B 样条作为计算机辅助几何设计 (CAGD)重要工具起到了至关重要的作用，称之为 de Boor-Cox 公式，即著名的 de Boor 算法&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1bezier贝塞尔曲线&#34;&gt;1、（Bezier）贝塞尔曲线&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig5_huabbb1fb5b7a7d33de92e8051846b5338_223478_358469c34f76ce019d490d6b79ae99ef.webp 400w,
               /media/posts/spline/fig5_huabbb1fb5b7a7d33de92e8051846b5338_223478_a6c07ad37a250b75791e7eed75cdbff3.webp 760w,
               /media/posts/spline/fig5_huabbb1fb5b7a7d33de92e8051846b5338_223478_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig5_huabbb1fb5b7a7d33de92e8051846b5338_223478_358469c34f76ce019d490d6b79ae99ef.webp&#34;
               width=&#34;760&#34;
               height=&#34;504&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不过 $P_1$ :
$$
\begin{gathered}
P_i&amp;amp;=(1-t) P_0+t P_1 ;\\
P_j&amp;amp;=(1-t) P_1+t P_2 ; \\
P_x&amp;amp;=(1-t) P_i+t P_j.
\end{gathered}
$$
其中，
$$
\frac{P_0 P_i}{P_0 P_1}=\frac{P_1 P_j}{P_1 P_2}=\frac{P_j P_x}{P_i P_j}=t
$$&lt;/li&gt;
&lt;li&gt;因此我们可以得到：$P_x=\left(1-t^2\right) P_0+2 t(1-t) P_1+t^2 P_2$&lt;/li&gt;
&lt;li&gt;我们可以把$P_0,P_1,P_2$看作控制点，把$(1-t^2)$，$2t(1-t)$和$t^2$看作是&lt;strong&gt;基函数&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;推广到一般情况，假设一共有 $n+1$ 个点, 就可以确定了$n$次的贝塞尔曲线
$$
B(t)=\sum_{i=0}^n C_n^i(1-t)^{n-i} t^i P_i, \quad t \in[0,1]
$$&lt;/li&gt;
&lt;li&gt;或者写成这样
$$
B(t)=W_{t, n}^0 P_0+W_{t, n}^1 P_1+\cdots+W_{t, n}^n P_n
$$&lt;/li&gt;
&lt;li&gt;可以理解为以$W$为基, $P$为系数的线性组合；其中$W_i=C_n^i(1-t)^{n-i} t^i$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;注：当有$n+1$个点, 有$n+1$个基函数, 确定$n$阶函数曲线&lt;/font&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$W_{t, n}^i$为$P_i$的系数, 是最高幂次为$n$的关于$t$的多项式。当 $t$确定后, 该值就为定值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;因此整个式子可以理解为$B(t)$插值点是这$n+1$个点施加各自的权重$W$后累加得到的。这也是为什么改变其中一个控制点, 整个贝塞尔曲线都会受到影响。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其实对于样条曲线的生成，本质上就对于各个控制点施加权重&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$n$阶贝塞尔曲线$B^n(t)$可以由&lt;strong&gt;前$n$个点&lt;/strong&gt;决定的$n-1$次贝塞尔曲线$B^{n-1}\left(t \mid P_0, \cdots, P_{n-1}\right)$与&lt;strong&gt;后$n$个点&lt;/strong&gt;决定的$n-1$次贝塞尔曲线$B^{n-1}\left(t \mid P_1, \cdots, P_n\right)$线性组合递推而来，即
$$
{\color{red}
\begin{aligned}
&amp;amp; B^n\left(t \mid P_0, P_1, \cdots, P_n\right)= \\
&amp;amp; (1-t) B^{n-1}\left(t \mid P_0, P_1, \cdots, P_{n-1}\right)+t B^{n-1}\left(t \mid P_1, P_2, \cdots, P_n\right)
\end{aligned}
}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2b-splines&#34;&gt;2、B-Splines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;我们比较好奇的是对于B样条，怎么得到各控制点前的权重（基函数）&lt;/li&gt;
&lt;li&gt;B-Splines的一些重要定义：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;控制点&lt;/strong&gt;: 控制曲线的点, 等价于贝塞尔函数的控制点, 通过控制点可以控制曲线形状。假设有 $n+1$ 个控制点 $P_0, P_1, \ldots, P_n$ 。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;节点&lt;/strong&gt;: 与控制点无关，是人为地将目标曲线分为若干个部分,其目的就是尽量使得各个部分有所影响但也有一定独立性,这也是为什么 $\mathrm{B}$ 样条中, 有时一个控制点的改变, 不会很大影响到整条曲线，而只影响到局部的原因，这区别于贝塞尔曲线。
&lt;ul&gt;
&lt;li&gt;节点划分影响权重计算，假设我们划分 $m+1$ 个节点 $t_0, t_1, \ldots, t_m$ ，将曲线分成了 $m$ 段。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;次 (degree) 与阶 (order)&lt;/strong&gt;: 次的概念是贝塞尔中次的概念, 即权重中$t$的最高幂次。阶 (order) $=$ 次 (degree) +1 。通常我们用 $k$ 表示次。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;注意到：
$$
B(t)=\sum_{i=0}^n W_i P_i=\sum_{i=0}^n B_{i, k}(t) P_i
$$
我们需要获得$W_i$即可。$W_i$是关于$t$的函数, 最高幂次为$k$。$B$样条中通常记为$B_{i, k}(t)$, 即表示第$i$点的权重, 是关于$t$的函数，且最高幂次为$k$。而这个权重函数$B_{i, k}(t)$，在 B样条里叫做&lt;font color=&#34;red&#34;&gt;$k$次B样条基函数&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;$B_{i, k}(t)$ 满足如下递推式 (&lt;strong&gt;de Boor 递推式&lt;/strong&gt;)
$$
{\color{blue}
\begin{gathered}
k=0, \quad B_{i, 0}(t)= \begin{cases}1, &amp;amp; t \in\left[t_i, t_{i+1}\right] \\
0, &amp;amp; \text { Otherwise }\end{cases} \\
k&amp;gt;0, \quad B_{i, k}(t)=\frac{t-t_i}{t_{i+k}-t_i} B_{i, k-1}(t)+\frac{t_{i+k+1}-t}{t_{i+k+1}-t_{i+1}} B_{i+1, k-1}(t)
\end{gathered}
}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;节点数、控制点数与次数的关系&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;控制点有$n+1=5$个, $n=4$ ，即 $P_0, P_1, P_2, P_3, P_4$&lt;/li&gt;
&lt;li&gt;节点规定为 $m+1=10$ 个, $m=9$ ，即 $t_0, t_1, \cdots, t_9$ ，该节点将要生成的目标曲线分为了 9 份，
&lt;ul&gt;
&lt;li&gt;这里的 $t$ 取值一般为 $0-1$的一系列非递减数。 $t_0, t_1, \cdots, t_9$ 组成的序列，叫做节点表，如等分的节点表 $\{0, \frac{1}{9}, \frac{2}{9}, \frac{3}{9}, \frac{4}{9}, \frac{5}{9}, \frac{6}{9}, \frac{7}{9}, \frac{8}{9}, 1\}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;次为$k$。&lt;/li&gt;
&lt;li&gt;三者有个必须要满足的关系式为
$$m=n+k+1$$
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig6_hucd6f327c77bf5a6fc4a28977e78830fc_176036_a4f1bb67cf9fc4bcc1c9c2b86f5a8723.webp 400w,
               /media/posts/spline/fig6_hucd6f327c77bf5a6fc4a28977e78830fc_176036_57bab267a8b45ca588e8544a47519cb2.webp 760w,
               /media/posts/spline/fig6_hucd6f327c77bf5a6fc4a28977e78830fc_176036_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig6_hucd6f327c77bf5a6fc4a28977e78830fc_176036_a4f1bb67cf9fc4bcc1c9c2b86f5a8723.webp&#34;
               width=&#34;760&#34;
               height=&#34;335&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;为什么满足$m=n+k+1$？
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig7_hu92917b485e74823b486796e8abf1aa8a_473303_8cb28449cf0f32c8b340d74819855a8c.webp 400w,
               /media/posts/spline/fig7_hu92917b485e74823b486796e8abf1aa8a_473303_452e0e3c04f767e91fdfd73e2fb45c3d.webp 760w,
               /media/posts/spline/fig7_hu92917b485e74823b486796e8abf1aa8a_473303_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig7_hu92917b485e74823b486796e8abf1aa8a_473303_8cb28449cf0f32c8b340d74819855a8c.webp&#34;
               width=&#34;760&#34;
               height=&#34;528&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig8_hu7f6ecb8edaf2c7d073ed44b848a97cde_1515393_d9c6c93dc0fa76d5eb6230735f24decf.webp 400w,
               /media/posts/spline/fig8_hu7f6ecb8edaf2c7d073ed44b848a97cde_1515393_bc3691af10c0f5a95686060ec641411a.webp 760w,
               /media/posts/spline/fig8_hu7f6ecb8edaf2c7d073ed44b848a97cde_1515393_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig8_hu7f6ecb8edaf2c7d073ed44b848a97cde_1515393_d9c6c93dc0fa76d5eb6230735f24decf.webp&#34;
               width=&#34;760&#34;
               height=&#34;605&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;实例计算&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;节点设置：
&lt;ul&gt;
&lt;li&gt;节点向量: $x=0,0.25,0.5,0.75,1$&lt;/li&gt;
&lt;li&gt;节点数: $m+1=5(m=4)$&lt;/li&gt;
&lt;li&gt;节点: $x_0=0, x_1=0.25, x_2=0.5, x_3=0.75, x_4=1$&lt;/li&gt;
&lt;li&gt;节点区间： $\left[x_0, x_1\right),\left[x_1, x_2\right),\left[x_2, x_3\right),\left[x_3, x_4\right)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;0 次 (degree) 基函数为:
$$
\begin{aligned}
&amp;amp; B_{0,0}(x)=\left\{\begin{array}{lcc}
1 &amp;amp; \text { if } &amp;amp; x_0 \leq x&amp;lt;x_1 \\
0 &amp;amp; &amp;amp; \text{ other }
\end{array}\right. \\
&amp;amp; B_{1,0}(x)=\left\{\begin{array}{ccc}
1 &amp;amp; \text { if } &amp;amp; x_1 \leq x&amp;lt;x_2 \\
0 &amp;amp; &amp;amp; \text{ other }
\end{array}\right.\\
&amp;amp; B_{2,0}(x)=\left\{\begin{array}{ccc}
1 &amp;amp; \text { if } &amp;amp; x_2 \leq x&amp;lt;x_3 \\
0 &amp;amp; &amp;amp; \text{ other }
\end{array}\right.\\
&amp;amp; B_{3,0}(x)=\left\{\begin{array}{ccc}
1 &amp;amp; \text { if } &amp;amp; x_3 \leq x&amp;lt;x_4 \\
0 &amp;amp; &amp;amp; \text{ other }
\end{array}\right.\\
&amp;amp; B_{4,0}(x)=\left\{\begin{array}{ccc}
1 &amp;amp; \text { if } &amp;amp; x_4 \leq x&amp;lt;x_5 \\
0 &amp;amp; &amp;amp; \text{ other }
\end{array}\right.
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;因此由上可知, 0次(degree)基函数均是分段函数&lt;/li&gt;
&lt;li&gt;基函数的迭代公式：
$$
\begin{aligned}
&amp;amp; B_{i, k}(x)=\frac{x-x_i}{x_{i+k}-x_i} B_{i, k-1}(x)+\frac{x_{i+k+1}-x}{x_{i+k+1}-x_{i+1}} B_{i+1, k-1}(x)
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;$k=1, i=0$时, 一次基函数
$$
\begin{aligned}
B_{0,1}(x)&amp;amp;=\frac{x-x_0}{x_1-x_0} B_{0,0}(x)+\frac{x_2-x}{x_2-x_1} B_{1,0}(x)\\
&amp;amp;=4 x B_{0,0}(x)+(2-4 x) B_{1,0}(x) \\
B_{0,1}(x)&amp;amp;=\left\{\begin{array}{cc}
4 x &amp;amp; 0 \leq x&amp;lt;0.25 \\
2-4 x &amp;amp; 0.25 \leq x&amp;lt;0.5 \\
0 &amp;amp; \text { other }
\end{array}\right.
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;若 $m=4, n=2, k=1$ 基函数 $n+1=3$ 个，基函数次数为 $k=1$&lt;/li&gt;
&lt;li&gt;若 $m=4, n=1, k=2$ 基函数 $n+1=2$ 个, 基函数次数为 $k=2$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig9_hu4029514fea627bd19700d5211331b497_340604_2738a064db6787d03d1d0084d3a4d540.webp 400w,
               /media/posts/spline/fig9_hu4029514fea627bd19700d5211331b497_340604_2f1e57cf81a50e716c60f257a907007a.webp 760w,
               /media/posts/spline/fig9_hu4029514fea627bd19700d5211331b497_340604_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig9_hu4029514fea627bd19700d5211331b497_340604_2738a064db6787d03d1d0084d3a4d540.webp&#34;
               width=&#34;760&#34;
               height=&#34;533&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h2 id=&#34;三样条估计&#34;&gt;三、样条估计&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;对于多元线性回归，我们有$\widehat{Y}=\mathbf{H}Y$，其中$\mathbf{H}$为帽子矩阵（hat matrix），定义为$\mathbf{H}=X(X^\top X)^{-1}X^\top$&lt;/li&gt;
&lt;li&gt;对于样条估计，假设模型形式为：
$$
\begin{aligned}
y&amp;amp;=\beta+\sum_{i=0}^n B_{i, k}(x) \beta_i\\
&amp;amp;=\beta+\beta_0 B_{0, k}(x)+\ldots+\beta_{m-k-1} B_{m-k-1, k}(x)
\end{aligned}
$$
其中 $\beta_i$ 是待估参数, $B_{i, k}(x)$ 为样条基函数，此时模型中的基函数个数为 $n=m-k-1$&lt;/li&gt;
&lt;li&gt;设计矩阵: 设计矩阵 $k=1, n=2, m=4$ ：(矩阵的列对应节点区间，相应的区间位置，填入相应的基函数)
$$
X=\left[\begin{array}{llll}
1 &amp;amp; B_{01} &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; B_{01} &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; B_{01} &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; B_{01} &amp;amp; B_{11} &amp;amp; B_{21}
\end{array}\right]=\left[\begin{array}{cccc}
1 &amp;amp; 4 x &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; 2-4 x &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; 0 &amp;amp; B_{11} &amp;amp; B_{21} \\
1 &amp;amp; 0 &amp;amp; B_{11} &amp;amp; B_{21}
\end{array}\right]
$$&lt;/li&gt;
&lt;li&gt;帽子矩阵: $H=X\left(X^T X\right)^{-1} X^T$, 样条估计为: $\hat{y}=H y$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;四拟合样条对深度学习中的双下降double-decent现象的解释&#34;&gt;四、拟合样条对深度学习中的双下降（Double Decent）现象的解释&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://threadreaderapp.com/thread/1292293102103748609.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;参考 Prof. Daniela Witten发在Twitter上的文章&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig10_hu0ca6d9fa5e692adffadf293007778908_458298_e33d673138bff9da2c08b6ca5648f04d.webp 400w,
               /media/posts/spline/fig10_hu0ca6d9fa5e692adffadf293007778908_458298_62698b2655eb5636044058c534368b3d.webp 760w,
               /media/posts/spline/fig10_hu0ca6d9fa5e692adffadf293007778908_458298_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig10_hu0ca6d9fa5e692adffadf293007778908_458298_e33d673138bff9da2c08b6ca5648f04d.webp&#34;
               width=&#34;742&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bias-Variance Trade-off ?
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig11_hue0e5b97ed47da14796153e2957ee5fc4_173808_4470bed254fb93229994a514612ca29c.webp 400w,
               /media/posts/spline/fig11_hue0e5b97ed47da14796153e2957ee5fc4_173808_62796b489d601067ca50d513b6324c67.webp 760w,
               /media/posts/spline/fig11_hue0e5b97ed47da14796153e2957ee5fc4_173808_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig11_hue0e5b97ed47da14796153e2957ee5fc4_173808_4470bed254fb93229994a514612ca29c.webp&#34;
               width=&#34;760&#34;
               height=&#34;403&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;$\mathrm{U}$型测试误差曲线基于以下公式：
$$\text{Exp. Pred. Error} = \text{Irreducible Error}+\operatorname{Bias}^2+ \text{Var}$$&lt;/li&gt;
&lt;li&gt;随着灵活性的增加, (平方) 偏差减少, 方差增加。sweet spot需要权衡偏差和方差, 即具有中等程度灵活性的模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;过去的几年中，尤其是在深度学习领域，已经出现双下降现象。当你继续拟合越来越灵活且对训练数据进行插值处理的模型时，测试误差会再次减小！
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig12_hu787ed22831ed752e51031e26be985eac_102739_5ebb47993bfa56002fa09938c91fd4f5.webp 400w,
               /media/posts/spline/fig12_hu787ed22831ed752e51031e26be985eac_102739_51cc0a311ce3c3ce9deea09dbcbaf19d.webp 760w,
               /media/posts/spline/fig12_hu787ed22831ed752e51031e26be985eac_102739_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig12_hu787ed22831ed752e51031e26be985eac_102739_5ebb47993bfa56002fa09938c91fd4f5.webp&#34;
               width=&#34;702&#34;
               height=&#34;461&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;考虑自然三次样条曲线（natural cubic spline），拟合模型为$Y=f(x)+\epsilon$，所用基函数的数量与样条曲线的自由度（degrees of freedom, DF）相同。基函数基本形式如下：
$$
\left(X-\psi_1\right)_{+}^3,\quad \ldots,\quad\left(X-\psi_K\right)_{+}^3
$$&lt;/li&gt;
&lt;li&gt;我们首先生成$n=20$组$(X,Y)$数据，首先拟合一个$df=4$的样条曲线【$n=20$ 时的观测值为灰色小圆点，$f(x)=\sin(x)$ 为黑色曲线，拟合函数为浅蓝色曲线。】
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig13_hu9c3226fce5e0df57ecb32b8fa1875fdf_58494_0e271fef4f18de9a42b1f4fd23dbf39a.webp 400w,
               /media/posts/spline/fig13_hu9c3226fce5e0df57ecb32b8fa1875fdf_58494_1bba8e95ecb048c93ad0d8276220fd58.webp 760w,
               /media/posts/spline/fig13_hu9c3226fce5e0df57ecb32b8fa1875fdf_58494_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig13_hu9c3226fce5e0df57ecb32b8fa1875fdf_58494_0e271fef4f18de9a42b1f4fd23dbf39a.webp&#34;
               width=&#34;676&#34;
               height=&#34;364&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;然后分别拟合$df=6$和$df=20$的样条曲线
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig14_hu33ef8ba9e1ea428490d24ba3846b3659_54374_1c0bcd67fbae243924fa005c1234f64d.webp 400w,
               /media/posts/spline/fig14_hu33ef8ba9e1ea428490d24ba3846b3659_54374_40cc56b6bc7bfd4635493fb1b2fffb47.webp 760w,
               /media/posts/spline/fig14_hu33ef8ba9e1ea428490d24ba3846b3659_54374_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig14_hu33ef8ba9e1ea428490d24ba3846b3659_54374_1c0bcd67fbae243924fa005c1234f64d.webp&#34;
               width=&#34;681&#34;
               height=&#34;359&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig15_hu76ae5e1e118b955f2590ebd220748f58_69288_0c1c80d0e27fcb40aab840ad5e08bc67.webp 400w,
               /media/posts/spline/fig15_hu76ae5e1e118b955f2590ebd220748f58_69288_d42be289742ae241cf457b9bf846a982.webp 760w,
               /media/posts/spline/fig15_hu76ae5e1e118b955f2590ebd220748f58_69288_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig15_hu76ae5e1e118b955f2590ebd220748f58_69288_0c1c80d0e27fcb40aab840ad5e08bc67.webp&#34;
               width=&#34;681&#34;
               height=&#34;351&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

注意到：为了拟合$df=20$的样条曲线，需要用20个数据点来运行最小二乘法！结果显示在训练集上零误差，但在测试集上误差非常大！这些糟糕的结果也非常符合偏差-方差权衡&lt;/li&gt;
&lt;li&gt;如果继续拟合$df=36$（$p&amp;gt;n$）的样条曲线，由于解是不唯一的，为了在无穷多个解中进行选择，可以选择范数拟合：系数平方和最小的那个（利用SVD计算）
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig16_hu4fb763d4e7cf60838a1e8b720bc87b4b_65565_c626d953e59475bef66158e904d81cd5.webp 400w,
               /media/posts/spline/fig16_hu4fb763d4e7cf60838a1e8b720bc87b4b_65565_d0452b852ec123a043e5615418ddd971.webp 760w,
               /media/posts/spline/fig16_hu4fb763d4e7cf60838a1e8b720bc87b4b_65565_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig16_hu4fb763d4e7cf60838a1e8b720bc87b4b_65565_c626d953e59475bef66158e904d81cd5.webp&#34;
               width=&#34;688&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;对比了$df=20$和$df=36$的结果，可见$df=36$的结果比$df=20$要好一点。这是什么原因呢？
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig17_hu4f65f573c3a188111c745770ec28ca50_49933_57611722881f134a3846df8043203293.webp 400w,
               /media/posts/spline/fig17_hu4f65f573c3a188111c745770ec28ca50_49933_7c580ee94b25688ff51b6101faa52f56.webp 760w,
               /media/posts/spline/fig17_hu4f65f573c3a188111c745770ec28ca50_49933_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig17_hu4f65f573c3a188111c745770ec28ca50_49933_57611722881f134a3846df8043203293.webp&#34;
               width=&#34;700&#34;
               height=&#34;179&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;下图是训练误差和测试误差曲线，两者的变化曲线差别非常大。以虚线为分界线，当$p&amp;gt;n$时，为什么测试误差（暂时）减少？
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/posts/spline/fig18_hu4c909df76bd900404e830bcee74a1f31_49763_461252ed6375aea96873374b408206e6.webp 400w,
               /media/posts/spline/fig18_hu4c909df76bd900404e830bcee74a1f31_49763_cc7f2bf6b30ca26f29e7d25fe338e1ce.webp 760w,
               /media/posts/spline/fig18_hu4c909df76bd900404e830bcee74a1f31_49763_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/spline/fig18_hu4c909df76bd900404e830bcee74a1f31_49763_461252ed6375aea96873374b408206e6.webp&#34;
               width=&#34;706&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;一个合理的解释为：关键在于当$df=20$，即$df=n$时，只有一个最小二乘拟合的训练误差为零。这种拟合会出现大量的振荡。&lt;/li&gt;
&lt;li&gt;但是当增加自由度，使得$p&amp;gt;n$时，则会出现大量的插值最小二乘拟合。最小范数的最小二乘拟合是这无数多个拟合中振荡最小的，甚至比$p=n$时的拟合更稳定。&lt;/li&gt;
&lt;li&gt;所以，选择最小范数最小二乘拟合实际上意味着$df=36$的样条曲线比$df=20$ 的样条曲线的灵活性差。&lt;/li&gt;
&lt;li&gt;如果在拟合样条曲线时使用了ridge penalty，而不是最小二乘，结果会怎么样呢？这时将不会有插值训练集，也不会看到双下降，而且会得到更好的测试误差&lt;/li&gt;
&lt;li&gt;所以，这些与深度学习有何关系？当使用（随机）梯度下降法来拟合神经网络时，实际上是在挑选最小范数解！因此，样条曲线示例非常类似于神经网络双下降时发生的情况。&lt;/li&gt;
&lt;li&gt;换种说法，当模型能力恰好能够产生零训练误差时，该现象导致测试误差达到峰值。但是，峰值不会出现在多层网络中，因为多层网络在优化时存在&lt;strong&gt;隐式正则化&lt;/strong&gt;的现象&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Distributed Estimation and Inference for Spatial Autoregression Model with Large Scale Networks</title>
      <link>https://ikerlz.github.io/publication/dsar/</link>
      <pubDate>Sat, 27 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://ikerlz.github.io/publication/dsar/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Disentangle Mixture Distributions and Instrumental Variable Inequalities</title>
      <link>https://ikerlz.github.io/slides/causalinferencechapter22/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://ikerlz.github.io/slides/causalinferencechapter22/</guid>
      <description>
&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;ZIBIN.jpg&#34;
  &gt;

&lt;h1 id=&#34;disentangle-mixture-distributions-and-instrumental-variable-inequalities&#34;&gt;Disentangle Mixture Distributions and Instrumental Variable Inequalities&lt;/h1&gt;
&lt;p&gt;$$
\begin{aligned}
\
\end{aligned}
$$&lt;/p&gt;
&lt;center&gt; Li Zhe &lt;center&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;School of Data Science, Fudan University &lt;center&gt; 
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;January 3, 2024&lt;center&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The IV model in last chapter imposes three assumptions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(&lt;mark&gt;randomization&lt;/mark&gt;) $Z \perp\!\!\!\perp\{D(1), D(0), Y(1), Y(0)\}$&lt;/li&gt;
&lt;li&gt;(&lt;mark&gt;monotonicity&lt;/mark&gt;) $\operatorname{pr}(U=\mathrm{d})=0$ or $D_i(1)\geq D_i(0)$&lt;/li&gt;
&lt;li&gt;(&lt;mark&gt;exclusionrestriction&lt;/mark&gt;) $Y(1)=Y(0) \text { for } U=\mathrm{a} \text { or } \mathrm{n}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
{\tiny
U_i= \begin{cases}
a, &amp;amp; \text { if } D_i(1)=1 \text { and } D_i(0)=1 \\
c, &amp;amp; \text { if } D_i(1)=1 \text { and } D_i(0)=0 \\
d, &amp;amp; \text { if } D_i(1)=0 \text { and } D_i(0)=1 \\
n, &amp;amp; \text { if } D_i(1)=0 \text { and } D_i(0)=0
\end{cases}
}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Observed groups and latent groups under the assumptions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
{\small
\begin{array}{cccl}
Z=1 &amp;amp; D=1 &amp;amp; D(1)=1 &amp;amp; U=\mathrm{c} \text { or a } \\
Z=1 &amp;amp; D=0 &amp;amp; D(1)=0 &amp;amp; U=\mathrm{n} \\
Z=0 &amp;amp; D=1 &amp;amp; D(0)=1 &amp;amp; U=\mathrm{a} \\
Z=0 &amp;amp; D=0 &amp;amp; D(0)=0 &amp;amp; U=\mathrm{c} \text { or } \mathrm{n}
\end{array}
}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interestingly, the assumptions have some &lt;mark&gt;&lt;strong&gt;testable implications&lt;/strong&gt;&lt;/mark&gt;. Balke and Pearl (1997) called them the &lt;mark&gt;&lt;strong&gt;instrumental variable inequalities&lt;/strong&gt;&lt;/mark&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;outline&#34;&gt;Outline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Disentangle Mixture Distributions and Instrumental Variable Inequalities&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Testable implications&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Examples&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;disentangle-mixture-distributions-and-iv-inequalities&#34;&gt;Disentangle Mixture Distributions and IV Inequalities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recall $\pi_u$ as the proportion of type $U=u$, and define&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\mu_{z u}=E\{Y(z) \mid U=u\}, \quad(z=0,1 ; u=\mathrm{a}, \mathrm{n}, \mathrm{c}) .
$$&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;&lt;strong&gt;Theorem 22.1&lt;/strong&gt;&lt;/mark&gt; Under the three assumptions, we can identify the proportions of the latent types by
$$
\begin{aligned}
&amp;amp; \pi_{\mathrm{n}}=\operatorname{pr}(D=0 \mid Z=1), \\
&amp;amp; \pi_{\mathrm{a}}=\operatorname{pr}(D=1 \mid Z=0), \\
&amp;amp; \pi_{\mathrm{c}}=E(D \mid Z=1)-E(D \mid Z=0),
\end{aligned}
$$
and the type-specific means of the potential outcomes by
$$
\begin{aligned}
\mu_{1 \mathrm{n}}=\mu_{0 \mathrm{n}} \equiv \mu_{\mathrm{n}} &amp;amp; =E(Y \mid Z=1, D=0) \\
\mu_{1 \mathrm{a}}=\mu_{0 \mathrm{a}} \equiv \mu_{\mathrm{a}} &amp;amp; =E(Y \mid Z=0, D=1) \\
\mu_{1 \mathrm{c}} &amp;amp; =\pi_{\mathrm{c}}^{-1}\{E(D Y \mid Z=1)-E(D Y \mid Z=0)\} \\
\mu_{0 \mathrm{c}} &amp;amp; =\pi_{\mathrm{c}}^{-1}[E\{(1-D) Y \mid Z=0\}-E\{(1-D) Y \mid Z=1\}]
\end{aligned}
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;proof-of-theorem-221-part-i&#34;&gt;Proof of Theorem 22.1 (Part I)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We can identify the proportion of the &lt;em&gt;never takers&lt;/em&gt; by&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{aligned}
\operatorname{pr}(D=0 \mid Z=1) &amp;amp; =\operatorname{pr}(U=\mathrm{n} \mid Z=1)=\operatorname{pr}(U=\mathrm{n})=\pi_{\mathrm{n}},
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the proportion of the &lt;em&gt;always takers&lt;/em&gt; by&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{aligned}
\operatorname{pr}(D=1 \mid Z=0) &amp;amp; =\operatorname{pr}(U=\mathrm{a} \mid Z=0)=\operatorname{pr}(U=\mathrm{a})=\pi_{\mathrm{a}} .
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the proportion of compliers is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{aligned}
\pi_{\mathrm{c}} &amp;amp; =\operatorname{pr}(U=\mathrm{c})=1-\pi_{\mathrm{n}}-\pi_{\mathrm{a}} \\
&amp;amp; =1-\operatorname{pr}(D=0 \mid Z=1)-\operatorname{pr}(D=1 \mid Z=0) \\
&amp;amp; =E(D \mid Z=1)-E(D \mid Z=0)=\tau_D,
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; Although we do not know individual latent compliance types for all units, we can identify the proportions of never takers, always takers, and compliers.&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;proof-of-theorem-221-part-ii&#34;&gt;Proof of Theorem 22.1 (Part II)&lt;/h3&gt;
&lt;p&gt;Under the three assumptions, we have&lt;/p&gt;
&lt;p&gt;$$
\mu_{1 \mathrm{a}}=\mu_{0 \mathrm{a}} \equiv \mu_{\mathrm{a}}, \quad \mu_{1 \mathrm{n}}=\mu_{0 \mathrm{n}} \equiv \mu_{\mathrm{n}} .
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The observed group $(Z=1, D=0)$ only has never takers, so&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
E(Y \mid Z=1, D=0)=E\{Y(1) \mid Z=1, U=\mathrm{n}\}=E\{Y(1) \mid U=\mathrm{n}\}=\mu_{\mathrm{n}} .
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The observed group $(Z=0, D=1)$ only has always takers, so&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
E(Y \mid Z=0, D=1)=E\{Y(0) \mid Z=0, U=\mathrm{a}\}=E\{Y(0) \mid U=\mathrm{a}\}=\mu_{\mathrm{a}}
$$&lt;/p&gt;
&lt;p&gt;$$~~$$&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;How about the observed groups $(Z=1, D=1)$ and $(Z=0, D=0)$ ?&lt;/font&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h4 id=&#34;the-observed-group-z1-d1&#34;&gt;The Observed Group $(Z=1, D=1)$&lt;/h4&gt;
&lt;p&gt;The observed group $(Z=1, D=1)$ has both &lt;em&gt;compliers&lt;/em&gt; and &lt;em&gt;always takers&lt;/em&gt;, so
$$
\small
\begin{aligned}
E(Y \mid Z=1, D=1)= &amp;amp; E\{Y(1) \mid Z=1, D(1)=1\} \\
= &amp;amp; E\{Y(1) \mid D(1)=1\} \\
= &amp;amp; \operatorname{pr}\{D(0)=1 \mid D(1)=1\} E\{Y(1) \mid D(1)=1, D(0)=1\} \\
&amp;amp; +\operatorname{pr}\{D(0)=0 \mid D(1)=1\} E\{Y(1) \mid D(1)=1, D(0)=0\} \\
= &amp;amp; \frac{\pi_{\mathrm{c}}}{\pi_{\mathrm{c}}+\pi_{\mathrm{a}}} \mu_{1 \mathrm{c}}+\frac{\pi_{\mathrm{a}}}{\pi_{\mathrm{c}}+\pi_{\mathrm{a}}} \mu_{\mathrm{a}} .
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Solve the linear equation above to obtain
$$
\small
\begin{aligned}
\mu_{1 \mathrm{c}}= &amp;amp; \pi_{\mathrm{c}}^{-1}\left\{\left(\pi_{\mathrm{c}}+\pi_{\mathrm{a}}\right) E(Y \mid Z=1, D=1)-\pi_{\mathrm{a}} E(Y \mid Z=0, D=1)\right\} \\
= &amp;amp; \pi_{\mathrm{c}}^{-1}\{\operatorname{pr}(D=1 \mid Z=1) E(Y \mid Z=1, D=1) \\
&amp;amp; \quad-\operatorname{pr}(D=1 \mid Z=0) E(Y \mid Z=0, D=1)\} \\
= &amp;amp; \pi_{\mathrm{c}}^{-1}\{E(D Y \mid Z=1)-E(D Y \mid Z=0)\} .
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Based on the formulas of $\mu_{1 \mathrm{c}}$ and $\mu_{0 \mathrm{c}}$ in Theorem 22.1, we have&lt;/p&gt;
&lt;p&gt;$$
\tau_{\mathrm{c}}=\mu_{1 \mathrm{c}}-\mu_{0 \mathrm{c}}=\{E(Y \mid Z=1)-E(Y \mid Z=0)\} / \pi_{\mathrm{c}}.
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;testable-implications&#34;&gt;Testable implications&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Is there any additional value of the this detour for deriving the formula of $\tau_c$ ?&lt;/li&gt;
&lt;li&gt;For binary outcome, the following inequalities must be true:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
0 \leq \mu_{1 \mathrm{c}} \leq 1, \quad 0 \leq \mu_{0 \mathrm{c}} \leq 1,
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It implies four inequalities&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\small
\begin{aligned}
E(D Y \mid Z=1)-E(D Y \mid Z=0) &amp;amp; \geq 0, \\
E(D Y \mid Z=1)-E(D Y \mid Z=0) &amp;amp; \leq E(D \mid Z=1)-E(D \mid Z=0), \\
E\{(1-D) Y \mid Z=0\}-E\{(1-D) Y \mid Z=1\} &amp;amp; \geq 0, \\
E\{(1-D) Y \mid Z=0\}-E\{(1-D) Y \mid Z=1\} &amp;amp; \leq E(D \mid Z=1)-E(D \mid Z=0) .
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;&lt;strong&gt;Theorem 22.2 (Instrumental Variable Inequalities)&lt;/strong&gt;&lt;/mark&gt; With a binary outcome $Y$, the three assumptions imply&lt;/p&gt;
&lt;p&gt;$$
E(Q \mid Z=1)-E(Q \mid Z=0) \geq 0,
$$&lt;/p&gt;
&lt;p&gt;where $Q=D Y, D(1-Y),(D-1) Y$ and $D+Y-D Y$.&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;remarks&#34;&gt;Remarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Under the three assumptions, the difference in means for $Q=$ $D Y, D(1-Y),(D-1) Y$ and $D+Y-D Y$ must all be non-negative.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Importantly, these implications only involve the distribution of the &lt;strong&gt;observed&lt;/strong&gt; variables.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rejection of the IV inequalities leads to rejection of the IV assumptions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Balke and Pearl (1997) derived more general IV inequalities without assuming monotonicity.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;eample-1&#34;&gt;Eample 1.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Investigators et al. (2014) assess the effectiveness of the &lt;em&gt;emergency endovascular&lt;/em&gt; versus &lt;em&gt;the open surgical repair strategies&lt;/em&gt; for patients with a clinical diagnosis of ruptured aortic aneurism.&lt;/li&gt;
&lt;li&gt;Patients are randomized to either the emergency endovascular or the open repair strategy.&lt;/li&gt;
&lt;li&gt;The primary outcome is the survival status after 30 days&lt;/li&gt;
&lt;li&gt;Let $Z$ be the treatment assigned, with $Z=1$ for the endovascular strategy and $Z=0$ for the open repair.&lt;/li&gt;
&lt;li&gt;Let $D$ be the treatment received.&lt;/li&gt;
&lt;li&gt;Let $Y$ be the survival status, with $Y=1$ for dead, and $Y=0$ for alive.&lt;/li&gt;
&lt;li&gt;The estimate of $\tau_{\mathrm{c}}$ is 0.131 with $95 \%$ confidence interval $(-0.036,0.298)$ including 0 .&lt;/li&gt;
&lt;li&gt;Using the function &amp;ldquo;&lt;code&gt;IVbinary&lt;/code&gt;&amp;rdquo; above, we can obtain $\hat\mu_{1c}=0.708$ and $\hat\mu_{0c}=0.629$&lt;/li&gt;
&lt;li&gt;There is no evidence of violating the IV assumptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;example-2&#34;&gt;Example 2.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In Hirano et al. (2000), physicians are randomly selected to receive a letter encouraging them to inoculate patients at risk for flu.&lt;/li&gt;
&lt;li&gt;The treatment is the actual flu shot, and the outcome is an indicator for flu-related hospital visits.&lt;/li&gt;
&lt;li&gt;However, some patients do not comply with their assignments. Let $Z_i$ be the indicator of encouragement to receive the flu shot, with $Z=1$ if the physician receives the encouragement letter, and $Z=0$ otherwise.&lt;/li&gt;
&lt;li&gt;Let $D$ be the treatment received.&lt;/li&gt;
&lt;li&gt;Let $Y$ be the outcome, with $Y=0$ if for a flu-related hospitalization during the winter, and $Y=1$ otherwise.&lt;/li&gt;
&lt;li&gt;The estimate of $\tau_{\mathrm{c}}$ is 0.116 with $95 %$ confidence interval $(-0.061,0.293)$ including 0 .&lt;/li&gt;
&lt;li&gt;Using the function above, we can obtain $\hat\mu_{1c}=-0.0045$ and $\hat\mu_{0c}=0.1200$&lt;/li&gt;
&lt;li&gt;Since $\hat\mu_{1c}&amp;lt;0$, there is evidence of violating the IV assumptions.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>On factor models with random missing： EM estimation, inference, and cross validation</title>
      <link>https://ikerlz.github.io/slides/factormissing/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://ikerlz.github.io/slides/factormissing/</guid>
      <description>
&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;ZIBIN.jpg&#34;
  &gt;

&lt;h1 id=&#34;on-factor-models-with-random-missing-em-estimation-inference-and-cross-validation&#34;&gt;On factor models with random missing: EM estimation, inference, and cross validation&lt;/h1&gt;
&lt;p&gt;$$
\begin{aligned}
\
\end{aligned}
$$&lt;/p&gt;
&lt;center&gt; Li Zhe &lt;center&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;School of Data Science, Fudan University &lt;center&gt; 
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;November 30, 2023&lt;center&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;outline&#34;&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Background: &lt;strong&gt;factor model&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Motivation&lt;/li&gt;
&lt;li&gt;Factor models with random missing:
&lt;ul&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt;EM estimator&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt;Asymptotic properties&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;Determining the number of factors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simulation&lt;/li&gt;
&lt;li&gt;Empirical application&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;background-factor-model&#34;&gt;Background: factor model&lt;/h2&gt;
&lt;p&gt;$$
\begin{aligned}
x_{i t} &amp;amp; =\lambda_i f_t+e_{i t} \\
\boldsymbol{X} &amp;amp;= \boldsymbol{\Lambda} \boldsymbol{F}^\top + \boldsymbol{E}
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{X}=(\boldsymbol{x}_{\cdot 1},\ldots,\boldsymbol{x}_{\cdot T})\in\mathbb{R}^{N\times T}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{\Lambda}\in\mathbb{R}^{N\times R}$: factor loadings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{F}\in\mathbb{R}^{T\times R}$: common factors (latent, unobserved)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{E}\in\mathbb{R}^{N\times T}$: idiosyncratic (or error) component&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;${e_{it}}$ can exhibit both cross-sectional and temporal dependence.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given the factor number $k$, we can estimate the factors and factor loadings by
$$
\Big\{\widehat{\boldsymbol{\Lambda}}^k, \widehat{\boldsymbol{F}}^k\Big\}=\arg\min_{\boldsymbol{\Lambda}^k,\boldsymbol{F}^k}\frac{1}{NT}\Big\|\boldsymbol{X}- \boldsymbol{\Lambda}^k{\boldsymbol{F}^k}^\top\Big\|_F^2
$$
where $\boldsymbol{\Lambda}^k\in\mathbb{R}^{N\times k}$ and $\boldsymbol{F}^k\in\mathbb{R}^{T\times k}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Factor model in balanced panel has been thoroughly investigated.
$$$$&lt;/li&gt;
&lt;li&gt;How to handle the missing data problem in factor models?
$$$$
&lt;ul&gt;
&lt;li&gt;the expectation–maximization (EM) algorithm
$$$$&lt;/li&gt;
&lt;li&gt;the Kalman filter (KF)
$$$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;There is no formal study of the &lt;strong&gt;asymptotic properties&lt;/strong&gt; for the EM estimators of the factors and factor loadings for the PC estimation with &lt;mark&gt;missing observations&lt;/mark&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;notations&#34;&gt;Notations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;consider the factor model
$$
\boldsymbol{X} =  \boldsymbol{F}\boldsymbol{\Lambda}^\top + \varepsilon
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol{X}=(X_1,\ldots,X_N)$ where $X_i \equiv\left(X_{i 1}, \ldots, X_{i T}\right)^{\prime}$ and $X_{it}$ are &lt;font color=red&gt; &lt;strong&gt;missing at random&lt;/strong&gt; &lt;/font&gt;&lt;/li&gt;
&lt;li&gt;$\varepsilon=\left(\varepsilon_1, \ldots, \varepsilon_N\right)$ and $\varepsilon_i \equiv\left(\varepsilon_{i 1}, \ldots, \varepsilon_{i T}\right)^{\prime}$ for $i=1, \ldots, N$.&lt;/li&gt;
&lt;li&gt;$F=\left(F_1, \ldots, F_T\right)^{\prime}$ and $\Lambda=\left(\lambda_1, \ldots, \lambda_N\right)^{\prime}$ where $F_t$ and $\lambda_i$ are $R \times 1$ vectors of factors and factor loadings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$F^0=\left(F_1^0, \ldots, F_T^0\right)^{\prime}$ and $\Lambda^0=\left(\lambda_1^0, \ldots, \lambda_N^0\right)^{\prime}$ are the true values of $F$ and $\Lambda$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Omega \subset[N] \times[T]$ be the index set of the observations that are observed. That is,
$$
\Omega=\Big\{(i, t) \in[N] \times[T]: X_{i t} \text { is observed }\Big\}.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let $G$ denote a $T \times N$ matrix with $(t, i)$ th element given by $g_{i t}=\mathbf{1}\{(i, t) \in \Omega\}$ and is &lt;mark&gt;independent of $X, F^0, \Lambda^0$ and $\varepsilon$&lt;/mark&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;the-initial-estimates&#34;&gt;The initial estimates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Let $\tilde{X}=X \circ G$ and $\tilde{X}_{i t}=X_{i t} g_{i t}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The common component $C^0 \equiv F^0 \Lambda^0$ is &lt;mark&gt;a low rank matrix&lt;/mark&gt; $\Rightarrow$ it is possible to recover $C^0$ even when a large proportion of elements in $X$ are missing at random.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Under the standard condition that $E\left(\varepsilon_{i t} \mid F_t^0, \lambda_i^0\right)=0$, we can verify that $E\left(\frac{1}{q} \tilde{X} \mid F^0, \Lambda^0\right)=F^0 \Lambda^{0 \prime}$ $\Rightarrow$ consider the following least squares objective function
$$
\mathcal{L}_{N T}^0(F, \Lambda) \equiv \frac{1}{N T} \operatorname{tr}\left[\left(\frac{1}{\tilde{q}} \tilde{X}-F \Lambda^{\prime}\right)\left(\frac{1}{\tilde{q}} \tilde{X}-F \Lambda^{\prime}\right)^{\prime}\right]
$$
&lt;strong&gt;identification restrictions&lt;/strong&gt;: $F^{\prime} F / T=I_R$ and $\Lambda^{\prime} \Lambda$ is a diagonal matrix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;By concentrating out $\Lambda$ and using the normalization that $F^{\prime} F / T=I_R$ $\Rightarrow$ identical to maximizing $\tilde{q}^{-2} \operatorname{tr}\Big\{F^{\prime} \tilde{X} \tilde{X}^{\prime} F\Big\}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;the-initial-estimates-1&#34;&gt;The initial estimates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The estimated factor matrix, denoted by $\hat{F}^{(0)}$ is $\sqrt{T}$ times the eigenvectors corresponding to the $R$ largest eigenvalues of the $T \times T$ matrix $\frac{1}{N T \tilde{q}^2} \tilde{X} \tilde{X}^{\prime}:$
$$
\frac{1}{N T \tilde{q}^2} \tilde{X} \tilde{X}^{\prime} \hat{F}^{(0)}=\hat{F}^{(0)} \hat{D}^{(0)},
$$
&lt;ul&gt;
&lt;li&gt;$\hat{D}^{(0)}$ is an $R \times R$ diagonal matrix consisting of the $R$ largest eigenvalues of $\left(N T \tilde{q}^2\right)^{-1} \tilde{X} \tilde{X}^{\prime}$, arranged in descending order along its diagonal line.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The estimator of $\Lambda^{\prime}$ is given by
$$
\hat{\Lambda}^{(0) \prime}=\frac{1}{\tilde{q}}\left(\hat{F}^{(0) \prime} \hat{F}^{(0)}\right)^{-1} \hat{F}^{(0) \prime} \tilde{X}=\frac{1}{T \tilde{q}} \hat{F}^{(0) \prime} \tilde{X} .
$$&lt;/li&gt;
&lt;li&gt;We can obtain an initial estimate of the $(t, i)$ th element, $C_{i t}^0$, of $C^0$ by $\hat{C}_{i t}^{(0)}=\hat{\lambda}_i^{(0)\prime} \hat{F}_t^{(0)}$.&lt;/li&gt;
&lt;li&gt;The initial estimators $\hat{F}_t^{(0)}, \hat{\lambda}_i^{(0)}$ and $\hat{C}_{i t}^{(0)}$ are consistent and follow mixture normal distributions under some standard conditions.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;the-iterated-estimates&#34;&gt;The iterated estimates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The initial estimators: consistency but &lt;mark&gt;not asymptotically efficient&lt;/mark&gt; $\Rightarrow$ &lt;strong&gt;iterative estimators&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;In step $\ell$, we can &lt;mark&gt; replace the missing values $\left(X_{i t}\right)$ in the matrix $X$ with the estimated common components $\hat{C}_{i t}^{(\ell-1)}$&lt;/mark&gt;. Define the $T \times N$ matrix $\hat{X}^{(\ell)}$ with its $(t, i)$ th element given by
$$
\hat{X}_{i t}^{(\ell)}=\left\{\begin{array}{ll}
X_{i t} &amp;amp; \text { if }(i, t) \in \Omega \\
\hat{C}_{i t}^{(\ell-1)} &amp;amp; \text { if }(i, t) \in \Omega_{\perp}
\end{array}, \ell \geq 1,\right.
$$
where $\Omega_{\perp}=\{(i, t) \in[N] \times[T]:(i, t) \notin \Omega\}$.&lt;/li&gt;
&lt;li&gt;Then we can conduct the PC analysis based on $\hat{X}^{(\ell)}$ and obtain $\hat{F}^{(\ell) \prime}$ and $\hat{\Lambda}^{(\ell)}$.&lt;/li&gt;
&lt;li&gt;We will study the asymptotic properties of $\hat{F}_t^{(\ell)}, \hat{\lambda}_i^{(\ell)}$ and $\hat{C}_{i t}^{\left(\ell^{\ell}\right)}, \ell=1,2, \ldots$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
~\\\
~\\\
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;asymptotic-properties-of-the-initial-estimators&#34;&gt;Asymptotic properties of the initial estimators&lt;/h2&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;strong&gt;Theorem 2.1.&lt;/strong&gt;&lt;/font&gt; Suppose some assumptions hold. Then
$$\frac{1}{T}\Big\|\hat{F}^{(0)}-F^0 \hat{H}^{(0)}\Big\|_F^2=O_P\left({\color{red}\delta_{N T}^{-2}}\right)$$
where $\delta_{N T}=\sqrt{N} \wedge \sqrt{T}$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{H}^{(0)}$ is defined as
$$
\hat{H}^{(0)}=\left(N^{-1} \Lambda^{0 \prime} \Lambda^0\right) T^{-1} F^{0 \prime} \hat{F}^{(0)}\left(\hat{D}^{(0)}\right)^{-1},
$$
where $\hat{D}^{(0)}$ is asymptotically nonsingular by Lemma A.1.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;asymptotic-distributions&#34;&gt;Asymptotic distributions&lt;/h2&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;strong&gt;Theorem 2.2.&lt;/strong&gt;&lt;/font&gt; Suppose some assumptions hold. Suppose that $\left(T^{1 / 2}+N^{1 / 2}\right) \delta_{N T}^{-2}=o(1)$. Let $\hat{\Pi}_{t N}^{(0)}=\sqrt{N}\Big(\hat{F}_t^{(0)}-\hat{H}^{(0) \prime} F_t^0\Big)$ and $\hat{\Pi}_{i T}^{(0)}=\sqrt{T}\Big(\hat{\lambda}_i^{(0)}-(\hat{H}^{(0)})^{-1} \lambda_i^0\Big)$. Then as $(N, T) \rightarrow \infty$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\hat{\Pi}_{t N}^{(0)}=\Big(\hat{D}^{(0)}\Big)^{-1} \frac{1}{T} \hat{F}^{(0) t} F^0 \frac{1}{\sqrt{N} q} \sum_{i=1}^N \lambda_i^0 \xi_{i t}+O_P\Big(N^{1 / 2} \delta_{N T}^{-2}\Big) \rightarrow N\Big(0, D^{-1} Q \Gamma_{g, t}(q) Q^{\prime} D^{-1}\Big) \mathcal{G}^t$-stably,&lt;/li&gt;
&lt;li&gt;$\hat{\Pi}_{i T}^{(0)}=\hat{H}^{(0) \prime} \frac{1}{\sqrt{T} q} \sum_{t=1}^T F_t^0 \xi_{i t}+O_P\left(T^{1 / 2} \delta_{N T}^{-2}\right) \rightarrow N\left(0,\left(Q^{\prime}\right)^{-1} \Phi_{g, i}(q) Q^{-1}\right) \mathcal{G}^i$-stably,&lt;/li&gt;
&lt;li&gt;$\Big(\frac{1}{N} \Sigma_{F, i t}^{(0)}(q)+\frac{1}{T} \Sigma_{\Lambda, i t}^{(0)}(q)\Big)^{-1 / 2}\Big(\hat{C}_{i t}^{(0)}-C_{i t}^0\Big) \stackrel{d}{\rightarrow} N(0,1)$,&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;asymptotic-properties-of-the-iterated-estimators&#34;&gt;Asymptotic properties of the iterated estimators&lt;/h2&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;strong&gt;Theorem 2.3.&lt;/strong&gt;&lt;/font&gt; Suppose some assumptions hold. Then
$$\frac{1}{T}\Big\|\hat{F}^{(\ell)}-F^0 \hat{H}^{(\ell)}\Big\|^2=O_P\left(\delta_{N T}^{-2}\right)$$
for each $\ell$, where $\hat{H}^{(\ell)}$ is defined as
$$
\hat{H}^{(\ell)}=\left(N^{-1} \Lambda^{0 \prime} \Lambda^0\right) T^{-1} F^{0 \prime} \hat{F}^{(\ell)}\Big(\hat{D}^{(\ell)}\Big)^{-1},
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;asymptotic-distributions-1&#34;&gt;Asymptotic distributions&lt;/h2&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;strong&gt;Theorem 2.4.&lt;/strong&gt;&lt;/font&gt; Suppose some assumptions hold. Suppose that &lt;mark&gt;$\sqrt{N}\big(T^{\gamma_1 / 4} \delta_{N T}^{-2} \ln T+T^{-1+3 \gamma_1 / 4}\big)=o(1)$&lt;/mark&gt; and &lt;mark&gt;$\sqrt{T}\big(N^{\gamma_2 / 4} \delta_{N T}^{-2} \ln N+N^{-1+3 \gamma_2 / 4}\big)=o(1)$&lt;/mark&gt;. Let $\hat{\Pi}_{t N}^{(\ell)}=\sqrt{N}\Big(\hat{F}_t^{(\ell)}-\hat{H}^{(\ell) \prime} F_t^0\Big)$ and $\hat{\Pi}_{i T}^{(\ell)}=\sqrt{T}\Big(\hat{\lambda}_i^{(\ell)}-\hat{H}^{(\ell)-1} \lambda_i^0\Big)$. Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\Pi}_{t N}^{(\ell)}=D^{-1} Q \frac{1}{\sqrt{N}} \sum_{i=1}^N \lambda_i^0 \varepsilon_{i t} g_{i t}+(1-q) \hat{\Pi}_{t N}^{(\ell-1)}+o_P(1)$ uniformly in $t$ and
$$
\hat{\Pi}_{t N}^{(\ell)} \stackrel{d}{\rightarrow} N\left(0, D^{-1} Q {\color{red}\Gamma_{1 g, t}(q)} Q^{\prime} D^{-1}\right) \text { as }(\ell, N, T) \rightarrow \infty
$$&lt;/li&gt;
&lt;li&gt;$\hat{\Pi}_{i T}^{(\ell)}=\left(Q^{\prime}\right)^{-1} \frac{1}{\sqrt{T}} \sum_{t=1}^T F_t^0 \varepsilon_{i t} g_{i t}+(1-q) \hat{\Pi}_{i T}^{(\ell-1)}+o_P(1)$ uniformly in $i$ and
$$
\hat{\Pi}_{i T} \stackrel{d}{\rightarrow} N\left(0,\left(Q^{\prime}\right)^{-1} {\color{red}\Phi_{1 g, i}(q)} Q^{-1}\right) \text { as }(\ell, N, T) \rightarrow \infty,
$$&lt;/li&gt;
&lt;li&gt;$\left(\frac{1}{N} \Sigma_{1 F, i t}+\frac{1}{T} \Sigma_{1 \Lambda, i t}\right)^{-1 / 2}\left(\hat{C}_{i t}^{(\ell)}-C_{i t}^0\right) \stackrel{d}{\rightarrow} N(0,1)$ as $(\ell, N, T) \rightarrow \infty$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;determining-the-number-of-factors-via-cv&#34;&gt;Determining the number of factors via CV&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Given the $T \times N$ matrix of observations $X$:
&lt;ul&gt;
&lt;li&gt;randomly sample elements in $X$ with a fixed probability $p \in(0,1)$&lt;/li&gt;
&lt;li&gt;leave the rest $(1-p)$-proportion of observations as held-out entries for the out-of-sample evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Let $\Omega^\star \subset[N] \times[T]$ be the index set of the training entries and $\Omega_{\perp}^\star$ the index set of the held-out entries.&lt;/li&gt;
&lt;li&gt;Define the operator $P_{\Omega^\star}: \mathbb{R}^{T \times N} \rightarrow \mathbb{R}^{T \times N}$ by
$$
\left(P_{\Omega^\star} X\right)_{t i}=X_{i t} g_{i t}^\star=X_{i t} \mathbf{1}\left\{(i, t) \in \Omega^\star\right\},
$$
where $g_{i t}^\star=\mathbf{1}\left\{(i, t) \in \Omega^\star\right\}$.&lt;/li&gt;
&lt;li&gt;Let $G^\star$ denote a $T \times N$ matrix with $(t, i)$ th element given by $g_{i t}^\star$.&lt;/li&gt;
&lt;li&gt;Now we can regard $P_{\Omega^\star} X$ as the $T \times N$ data matrix with missing values replaced by zeros.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;determining-the-number-of-factors-via-cv-1&#34;&gt;Determining the number of factors via CV&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Given $P_{\Omega^\star} X$, we apply the proposed EM algorithm to recover the data via estimating the common component matrix $C$ for any given number of factors.&lt;/li&gt;
&lt;li&gt;To proceed, we consider the full singular value decomposition (SVD) for $\frac{1}{p} P_{\Omega^\star} X$:
$$
\frac{1}{p} P_{\Omega^\star} X=\tilde{U} \tilde{\Sigma} \tilde{V}^{\prime}=\sum_{r=1}^{T \wedge N} \tilde{u}_r \tilde{v}_r^{\prime} \tilde{\sigma}_r,
$$
&lt;ul&gt;
&lt;li&gt;$\tilde{U}\in\mathbb{R}^{T \times T}=\left(\tilde{u}_1, \ldots, \tilde{u}_T\right)$&lt;/li&gt;
&lt;li&gt;$\tilde{V}\in\mathbb{R}^{N \times N}=\left(\tilde{v}_1, \ldots, \tilde{v}_N\right)$&lt;/li&gt;
&lt;li&gt;$\tilde{\Sigma}\in\mathbb{R}^{T \times N}$ is the diagonal matrix that contains the singular values, $\tilde{\sigma}_1, \tilde{\sigma}_2, \ldots, \tilde{\sigma}_{T \wedge N}$, arranged in descending order along the main diagonal line.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Given any $R \leq T \wedge N$ and the training entries in $P_{\Omega^\star} X$, we can estimate the common component $C$ by the singular value thresholding procedure:
$$
\tilde{C}_R=S_H\left(\frac{1}{p} P_{\Omega^\star} X, R\right)=\tilde{U}_R \tilde{\Sigma}_R \tilde{V}_R^{\prime}=\sum_{r=1}^R \tilde{u}_r \tilde{v}_r^{\prime} \tilde{\sigma}_r,
$$
where $S_H(\cdot, R)$ is the rank-R truncated SVD of $\cdot$, t&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;determining-the-number-of-factors-via-cv-2&#34;&gt;Determining the number of factors via CV&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let $\tilde{C}_{R, i t}$ denote the $(t, i)$ th element of $\tilde{C}_R$ for $R \geq 1$.&lt;/li&gt;
&lt;li&gt;We propose to choose $R$ to minimize the following CV criterion function
$$
{\color{red}\widetilde{C V}(R)=\frac{1}{N T} \sum_{(i, t) \in \Omega_{\perp}^\star}\left[X_{i t}-\tilde{C}_{R, i t}\right]^2.}
$$&lt;/li&gt;
&lt;li&gt;Let $\tilde{R}=\arg \min _{0 \leq R \leq R_{\max }} \widetilde{C V}(R)$ where $R_{\max }$ is a fixed integer that is no less than $R_0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
~\\\
~\\\
~\\\
~\\\
~\\\
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;a-more-efficient-method&#34;&gt;A more efficient method&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt; Can we use the $\ell$-step estimator $\hat C_{R,it}^{(\ell)}$ ?&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;Suppose that we have obtained the estimates $\hat{C}_{R, i t}^{(\ell-1)}$. In step $\ell$, we can replace the zero elements in $X^\star \equiv P_{\Omega^\star} X$ with the estimated common components $\hat{C}_{R_{\max }, i t}^{(\ell-1)}$&lt;/li&gt;
&lt;li&gt;Define the $T \times N$ matrix $\hat{X}^{\star(\ell)}$ with its $(t, i)$ th element given by
$$
\hat{X}_{i t}^{\star(\ell)}= \begin{cases}X_{i t} &amp;amp; \text { if }(i, t) \in \Omega^\star \\ \hat{C}_{R_{\max }^{(\ell-1)}}, &amp;amp; \text { if }(i, t) \in \Omega_{\perp}^\star, \ell \geq 1,\end{cases}
$$
where $\Omega_{\perp}^\star=\big\{(i, t) \in[N] \times[T]:(i, t) \notin \Omega^\star\big\}$&lt;/li&gt;
&lt;li&gt;Conduct the singular value thresholding procedure:
$$
\hat{C}_R^{(\ell)}=S_H\big(\hat{X}^{*(\ell)}, R\big)=\hat{U}_R^{(\ell)} \hat{\Sigma}_R^{(\ell)} \hat{V}_R^{(\ell) \prime},
$$&lt;/li&gt;
&lt;li&gt;repeating the above procedure for $\ell=1, \ldots, \ell^\star \equiv\left\lfloor\ln \left(\epsilon_{N T}\right) / \log (p)\right\rfloor$&lt;/li&gt;
&lt;li&gt;Let $\hat{C}_R=\hat{C}_R^{(\ell^\star)}$ and &lt;font color=&#34;blue&#34;&gt;$\hat{R}=\arg \min _{0 \leq R \leq R_{\max }} \widehat{C V}(R)$&lt;/font&gt;, where
$$
\widehat{C V}(R)=\frac{1}{N T} \sum_{(i, t) \in \Omega_{\perp}^\star}\left[X_{i t}-\hat{C}_{R, i t}\right]^2
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;the-consistency-of-the-cv-method&#34;&gt;The consistency of the CV method&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumption A.7.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For $r=R_{0+1}, \ldots, R_{\max }, \\ P\Big(\big\|\tilde{u}_r\big\|_{\infty}\big\|\tilde{v}_r\big\|_{\infty} \leq 1 /\big(c_0 \sqrt{(N+T) \log (N+T)}\big)\Big) \rightarrow 1$
for some fixed $c_0&amp;lt;\infty$ as $(N, T) \rightarrow \infty$, $\left\|\tilde{u}_r\right\|_{\infty}=o_P(1)$, and $\left\|\tilde{v}_r\right\|_{\infty}=o_P(1)$&lt;/li&gt;
&lt;li&gt;$\max _{(i, t) \in \Omega_{\perp}^\star} \sum_{(j, s) \in \Omega_{\perp}^\star}\left|E\left[\varepsilon_{i t} \varepsilon_{j s} \mid P_{\Omega^\star} X, \Omega^\star\right]\right|=o_P\left(\delta_{N T}^2\right)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt; &lt;strong&gt;Theorem 3.1.&lt;/strong&gt; &lt;/font&gt; Suppose some assumptions hold. Then $P\left(\tilde{R}&amp;lt;R_0\right) \rightarrow 0$ as $(N, T) \rightarrow \infty$. If Assumption A.7 also holds, then $P\left(\tilde{R}&amp;gt;R_0\right) \rightarrow 0$ as $(N, T) \rightarrow \infty$.&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt; &lt;strong&gt;Theorem 3.2.&lt;/strong&gt; &lt;/font&gt; Suppose some assumptions hold. Then $P\left(\hat{R}&amp;lt;R_0\right) \rightarrow 0$ as $(N, T) \rightarrow \infty$. If Assumption A.7 also holds, then $P\left(\hat{R}&amp;gt;R_0\right) \rightarrow 0$ as $(N, T) \rightarrow \infty$.&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;cv-in-the-presence-of-random-missing&#34;&gt;CV in the presence of random missing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Consider the SVD for $\frac{1}{p \tilde{q}} P_{\Omega^\star} P_{\Omega} X$ :
$$
\frac{1}{p \tilde{q}} P_{\Omega^\star} P_{\Omega} X=\tilde{U} \tilde{\Sigma} \tilde{V}^{\prime},
$$&lt;/li&gt;
&lt;li&gt;Then we estimate the common component $C$:
$$
\tilde{C}_R=S_H\left(\frac{1}{p \tilde{q}} P_{\Omega^\star} P_{\Omega} X, R\right)=\tilde{U}_R \tilde{\Sigma}_R \tilde{V}_R^{\prime},
$$
where $\tilde{U}_R, \tilde{V}_R$, and $\tilde{\Sigma}_R$ are defined as before. Let $\tilde{R} \in\big\{0,1,2, \ldots, R_{\max }\big\}$ minimize the following $\mathrm{CV}$ function
$$
\widetilde{C V}(R)=\frac{1}{N T} \sum_{(i, t) \in {\color{red}\Omega_{\perp}^\star \cap \Omega}}\left[X_{i t}-\tilde{C}_{R, i t}\right]^2,
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;simulation&#34;&gt;Simulation&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /slides/factormissing/res1_hud27e756e265fc0d2cb710f209c9d45db_107607_2f7cec5a10d7bfe6d412012a5ab9c0a5.webp 400w,
               /slides/factormissing/res1_hud27e756e265fc0d2cb710f209c9d45db_107607_05f2bf9c31e512fe355ea8d1ef4f5611.webp 760w,
               /slides/factormissing/res1_hud27e756e265fc0d2cb710f209c9d45db_107607_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/slides/factormissing/res1_hud27e756e265fc0d2cb710f209c9d45db_107607_2f7cec5a10d7bfe6d412012a5ab9c0a5.webp&#34;
               width=&#34;760&#34;
               height=&#34;341&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;simulation-1&#34;&gt;Simulation&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /slides/factormissing/res2_hu23b46a819b33dded3f66308db50b25de_133888_88b57d810d199bbfeed191044ef9817f.webp 400w,
               /slides/factormissing/res2_hu23b46a819b33dded3f66308db50b25de_133888_66fba2e54ad960eb84d5c21b2ad1b172.webp 760w,
               /slides/factormissing/res2_hu23b46a819b33dded3f66308db50b25de_133888_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/slides/factormissing/res2_hu23b46a819b33dded3f66308db50b25de_133888_88b57d810d199bbfeed191044ef9817f.webp&#34;
               width=&#34;760&#34;
               height=&#34;341&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;simulation-2&#34;&gt;Simulation&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /slides/factormissing/res3_hu7874055ca76ca648576350362febeb90_146809_05d4439d9d2a0dc25dcadca149a1e7c4.webp 400w,
               /slides/factormissing/res3_hu7874055ca76ca648576350362febeb90_146809_f351b6581254c888c8fc99a8d24e00bc.webp 760w,
               /slides/factormissing/res3_hu7874055ca76ca648576350362febeb90_146809_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/slides/factormissing/res3_hu7874055ca76ca648576350362febeb90_146809_05d4439d9d2a0dc25dcadca149a1e7c4.webp&#34;
               width=&#34;760&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;empirical-application-forecasting-macroeconomic-variables&#34;&gt;Empirical application: Forecasting macroeconomic variables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We use a panel dataset FRED-QD, which is an unbalanced panel at the quarterly frequency.&lt;/li&gt;
&lt;li&gt;The dataset consists of 248 quarterly U.S. indicators from 1959Q1 to 2018Q2.&lt;/li&gt;
&lt;li&gt;Use 125 time series to estimate the latent factors.&lt;/li&gt;
&lt;li&gt;Consider the forecast based on the following factor-augmented autoregression (FA-AR) models:
$$
y_{t+h}^h=\phi_h^{(1)}+\phi_h^{(2)}(L) \hat{F}_t+\phi_h^{(3)}(L) y_t+\varepsilon_{t+h}^h, h=1,2,4,
$$
&lt;ul&gt;
&lt;li&gt;$y_t$ is one of the four macro-variables (i.e., RGDP, GDP, IP, and RDPI)&lt;/li&gt;
&lt;li&gt;$\hat{F}_t$ is the estimated vector of factors&lt;/li&gt;
&lt;li&gt;$\phi_h^{(1)}$ is the intercept term, $L$ is the lag operator&lt;/li&gt;
&lt;li&gt;$\phi_h^{(2)}(L)$ and $\phi_h^{(3)}(L)$ are finite-order polynomials of the lag operators&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;empirical-application-forecasting-macroeconomic-variables-1&#34;&gt;Empirical application: Forecasting macroeconomic variables&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /slides/factormissing/res4_hub014dcadc3f19bd2f11bc127b0f473cb_90038_1f791bf979cecda35c8f819415e4ea57.webp 400w,
               /slides/factormissing/res4_hub014dcadc3f19bd2f11bc127b0f473cb_90038_9e51371ff9ad119475e615645fe18bd8.webp 760w,
               /slides/factormissing/res4_hub014dcadc3f19bd2f11bc127b0f473cb_90038_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/slides/factormissing/res4_hub014dcadc3f19bd2f11bc127b0f473cb_90038_1f791bf979cecda35c8f819415e4ea57.webp&#34;
               width=&#34;760&#34;
               height=&#34;234&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h1 id=&#34;thanks-&#34;&gt;Thanks !&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Notes on &#39;&#39;The three-pass regression filter: A new approach to forecasting using many predictors&#39;&#39;</title>
      <link>https://ikerlz.github.io/notes/3prf/</link>
      <pubDate>Mon, 20 Nov 2023 22:16:45 +0800</pubDate>
      <guid>https://ikerlz.github.io/notes/3prf/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-the-three-pass-regression-filter&#34;&gt;2. The three-pass regression filter&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#21-the-estimator&#34;&gt;2.1. The estimator&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#22-assumptions&#34;&gt;2.2. Assumptions&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#23-consistency&#34;&gt;2.3. Consistency&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#24-asymptotic-distributions&#34;&gt;2.4. Asymptotic distributions&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#25-proxy-selection&#34;&gt;2.5. Proxy selection&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-related-procedures&#34;&gt;3. Related procedures&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#31-constrained-least-squares&#34;&gt;3.1. Constrained least squares&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#32-partial-least-squares&#34;&gt;3.2. Partial least squares&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-empirical-evidence&#34;&gt;4. Empirical evidence&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#41-forecasting-macroeconomic-aggregates&#34;&gt;4.1. Forecasting macroeconomic aggregates&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#42-forecasting-market-returns&#34;&gt;4.2. Forecasting market returns&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;hr&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_4521b0390b02ac01acfcf8802e497a33.webp 400w,
               /media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_a6ae2965e4ec4df5bfad2235d8225cf9.webp 760w,
               /media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_4521b0390b02ac01acfcf8802e497a33.webp&#34;
               width=&#34;760&#34;
               height=&#34;296&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;How to use the vast predictive information to forecast important economic aggregates like national product or stock market value.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the predictors number near or more than the number of observations, the standard OLS forecaster is known to be poorly behaved or nonexistent (Huber, 1973)&lt;/li&gt;
&lt;li&gt;A economic view: data are generated from a model in which &lt;strong&gt;latent factors&lt;/strong&gt; drive the systematic variation of &lt;mark&gt;both the forecast target, $\boldsymbol{y}$, and the matrix of predictors, $\boldsymbol{X}$&lt;/mark&gt; $\Rightarrow$ &lt;font color=&#34;red&#34;&gt;the best prediction of $\boldsymbol{y}$ is infeasible&lt;/font&gt; since the factors are unobserved $\Rightarrow$ require a factor estimation step&lt;/li&gt;
&lt;li&gt;A benchmark method: extract factors that are significant drivers of variation in $\boldsymbol{X}$ and then uses these to forecast $\boldsymbol{y}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Motivations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the factors that are relevant to $\boldsymbol{y}$ may be a strict subset of all the factors driving $\boldsymbol{X}$&lt;/li&gt;
&lt;li&gt;The method, called the &lt;font color=&#34;red&#34;&gt;three-pass regression filter&lt;/font&gt; (3PRF), selectively identifies only the subset of factors that &lt;mark&gt;influence the forecast target&lt;/mark&gt; while discarding factors that &lt;mark&gt;are irrelevant for the target but that may be pervasive among predictors&lt;/mark&gt;&lt;/li&gt;
&lt;li&gt;The 3PRF has the advantage of being expressed in closed form and virtually instantaneous to compute.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Contributions&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;develop asymptotic theory for the 3PRF&lt;/li&gt;
&lt;li&gt;verify the finite sample accuracy of the asymptotic theory&lt;/li&gt;
&lt;li&gt;compare the 3PRF to other methods&lt;/li&gt;
&lt;li&gt;provide empirical support for the 3PRF&amp;rsquo;s strong forecasting performance&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-the-three-pass-regression-filter&#34;&gt;2. The three-pass regression filter&lt;/h2&gt;
&lt;h3 id=&#34;21-the-estimator&#34;&gt;2.1. The estimator&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;The environment for 3PRF&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;There is &lt;mark&gt;a target variable&lt;/mark&gt; which we wish to forecast.&lt;/li&gt;
&lt;li&gt;There exist many predictors which may contain information useful for predicting the target variable.
&lt;ul&gt;
&lt;li&gt;The number of predictors &lt;font color=&#34;red&#34;&gt;$N$ may be large and number near or more than the available time series observations $T$&lt;/font&gt;, which makes OLS problematic.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Therefore we look to reduce the dimension of predictive information $\Rightarrow$ &lt;font color=&#34;red&#34;&gt;assume the data can be described by an approximate factor model.&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;In order to make forecasts, the 3PRF uses &lt;strong&gt;proxies&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;These are variables, driven by the factors (and as we emphasize below, driven by target-relevant factors in particular), which we show are always available from
&lt;ul&gt;
&lt;li&gt;the target and predictors themselves&lt;/li&gt;
&lt;li&gt;the econometrician on the basis of economic theory.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The target is a linear function of a subset of the latent factors plus some unforecastable noise.&lt;/li&gt;
&lt;li&gt;The optimal forecast therefore comes from a regression on the true underlying relevant factors. However, since these factors are unobservable, we call this the &lt;font color=&#34;red&#34;&gt;infeasible best forecast&lt;/font&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;$$
\boldsymbol{y} = \boldsymbol{Z} \boldsymbol{\beta} + \boldsymbol{\epsilon}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol{y}\in\mathbb{R}^{T \times 1}$: the target variable time series from $2,3, \ldots, T+1$&lt;/li&gt;
&lt;li&gt;$\boldsymbol{X}\in\mathbb{R}^{T \times N}$: the predictors that have been standardized to have unit time series variance.
&lt;ul&gt;
&lt;li&gt;Temporal dimension: $\boldsymbol{X}=\left(\boldsymbol{x}_1^{\prime}, \boldsymbol{x}_2^{\prime}, \ldots, \boldsymbol{x}_T^{\prime}\right)^{\prime}$&lt;/li&gt;
&lt;li&gt;Cross section: $\boldsymbol{X}=\left(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\right)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\boldsymbol{Z}\in\mathbb{R}^{T \times L}$: stacks period-by-period &lt;strong&gt;proxy data&lt;/strong&gt; as $\boldsymbol{Z}=\left(\boldsymbol{z}_1^{\prime}, \boldsymbol{z}_2^{\prime}, \ldots, \boldsymbol{z}_T^{\prime}\right)^{\prime}$&lt;/li&gt;
&lt;li&gt;Make no assumption on the relationship between $N$ and $T$ but assume &lt;font color=&#34;red&#34;&gt;$L \ll \min (N, T)$&lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Construct the 3PRF&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Pass&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Run &lt;mark&gt;time series regression&lt;/mark&gt; of $\mathbf{x}_i$ on $\boldsymbol{Z}$ for $i=1, \ldots, N$,&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_{i, t}=\phi_{0, i}+\boldsymbol{z}^{\prime} \boldsymbol{\phi}_i+\epsilon_{i t}$, retain slope estimate $\hat{\boldsymbol{\phi}}_i$.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Run &lt;mark&gt;cross section regression&lt;/mark&gt; of $\boldsymbol{x}_t$ on $\hat{\boldsymbol{\phi}}_i$ for $t=1, \ldots, T$,&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_{i, t}=\phi_{0, t}+\hat{\boldsymbol{\phi}}^{\prime} \boldsymbol{F}_t+\varepsilon_{i t}$, retain slope estimate $\hat{\boldsymbol{F}}_t$.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3.&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Run &lt;mark&gt;time series regression&lt;/mark&gt; of $y_{t+1}$ on predictive factors $\hat{\boldsymbol{F}}_t$,&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y_{t+1}=\beta_0+\hat{\boldsymbol{F}}^{\prime} \boldsymbol{\beta}+\eta_{t+1}$, delivers forecast $\hat{y}_{t+1}$.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pass 1&lt;/strong&gt; : the estimated coefficients describe the sensitivity of the predictor to factors represented by the proxies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pass 2&lt;/strong&gt; : first-stage coefficient estimates map the cross-sectional distribution of predictors to the latent factors. Second-stage cross section regressions use this map to back out estimates of the factors at each point in time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pass 3&lt;/strong&gt; : This is a single time series forecasting regression of the target variable $y_{t +1}$ on the second-pass estimated predictive factors $\hat{\boldsymbol{F}}_t$.  The third-pass fitted value $\beta_0+\hat{\boldsymbol{F}_t}^{\prime} \boldsymbol{\beta}$ is the 3PRF time $t$ forecast&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;An one-step closed form:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
{\color{red}{\hat{\boldsymbol{y}}=\boldsymbol{\iota}_T \bar{y}+\boldsymbol{J}_T \boldsymbol{X} \boldsymbol{W}_{X Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{S}_{X X} \boldsymbol{W}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X y}}}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol{J}_T \equiv \boldsymbol{I}_T-\frac{1}{T} \boldsymbol{\iota}_T \boldsymbol{\iota}_T^{\prime}$
&lt;ul&gt;
&lt;li&gt;$\boldsymbol{I}_T$: the $T$-dimensional identity matrix&lt;/li&gt;
&lt;li&gt;$\boldsymbol{\iota}_T$: the $T$-vector of ones $\left(\boldsymbol{J}_N\right.$ is analogous)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\bar{y}=\boldsymbol{\iota}_T^{\prime} \boldsymbol{y} / T, \boldsymbol{W}_{X Z} \equiv$ $\boldsymbol{J}_{\boldsymbol{N}} \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{Z}, \boldsymbol{S}_{X X} \equiv \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{X}$ and $\boldsymbol{s}_{X \boldsymbol{y}} \equiv \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{y}$.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Two advantages:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, in practice (particularly with many predictors) one often faces unbalanced panels and missing data.&lt;/li&gt;
&lt;li&gt;Second, it is useful for developing intuition behind the procedure and for understanding its relation to partial least squares.&lt;/li&gt;
&lt;/ul&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Two interpretations&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;Rewrite the forecast as&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; \hat{\boldsymbol{y}}=\boldsymbol{\iota}_T \bar{y}+\hat{\boldsymbol{F}} \hat{\boldsymbol{\beta}} \\
&amp;amp; \hat{\boldsymbol{F}}^{\prime}=\boldsymbol{S}_{Z Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{X}^{\prime}, \\
&amp;amp; \hat{\boldsymbol{\beta}}=\boldsymbol{S}_{Z Z} \boldsymbol{W}_{X Z} \boldsymbol{s}_{X Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X X} \boldsymbol{W}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X y}
\end{aligned}
$$
where $\boldsymbol{S}_{X Z} \equiv \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{Z}$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\boldsymbol{F}}$ is the &lt;mark&gt;predictive factor&lt;/mark&gt; and $\hat{\boldsymbol{\beta}}$ is &lt;mark&gt;the predictive coefficient&lt;/mark&gt; on that factor.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;Also rewrite the forecast as&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; \hat{\boldsymbol{y}}=\boldsymbol{\iota} \bar{y}+\boldsymbol{J}_T \boldsymbol{X} \hat{\boldsymbol{\alpha}} \\
&amp;amp; \hat{\boldsymbol{\alpha}}=\boldsymbol{W}_{X Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X X} \boldsymbol{W}_{X Z}^{\prime}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X y}
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\alpha}$ is the predictive coefficient on individual predictors.&lt;/li&gt;
&lt;li&gt;The regular OLS estimate of the projection coefficient $\boldsymbol{\alpha}$ is $\left(\boldsymbol{S}_{X X}\right)^{-1} \boldsymbol{s}_{X y}$.&lt;/li&gt;
&lt;li&gt;This representation suggests that our approach can be interpreted as &lt;mark&gt;a constrained version of least squares&lt;/mark&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;h3 id=&#34;22-assumptions&#34;&gt;2.2. Assumptions&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Necessary assumptions&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Assumption 1 (Factor Structure)&lt;/strong&gt;&lt;/font&gt;. The data are generated by the following:
$$
\begin{array}{l}
\boldsymbol{x}_t=\boldsymbol{\phi}_0+\boldsymbol{\Phi} \boldsymbol{F}_t+\boldsymbol{\varepsilon}_t;\\
y_{t+1}=\beta_0+\boldsymbol{\beta}^{\prime} \boldsymbol{F}_t+\eta_{t+1};\\
\boldsymbol{z}_t=\boldsymbol{\lambda}_0+\boldsymbol{\Lambda} \boldsymbol{F}_t+\boldsymbol{\omega}_t \\
\boldsymbol{X}=\boldsymbol{\iota} \boldsymbol{\phi}_0^{\prime}+\boldsymbol{F} \boldsymbol{\Phi}^{\prime}+\boldsymbol{\varepsilon};\\
\boldsymbol{y}=\boldsymbol{\iota} \beta_0+\boldsymbol{F} \boldsymbol{\beta}+\boldsymbol{\eta};\\
\boldsymbol{Z}=\boldsymbol{\iota} \boldsymbol{\lambda}_0^{\prime}+\boldsymbol{F} \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\omega}
\end{array}
$$
where $\boldsymbol{F}_t=\left(\boldsymbol{f}_t^{\prime}, \mathbf{g}_t^{\prime}\right)^{\prime}, \boldsymbol{\Phi}=\left(\boldsymbol{\Phi}_f, \boldsymbol{\Phi}_g\right), \boldsymbol{\Lambda}=\left(\boldsymbol{\Lambda}_f, \boldsymbol{\Lambda}_g\right)$, and $\boldsymbol{\beta}=$ $\left(\boldsymbol{\beta}_f^{\prime}, \mathbf{0}^{\prime}\right)^{\prime}$ with $\left|\boldsymbol{\beta}_f\right|&amp;gt;$ 0. $K_f&amp;gt;0$ is the dimension of vector $\boldsymbol{f}_t$, $K_g \geq 0$ is the dimension of vector $g_t, L$ is the dimension of vector $z_t(0&amp;lt;L&amp;lt;\min (N, T))$, and $K=K_f+K_g$.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The target&amp;rsquo;s factor loadings $\big(\boldsymbol{\beta}=(\boldsymbol{\beta}_f^{\prime}, \mathbf{0}^{\prime})^{\prime}\big)$ allow the target to depend on a strict subset of the factors driving the predictors.&lt;/li&gt;
&lt;li&gt;We refer to this subset as the relevant factors, which are denoted $\boldsymbol{f}_t$.&lt;/li&gt;
&lt;li&gt;In contrast, irrelevant factors, $\mathbf{g}_t$, do not influence the forecast target but may drive the cross section of predictive information $\boldsymbol{x}_t$.&lt;/li&gt;
&lt;li&gt;The proxies $\boldsymbol{z}_t$ are driven by factors and proxy noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2 (Factors, Loadings and Residuals)&lt;/strong&gt;. Let $M&amp;lt;\infty$. For any $i, s, t$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\mathbb{E}\left|\boldsymbol{F}_t\right|^4&amp;lt;M, T^{-1} \sum_{s=1}^T \boldsymbol{F}_s \underset{T \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{\mu}$ and $T^{-1} \boldsymbol{F}^{\prime} \boldsymbol{J}_T \boldsymbol{F} \underset{T \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{\Delta}_F$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\left|\boldsymbol{\phi}_i\right|^4 \leq M, N^{-1} \sum_{j=1}^N \boldsymbol{\phi}_j \underset{T \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \overline{\boldsymbol{\phi}}, N^{-1} \boldsymbol{\Phi}^{\prime} \boldsymbol{J}_N \boldsymbol{\Phi} \underset{N \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \mathcal{P}$ and $N^{-1} \boldsymbol{\Phi}^{\prime} \boldsymbol{J}_N \boldsymbol{\phi}_0 \underset{N \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{P}_1^6$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\left(\varepsilon_{i t}\right)=0, \mathbb{E}\left|\varepsilon_{i t}\right|^8 \leq M$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\left(\boldsymbol{\omega}_t\right)=\mathbf{0}, \mathbb{E}\left|\boldsymbol{\omega}_t\right|^4 \leq M, T^{-1 / 2} \sum_{s=1}^T \boldsymbol{\omega}_s=\boldsymbol{o}_p(1)$ and $T^{-1} \boldsymbol{\omega}^{\prime} \mathbf{J}_T \boldsymbol{\omega} \underset{N \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{\Delta}_\omega$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}_t\left(\eta_{t+1}\right)=\mathbb{E}\left(\eta_{t+1} \mid y_t, F_t, y_{t-1}, F_{t-1}, \ldots\right)=0, \mathbb{E}\left(\eta_{t+1}^4\right) \leq M$, and $\eta_{t+1}$ is independent of $\phi_i(m)$ and $\varepsilon_{i, t}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3 (Dependence)&lt;/strong&gt;. Let $x(m)$ denote the $m$ th element of $\boldsymbol{x}$. For $M&amp;lt;\infty$ and any $i, j, t, s, m_1, m_2$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\mathbb{E}\left(\varepsilon_{i t} \varepsilon_{j s}\right)=\sigma_{i j, t s},\left|\sigma_{i j, t s}\right| \leq \bar{\sigma}_{i j}$ and $\left|\sigma_{i j, t s}\right| \leq \tau_{t s}$, and&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;$N^{-1} \sum_{i, j=1}^N \bar{\sigma}_{i j} \leq M$&lt;/li&gt;
&lt;li&gt;$T^{-1} \sum_{t, s=1}^T \tau_{t s} \leq M$&lt;/li&gt;
&lt;li&gt;$N^{-1} \sum_{i, s}\left|\sigma_{i i, t s}\right| \leq M$&lt;/li&gt;
&lt;li&gt;$N^{-1} T^{-1} \sum_{i, j, t, s}\left|\sigma_{i j, t s}\right| \leq M$&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;$\mathbb{E}\left|N^{-1 / 2} T^{-1 / 2} \sum_{s=1}^T \sum_{i=1}^N\left[\varepsilon_{i s} \varepsilon_{i t}-\mathbb{E}\left(\varepsilon_{i s} \varepsilon_{i t}\right)\right]\right|^2 \leq M$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\left|T^{-1 / 2} \sum_{t=1}^T F_t\left(m_1\right) \omega_t\left(m_2\right)\right|^2 \leq M$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\left|T^{-1 / 2} \sum_{t=1}^T \omega_t\left(m_1\right) \varepsilon_{i t}\right|^2 \leq M$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 4 (Central Limit Theorems)&lt;/strong&gt;. For any $i, t$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$N^{-1 / 2} \sum_{i=1}^N \phi_i \varepsilon_{i t} \stackrel{d}{\rightarrow} \mathcal{N}\left(0, \Gamma_{\Phi_{\varepsilon}}\right)$, where $\Gamma_{\Phi_{\varepsilon}}=\operatorname{plim}_{N \rightarrow \infty} N^{-1}$ $\sum_{i, j=1}^N \mathbb{E}\left[\boldsymbol{\phi}_i \boldsymbol{\phi}_j^{\prime} \varepsilon_{i t} \varepsilon_{j t}\right]$&lt;/li&gt;
&lt;li&gt;$T^{-1 / 2} \sum_{t=1}^T \boldsymbol{F}_t \eta_{t+1} \stackrel{d}{\rightarrow} \mathcal{N}\left(0, \Gamma_{F \eta}\right)$, where $\boldsymbol{\Gamma}_{F \eta}=\operatorname{plim}_{T \rightarrow \infty} T^{-1}$ $\sum_{t=1}^T \mathbb{E}\left[\eta_{t+1}^2 \boldsymbol{F}_t \boldsymbol{F}_t^{\prime}\right]&amp;gt;0$&lt;/li&gt;
&lt;li&gt;$T^{-1 / 2} \sum_{t=1}^T \boldsymbol{F}_t \varepsilon_{i t} \stackrel{d}{\rightarrow} \mathcal{N}\left(0, \boldsymbol{\Gamma}_{F \varepsilon, i}\right)$, where $\boldsymbol{\Gamma}_{F \varepsilon, i}=\operatorname{plim}_{T \rightarrow \infty} T^{-1}$ $\sum_{t, s=1}^T \mathbb{E}\left[\boldsymbol{F}_t \boldsymbol{F}_s^{\prime} \varepsilon_{i t} \varepsilon_{i s}\right]&amp;gt;0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 5 (Normalization)&lt;/strong&gt;. $\mathcal{P}=\mathbf{I}, \boldsymbol{P}_1=\mathbf{0}$ and $\boldsymbol{\Delta}_F$ is diagonal, positive definite, and each diagonal element is unique.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 6 (Relevant Proxies)&lt;/strong&gt;. $\boldsymbol{\Lambda}=\left[\boldsymbol{\Lambda}_f, \mathbf{0}\right]$ and $\boldsymbol{\Lambda}_f$ is nonsingular.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;h3 id=&#34;23-consistency&#34;&gt;2.3. Consistency&lt;/h3&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 1.&lt;/strong&gt;&lt;/font&gt; Let Assumptions 1-6 hold. The three-pass regression filter forecast is consistent for the infeasible best forecast,
$$\hat{y}_{t+1} \underset{T, N \rightarrow \infty}{\stackrel{p}{\longrightarrow}}\beta_0+\boldsymbol{F}_t^{\prime} \boldsymbol{\beta}.$$&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 2.&lt;/strong&gt;&lt;/font&gt; Let $\hat{\alpha}_i$ denote the $i$th element of $\hat{\boldsymbol{\alpha}}$, and let Assumptions 1-6 hold. Then for any $i$,
$$
N \hat{\alpha}_i \underset{T, N \rightarrow \infty}{\stackrel{p}{\longrightarrow}}\left(\boldsymbol{\phi}_i-\overline{\boldsymbol{\phi}}\right)^{\prime} \boldsymbol{\beta} .
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3PRF uses only as many predictive factors as the number of factors relevant to $y_{t+1}$&lt;/li&gt;
&lt;li&gt;the PCR forecast is asymptotically efficient when there are as many predictive factors as the total number of factors driving $\boldsymbol{x}_t$&lt;/li&gt;
&lt;li&gt;if the factors driving the target are weak in the sense that they contribute a only small fraction of the total variability in the predictors, &lt;strong&gt;then principal components may have difficulty identifying them&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;strong&gt;Corollary 1.&lt;/strong&gt;&lt;/font&gt; Let Assumptions 1–5 hold with the exception of Assumptions 2.4, 3.3 and 3.4. Additionally, assume that there is only one relevant factor. Then the target-proxy three-pass regression filter forecaster is consistent for the infeasible best forecast.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Corollary 1 holds regardless of the number of irrelevant factors driving $\boldsymbol{X}$ and regardless of where the relevant factor stands in the principal component ordering for $\boldsymbol{X}$.&lt;/li&gt;
&lt;li&gt;Compare this to PCR, whose first predictive factor is ensured to be the one that explains most of the covariance among $\boldsymbol{x}_t$, regardless of that factor&amp;rsquo;s relationship to $y_{t+1}$.&lt;/li&gt;
&lt;li&gt;Only if the relevant factor happens to also drive most of the variation within the predictors does the first component achieve the infeasible best.&lt;/li&gt;
&lt;li&gt;$\Rightarrow$ the forecast performance of the 3PRF is robust to the presence of irrelevant factors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;24-asymptotic-distributions&#34;&gt;2.4. Asymptotic distributions&lt;/h3&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 3.&lt;/strong&gt;&lt;/font&gt; Under Assumptions 1-6, as $N, T \rightarrow \infty$ we have
$$
\frac{\sqrt{T} N\left(\hat{\alpha}_i-\tilde{\alpha}_i\right)}{A_i} \stackrel{d}{\rightarrow} \mathcal{N}(0,1)
$$
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;where&lt;/summary&gt;
  &lt;p&gt;$A_i^2$ is the ith diagonal element of $\widehat{\operatorname{Avar}}(\hat{\boldsymbol{\alpha}})=\boldsymbol{\Omega}_\alpha\left(\frac{1}{T} \sum_t \hat{\eta}_{t+1}^2\left(\boldsymbol{X}_t\right.\right.$ $\left.-\overline{\boldsymbol{X}})\left(\boldsymbol{X}_t-\overline{\boldsymbol{X}}\right)^{\prime}\right) \boldsymbol{\Omega}_\alpha^{\prime}, \hat{\eta}_{t+1}$ is the estimated 3PRF forecast error, $\tilde{\alpha}_i \equiv$ $\boldsymbol{S}_i \boldsymbol{G}_\alpha \boldsymbol{\beta}$, where $\boldsymbol{S}_i$ is selects the ith element of vector $\boldsymbol{G}_\alpha \boldsymbol{\beta}$ and
$$
\begin{aligned}
\boldsymbol{G}_\alpha&amp;amp;=\boldsymbol{J}_N\left(T^{-1} \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{Z}\right)\left(T^{-3} N^{-2} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{S}_{X X} \boldsymbol{W}_{X Z}\right)^{-1}\times\\
&amp;amp;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\left(N^{-1} T^{-2} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{F}\right),
\end{aligned}
$$
and
$$
\boldsymbol{\Omega}_\alpha=\boldsymbol{J}_N\left(\frac{1}{T} \boldsymbol{S}_{X Z}\right)\left(\frac{1}{T^3 N^2} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{S}_{X X} \boldsymbol{W}_{X Z}\right)^{-1}\left(\frac{1}{T N} \boldsymbol{W}_{X Z}^{\prime}\right)
$$&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 4.&lt;/strong&gt;&lt;/font&gt; Under Assumptions $1-6$, as $N, T \rightarrow \infty$ we have
$$
\frac{\sqrt{T}\left(\hat{y}_{t+1}-\mathbb{E}_t y_{t+1}\right)}{Q_t} \stackrel{d}{\rightarrow} \mathcal{N}(0,1)
$$
where $\mathbb{E}_t y_{t+1}=\beta_0+\boldsymbol{\beta}^{\prime} \boldsymbol{F}_t$ and $Q_t^2$ is the th diagonal element of $\frac{1}{N^2} J_T \boldsymbol{X} \widehat{\operatorname{Avar}}(\hat{\boldsymbol{\alpha}}) \boldsymbol{X}^{\prime} \boldsymbol{J}_T$&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 5.&lt;/strong&gt;&lt;/font&gt; Under Assumptions $1-6$, as $N, T \rightarrow \infty$ we have
$$
\sqrt{T}\left(\hat{\boldsymbol{\beta}}-\boldsymbol{G}_\beta \boldsymbol{\beta}\right) \stackrel{d}{\rightarrow} \mathcal{N}\left(\mathbf{0}, \boldsymbol{\Sigma}_\beta\right)
$$
where $\boldsymbol{\Sigma}_\beta=\boldsymbol{\Sigma}_z^{-1} \boldsymbol{\Gamma}_{F \eta} \boldsymbol{\Sigma}_z^{-1}$ and $\boldsymbol{\Sigma}_z=\mathbf{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\Delta}_\omega$. Furthermore,
$$
\begin{aligned}
\widehat{\operatorname{Avar}}(\hat{\boldsymbol{\beta}})= &amp;amp; \left(T^{-1} \hat{\boldsymbol{F}}^{\prime} \boldsymbol{J}_T \hat{\boldsymbol{F}}\right)^{-1} T^{-1} \sum_t \hat{\eta}_{t+1}^2\left(\hat{\boldsymbol{F}}_t-\hat{\boldsymbol{\mu}}\right)\left(\hat{\boldsymbol{F}}_t-\hat{\boldsymbol{\mu}}\right)^{\prime} \
&amp;amp; \times\left(T^{-1} \hat{\boldsymbol{F}}^{\prime} \boldsymbol{J}_T \hat{\boldsymbol{F}}\right)^{-1}
\end{aligned}
$$
is a consistent estimator of $\boldsymbol{\Sigma}_\beta$. $\boldsymbol{G}_\beta$ is defined in the Appendix.&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 6.&lt;/strong&gt;&lt;/font&gt; Under Assumptions 1-6, as $N, T \rightarrow \infty$ we have for every $t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $\sqrt{N} / T \rightarrow 0$, then
$$
\sqrt{N}\left[\hat{\boldsymbol{F}}_t-\left(\boldsymbol{H}_0+\boldsymbol{H} \boldsymbol{F}_t\right)\right] \stackrel{d}{\rightarrow} \mathcal{N}\left(\mathbf{0}, \boldsymbol{\Sigma}_F\right)
$$&lt;/li&gt;
&lt;li&gt;if $\liminf \sqrt{N} / T \geq \tau \geq 0$, then
$$
T\left[\hat{\boldsymbol{F}}_t-\left(\boldsymbol{H}_0+\boldsymbol{H F}_t\right)\right]=\boldsymbol{O}_p(1)
$$
where $\boldsymbol{\Sigma}_F=\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\Delta}_\omega\right)\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F^2 \boldsymbol{\Lambda}^{\prime}\right)^{-1} \boldsymbol{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Gamma}_{\Phi \varepsilon} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F^2\right.$ $\left.\boldsymbol{\Lambda}^{\prime}\right)^{-1}\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\Delta}_\omega\right) . \boldsymbol{H}_0$ and $\boldsymbol{H}$ are defined in the Appendix.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;25-proxy-selection&#34;&gt;2.5. Proxy selection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;How to select the proxies that depend only on relevant factors ?&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;251-automatic-proxies&#34;&gt;2.5.1. Automatic proxies&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Initialize $\boldsymbol{r}_0=\boldsymbol{y}$.&lt;/li&gt;
&lt;li&gt;For $k=1, \ldots, L$ :
&lt;ul&gt;
&lt;li&gt;Define the $k$ th automatic proxy to be $\boldsymbol{r}_{k-1}$. Stop if $k=L$; otherwise proceed.&lt;/li&gt;
&lt;li&gt;Compute the 3PRF for target $\boldsymbol{y}$ using cross section $\boldsymbol{X}$ using statistical proxies 1 through $k$. Denote the resulting forecast $\hat{\boldsymbol{y}}_k$.&lt;/li&gt;
&lt;li&gt;Calculate $\boldsymbol{r}_k=\boldsymbol{y}-\hat{\boldsymbol{y}}_k$, advance $k$, and go to step 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 7.&lt;/strong&gt;&lt;/font&gt; Let Assumptions 1-5 hold with the exception of Assumptions 2.4, 3.3 and 3.4. Then the L-automatic-proxy three pass regression filter forecaster of $\boldsymbol{y}$ automatically satisfies Assumptions 2.4, 3.3, 3.4 and 6 when $L=K_f$. As a result, &lt;mark&gt;the $L$ automatic-proxy is consistent and asymptotically normal according to Theorems 1 and 4.&lt;/mark&gt;&lt;/p&gt;
&lt;h4 id=&#34;252-theory-proxies&#34;&gt;2.5.2. Theory proxies&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The filter may instead be employed using alternative disciplining variables (factor proxies) which may &lt;mark&gt;be distinct from the target and chosen on the basis of economic theory or by statistical arguments&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;Consider a situation in which $K_f$ is one, so that the target and proxy are given by $y_{t+1}=\beta_0+\beta f_t+\eta_{t+1}$ and $z_t=\lambda_0+\Lambda f_t+\omega_t$.&lt;/li&gt;
&lt;li&gt;Also suppose that the population $R^2$ of the proxy equation is substantially higher than the population $R^2$ of the target equation.
&lt;ul&gt;
&lt;li&gt;The forecasts from using either $z_t$ or the target as proxy are asymptotically identical.&lt;/li&gt;
&lt;li&gt;However, in finite samples, forecasts can be improved by proxying with $z_t$ due to its higher signal-to-noise ratio.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;252-information-criteria&#34;&gt;2.5.2 Information criteria&lt;/h4&gt;
&lt;p&gt;&amp;lsquo;&amp;lsquo;Trace of the Krylov Representation&amp;rsquo;&amp;rsquo; method of Kramer and Sugiyama (2011).&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; \widehat{\operatorname{DoF}}(m)=1+\sum_{j=1}^m c_j \operatorname{trace}\left(\boldsymbol{K}^j\right)-\sum_{l, j=1}^m \boldsymbol{t}_l^{\prime} \mathbf{K}^j \boldsymbol{t}_l \\
&amp;amp; ~~~~~~~~~~~~~~~~~~~~~~+\left(\boldsymbol{y}-\hat{\boldsymbol{y}}_m\right)^{\prime} \sum_{j=1}^m \boldsymbol{K}^j \boldsymbol{v}_j+m
\end{aligned}
$$
where $\boldsymbol{K}=\boldsymbol{X} \boldsymbol{X}^{\prime}, c_j$ are elements of the vector $\boldsymbol{c}=\boldsymbol{B}^{-1} \boldsymbol{T} \boldsymbol{y}, \boldsymbol{B}$ is a Krylov basis decomposition, $\boldsymbol{T}$ is the matrix of PLS factor estimate vectors $\boldsymbol{t}_j$, and $\boldsymbol{v}_j$ are columns of the matrix $\boldsymbol{T}\left(\boldsymbol{B}^{-1}\right)^{\prime}$. The BIC is then calculated as
$$\sum_t\left(y_t-\hat{y}_{m, t}\right)^2 / T+\log (T) \hat{\sigma}^2 \widehat{D o F}(m) / T$$
where $\hat{\sigma}=\sqrt{\sum_t\left(y_t-\hat{y}_{m, t}\right)^2 /(T-D o F(m))}$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-related-procedures&#34;&gt;3. Related procedures&lt;/h2&gt;
&lt;h3 id=&#34;31-constrained-least-squares&#34;&gt;3.1. Constrained least squares&lt;/h3&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 8.&lt;/strong&gt;&lt;/font&gt; The three-pass regression filter&amp;rsquo;s implied $N$-dimensional predictive coefficient, $\hat{\alpha}$, is the solution to
$$
\begin{aligned}
&amp;amp; \arg \min _{\alpha_0, \boldsymbol{\alpha}}\left\|\boldsymbol{y}-\alpha_0-\boldsymbol{X} \boldsymbol{\alpha}\right\| \\
&amp;amp; \text { subject to } \quad\left(\boldsymbol{I}-\boldsymbol{W}_{X Z}\left(\boldsymbol{S}_{X Z}^{\prime} \boldsymbol{W}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}\right) \boldsymbol{\alpha}=\mathbf{0} .\quad (5)
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The 3PRF&amp;rsquo;s answer is to impose the constraint in Eq. (5), which exploits the proxies and has an intuitive interpretation.&lt;/li&gt;
&lt;li&gt;Premultiplying both sides of the equation by $\boldsymbol{J}_T \boldsymbol{X}$, we can rewrite the constraint as $\left(\boldsymbol{J}_T \boldsymbol{X}-\boldsymbol{J}_T \hat{\boldsymbol{F}} \hat{\boldsymbol{\Phi}}^{\prime}\right) \boldsymbol{\alpha}=$ 0. For large $N$ and $T$,
$$
\boldsymbol{J}_T \boldsymbol{X}-\boldsymbol{J}_T \hat{\boldsymbol{F}} \hat{\boldsymbol{\Phi}}^{\prime} \approx \boldsymbol{\varepsilon}+(\boldsymbol{F}-\boldsymbol{\mu})\left(\boldsymbol{I}-\boldsymbol{S}_{K_f}\right) \boldsymbol{\Phi}^{\prime}
$$&lt;/li&gt;
&lt;li&gt;Because the covariance between $\boldsymbol{\alpha}$ and $\varepsilon$ is zero by the assumptions of the model, the constraint simply imposes that &lt;mark&gt;the product of $\alpha$ and the target-irrelevant common component of $\boldsymbol{X}$ is equal to zero&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;This is because the matrix $\boldsymbol{I}-\boldsymbol{S}_{K_f}$ selects only the terms in the total common component $\boldsymbol{F} \boldsymbol{\Phi}^{\prime}$ that &lt;mark&gt;are associated with irrelevant factors&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;This constraint is important because it ensures that &lt;mark&gt;factors irrelevant to $\boldsymbol{y}$ drop out of the 3PRF forecast&lt;/mark&gt;. It also ensures that $\hat{\boldsymbol{\alpha}}$ is consistent for the factor model&amp;rsquo;s population projection coefficient of $y_{t+1}$ on $\boldsymbol{x}_t$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;32-partial-least-squares&#34;&gt;3.2. Partial least squares&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-7&#34;&gt;
  &lt;summary&gt;PLS&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;See &lt;a href=&#34;https://aarmey.github.io/ml-for-bioe/public/Wk4-Lecture8.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reference1&lt;/a&gt; and &lt;a href=&#34;https://personal.utdallas.edu/~herve/Abdi-PLS-pretty.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reference2&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The goal of PLS regression is to predict $\mathbf{Y}$ from $\mathbf{X}$ and to &lt;mark&gt;describe their common structure&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;When $\mathbf{Y}$ is a vector and $\mathbf{X}$ is full rank, this goal could be accomplished using &lt;strong&gt;ORDINARY MULTIPLE REGRESSION&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;When the number of predictors is large compared to the number of observations, $\mathbf{X}$ is likely to be singular and the regression approach is no longer feasible (i.e., because of MULTICOLLINEARITY).&lt;/li&gt;
&lt;li&gt;Several approaches have been developed to cope with this problem.
&lt;ul&gt;
&lt;li&gt;One approach is to eliminate some predictors (e.g., using stepwise methods)&lt;/li&gt;
&lt;li&gt;Another one, called principal component regression (PCR): perform a PRINCIPAL COMPONENT ANALYSIS (PCA) of the $\mathbf{X}$ matrix and then use the principal components of $\mathbf{X}$ as regressors on $\mathbf{Y}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How to choose an optimum subset of predictors ? A possible strategy is to keep only a few of the first components.
&lt;ul&gt;
&lt;li&gt;They are chosen to explain $\mathbf{X}$ rather than $\mathbf{Y}$, and so, &lt;font color=&#34;red&#34;&gt;nothing guarantees that the principal components, which &amp;ldquo;explain&amp;rdquo; $\mathbf{X}$, are relevant for $\mathbf{Y}$.&lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;By contrast, PLS regression finds components from $\mathbf{X}$ that are also relevant for $\mathbf{Y}$.&lt;/li&gt;
&lt;li&gt;Specifically, PLS regression searches for a set of components (called latent vectors) that performs a simultaneous decomposition of $\mathbf{X}$ and $\mathbf{Y}$ with &lt;mark&gt;the constraint that these components explain as much as possible of the covariance between $\mathbf{X}$ and $\mathbf{Y}$&lt;/mark&gt;. This step generalizes PCA.&lt;/li&gt;
&lt;li&gt;It is followed by a regression step where the decomposition of $\mathbf{X}$ is used to predict $\mathbf{Y}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;partial least squares (PLS) constructs forecasting indices as linear combinations of the underlying predictors.&lt;/li&gt;
&lt;li&gt;These predictive indices are referred to as &amp;ldquo;directions&amp;rdquo; in the language of PLS.&lt;/li&gt;
&lt;li&gt;The PLS forecast based on the first $K$ PLS directions, $\hat{\boldsymbol{y}}^{(k)}$, is constructed according to the following algorithm (as stated in Hastie et al. (2009)):
&lt;ol&gt;
&lt;li&gt;Standardize each $\mathbf{x}_i$ to have mean zero and variance one by setting $\tilde{\mathbf{x}}_i=\frac{\mathbf{x}_i-\hat{\mathbb{E}}\left[\mathrm{x}_{i t}\right]}{\hat{\sigma}\left(\mathrm{x}_{i t}\right)}, i=1, \ldots, N$&lt;/li&gt;
&lt;li&gt;Set $\hat{\boldsymbol{y}}^{(0)}=\bar{y}$, and $\mathbf{x}_i^{(0)}=\tilde{\mathbf{x}}_i, i=1, \ldots, N$&lt;/li&gt;
&lt;li&gt;For $k=1,2, \ldots, K$&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol{u}_k=\sum_{i=1}^N \hat{\phi}_{k i} \mathbf{x}_i^{(k-1)}$, where $\hat{\phi}_{k i}=\widehat{\operatorname{Cov}}\left(\mathbf{x}_i^{(k-1)}, \boldsymbol{y}\right)$&lt;/li&gt;
&lt;li&gt;$\hat{\beta}_k=\widehat{\operatorname{Cov}}\left(\boldsymbol{u}_k, \boldsymbol{y}\right) / \widehat{\operatorname{Var}}\left(\boldsymbol{u}_k\right)$&lt;/li&gt;
&lt;li&gt;$\hat{\boldsymbol{y}}^{(k)}=\hat{\boldsymbol{y}}^{(k-1)}+\hat{\beta}_k \boldsymbol{u}_k$&lt;/li&gt;
&lt;li&gt;Orthogonalize each $\mathbf{x}_i^{(k-1)}$ with respect to $\boldsymbol{u}_k$ :
$$
\begin{aligned}
\mathbf{x}_i^{(k)} &amp;amp; =\mathbf{x}_i^{(k-1)}-\left(\widehat{\operatorname{Cov}}\left(\boldsymbol{u}_k, \mathbf{x}_i^{(k-1)}\right) / \widehat{\operatorname{Var}}\left(\boldsymbol{u}_k\right)\right) \boldsymbol{u}_k, \\
i &amp;amp; =1,2, \ldots, N .
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;ul&gt;
&lt;li&gt;partial least squares forecasts are identical to those from the 3PRF when
&lt;ol&gt;
&lt;li&gt;the predictors are demeaned and variance-standardized in a preliminary step&lt;/li&gt;
&lt;li&gt;the first two regression passes are run without constant terms&lt;/li&gt;
&lt;li&gt;proxies are automatically selected.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consider the case where a single predictive index is constructed from the partial least squares algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assume, for the time being, that each predictor has been previously standardized to have mean zero and variance one. Following the construction of the PLS forecast given above, we have&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Set $\hat{\phi}_i=x_i^{\prime} y$, and $\hat{\boldsymbol{\Phi}}=\left(\hat{\phi}_1, \ldots, \hat{\phi}_N\right)^{\prime}$.&lt;/li&gt;
&lt;li&gt;Set $\hat{u}_t=\boldsymbol{x}_t^{\prime} \hat{\Phi}$, and $\hat{\boldsymbol{u}}=\left(\hat{u}_1, \ldots, \hat{u}_T\right)^{\prime}$.&lt;/li&gt;
&lt;li&gt;Run a predictive regression of $\boldsymbol{y}$ on $\hat{\boldsymbol{u}}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Constructing the forecast in this manner may be represented as a one-step estimator
$$
\hat{\boldsymbol{y}}^{\mathrm{PLS}}=\boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{y}\left(\boldsymbol{y}^{\prime} \boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{y}\right)^{-1} \boldsymbol{y}^{\prime} \boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{y}
$$&lt;/li&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt;which upon inspection is identical to the 1-automatic-proxy 3PRF forecast when constants are omitted from the first and second passes. &lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;4-empirical-evidence&#34;&gt;4. Empirical evidence&lt;/h2&gt;
&lt;h3 id=&#34;41-forecasting-macroeconomic-aggregates&#34;&gt;4.1. Forecasting macroeconomic aggregates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Quarterly data from Stock and Watson (2012) for the sample 1959:I-2009:IV&lt;/li&gt;
&lt;li&gt;Take as our predictors a set of 108 macroeconomic variables compiled by Stock and Watson (2012)&lt;/li&gt;
&lt;li&gt;Out-of-sample $R^2$ of one quarter ahead forecasts, in percentage.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;3PRF1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PCR1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PCLAR&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PCLAS&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;10LAR&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;FA1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;GDP&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;30.12&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$35.18^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;29.70&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;29.51&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;26.38&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Consumption&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$23.20^{\mathrm{a},{ }^*}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;7.06&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.12&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;7.32&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-14.85&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Investment&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$38.88^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.37&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;36.81&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;36.30&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;24.01&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;34.35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Exports&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$16.75^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13.25&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-11.58&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-9.42&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-61.36&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Imports&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$37.18^a$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;36.50&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18.46&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16.93&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;22.77&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;36.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Industrial Production&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$16.56^{\mathrm{a},{ }^*}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;8.92&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5.67&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5.71&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;11.04&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;12.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Capacity Utilization&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;54.32&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;54.79&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;53.77&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;54.85&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$64.69^{a,{}^*}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;55.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Total Hours&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$53.81^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;50.47&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;48.58&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;47.39&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;39.56&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;42.53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Total Employment&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$48.84^a$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;47.27&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;38.14&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.16&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18.91&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;41.73&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Average Hours&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$20.12^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10.12&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18.52&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13.89&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17.55&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;15.84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Housing Starts&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;26.97&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-0.14&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;31.54&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;29.66&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$46.89^a$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;GDP Inflation&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.64&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.05&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-0.94&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.38&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-5.89&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$2.80^{\mathrm{a}}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PCE Inflation&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-1.29&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-3.73&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10.60&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.82&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$12.22^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-2.50&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;42-forecasting-market-returns&#34;&gt;4.2. Forecasting market returns&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Building from the present value identity, Kelly and Pruitt (2013) map the cross section of price–dividend ratios into the approximate latent factor model of Assumption 1, and argue that &lt;mark&gt;this set of predictors should possess forecasting power for log returns on the aggregate market.&lt;/mark&gt;&lt;/li&gt;
&lt;li&gt;We estimate the extent of market return predictability using 25 log price-dividend ratios of portfolios sorted by market equity and book-to-market ratio.&lt;/li&gt;
&lt;li&gt;The data is annual over the post-war period 1945-2010 (following Fama and French (1992)).&lt;/li&gt;
&lt;li&gt;We assume that the predictors take
&lt;ul&gt;
&lt;li&gt;the form $p d_{i, t}=\phi_{i, 0}+\boldsymbol{\phi}_i^{\prime} \boldsymbol{F}_t+\varepsilon_{i, t}$&lt;/li&gt;
&lt;li&gt;the target takes the form $r_{t+1}=\beta_0^r+\boldsymbol{F}_t^{\prime} \boldsymbol{\beta}^r+\eta_{t+1}^r$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;3PRF1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;3PRF2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;3PRF-IC&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PC1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Return $R^2$ &lt;br&gt; # of factors&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;27.63&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$36.34^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;31.15 &lt;br&gt; 1.36&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-10.45&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;PC2&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;PC-IC&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;10LAR&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;FA1&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Return $R^2$ &lt;br&gt; # of factors&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-8.89&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;27.00 &lt;br&gt; 4.58&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13.28&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-10.08&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Notes on &#39;&#39;On factor models with random missing: EM estimation, inference, and cross validation&#39;&#39;</title>
      <link>https://ikerlz.github.io/notes/factor-model/</link>
      <pubDate>Tue, 14 Nov 2023 22:16:45 +0800</pubDate>
      <guid>https://ikerlz.github.io/notes/factor-model/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#0-background&#34;&gt;0. Background&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-large-dimensional-factor-models-with-random-missing&#34;&gt;2. Large dimensional factor models with random missing&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#21-em-estimation&#34;&gt;2.1. EM estimation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-determining-the-number-of-factors-via-cross-validation&#34;&gt;3. Determining the number of factors via cross validation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-monte-carlo-simulations&#34;&gt;4. Monte Carlo simulations&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-empirical-application-forecasting-macroeconomic-variables&#34;&gt;5. Empirical application: Forecasting macroeconomic variables&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;hr&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/notes/factor-model/paper_intro_hu1fc2d9e7412cda710028560e51ff3652_53793_21d412abc8825e11623e78b70c39680f.webp 400w,
               /media/notes/factor-model/paper_intro_hu1fc2d9e7412cda710028560e51ff3652_53793_c705a68ddc94c1a3588600831f692041.webp 760w,
               /media/notes/factor-model/paper_intro_hu1fc2d9e7412cda710028560e51ff3652_53793_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/notes/factor-model/paper_intro_hu1fc2d9e7412cda710028560e51ff3652_53793_21d412abc8825e11623e78b70c39680f.webp&#34;
               width=&#34;670&#34;
               height=&#34;260&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;0-background&#34;&gt;0. Background&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Factor model in balanced panel has been thoroughly investigated.&lt;/li&gt;
&lt;li&gt;How to handle the missing data problem in factor models ?
&lt;ul&gt;
&lt;li&gt;the expectation–maximization (EM) algorithm&lt;/li&gt;
&lt;li&gt;the Kalman filter (KF)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;There is no formal study of the &lt;strong&gt;asymptotic properties&lt;/strong&gt; for the EM estimators of the factors and factor loadings for the PC estimation with &lt;mark&gt;missing observations&lt;/mark&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-large-dimensional-factor-models-with-random-missing&#34;&gt;2. Large dimensional factor models with random missing&lt;/h2&gt;
&lt;h3 id=&#34;21-em-estimation&#34;&gt;2.1. EM estimation&lt;/h3&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-determining-the-number-of-factors-via-cross-validation&#34;&gt;3. Determining the number of factors via cross validation&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;4-monte-carlo-simulations&#34;&gt;4. Monte Carlo simulations&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;5-empirical-application-forecasting-macroeconomic-variables&#34;&gt;5. Empirical application: Forecasting macroeconomic variables&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Notes on &#39;&#39;One-step Estimators and Pathwise Derivatives&#39;&#39;</title>
      <link>https://ikerlz.github.io/notes/path-dev/</link>
      <pubDate>Sun, 05 Nov 2023 22:16:45 +0800</pubDate>
      <guid>https://ikerlz.github.io/notes/path-dev/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;You can locate the original blog post on &lt;a href=&#34;https://observablehq.com/@herbps10/one-step-estimators-and-pathwise-derivatives&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;observablehq.com&lt;/a&gt;, written by &lt;strong&gt;Herb Susmann&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-functional&#34;&gt;1. Functional&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-pathwise-derivatives&#34;&gt;2. Pathwise Derivatives&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-one-step-estimator&#34;&gt;3. One-step Estimator&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-pathwise-differentiability&#34;&gt;4. Pathwise differentiability&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-influence-function&#34;&gt;5. Influence function&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#51-the-gâteaux-derivative&#34;&gt;5.1 The Gâteaux derivative&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#52-the-influence-function&#34;&gt;5.2 The influence function&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;hr&gt;
&lt;h2 id=&#34;1-functional&#34;&gt;1. Functional&lt;/h2&gt;
&lt;p&gt;A functional (you could also call it a parameter) is &lt;strong&gt;any function&lt;/strong&gt; that takes in a probability distribution and returns a number (or multiple numbers). For example, the mean is a functional: given a probability distribution, you compute &lt;mark&gt;$\mathbb{E}(X)=\int x p(x) d x$&lt;/mark&gt; and get back a number.
Mathematically, we can define the functional as $T(P):\mathscr{P} \rightarrow \mathbb{R}^q$, where ${\color{red}\mathscr{P}=\{P\}}$ is the family of the distributions.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;A toy example&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;$$
T(P)=\int p(x)^2 d x\qquad(\star)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This functional doesn&amp;rsquo;t usually have any practical significance&lt;/li&gt;
&lt;li&gt;Other functionals might be
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;average treatment effect&lt;/strong&gt; in causal inference&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;probability of survival&lt;/strong&gt; after a certain time point in survival analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;For the true data generating distribution $P$ (here, we can use $N(0,1)$), the value of this squared density parameter is $T(P)=0.282$.
However, we don&amp;rsquo;t know the true data generating distribution and we need to estimate it. Once we have an estimate of the distribution, we can &amp;ldquo;plug it in&amp;rdquo; to the formula for the functional to get an estimate for the parameter.&lt;/p&gt;
&lt;p&gt;Take 100 samples from the true data generating distribution and use a &lt;strong&gt;kernel density estimator&lt;/strong&gt; to obtain the  estimated distribution $\tilde{P}$, with a corresponding density $\tilde{p}$. The following &lt;a href=&#34;#figure-est_diff&#34;&gt;FIGURE&lt;/a&gt; shows how the estimated $\tilde{P}$ compares to the truth, $P$.&lt;/p&gt;
&lt;figure  id=&#34;figure-est_diff&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The difference between $\tilde{P}$ and the truth $P$&#34; srcset=&#34;
               /media/posts/path-dev/toy_dist_est_hu6b57dde35fb621b2e402235d9ae3ac15_48832_7a60d827413f043c5240737d8c770d91.webp 400w,
               /media/posts/path-dev/toy_dist_est_hu6b57dde35fb621b2e402235d9ae3ac15_48832_cb656030e87868ac2edbf9006a565a23.webp 760w,
               /media/posts/path-dev/toy_dist_est_hu6b57dde35fb621b2e402235d9ae3ac15_48832_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/path-dev/toy_dist_est_hu6b57dde35fb621b2e402235d9ae3ac15_48832_7a60d827413f043c5240737d8c770d91.webp&#34;
               width=&#34;760&#34;
               height=&#34;227&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      The difference between $\tilde{P}$ and the truth $P$
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The estimate of the functional $(\star)$ using this estimated distribution is $T(\tilde P)=0.282$ (off by 1.5% compared to the true value).&lt;/p&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;strong&gt;How would our estimate of the functional change if we could nudge our estimated distribution towards the true distribution ?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We define a family of distributions $P_\epsilon$ which are mixtures of the estimated distribution and the true distribution. The density of $p_\epsilon$ is given by
$$
p_\epsilon=(1-\epsilon) p+\epsilon \tilde{p}
$$
where $\epsilon\in[0,1]$. For every value of $\epsilon$ we can compute a corresponding value of the functional: $T\left(P_\epsilon\right)$. These values trace out a trajectory as they move from the initual guess, $T(\tilde{P})$, to the true value, $T(P)$, as shown in &lt;a href=&#34;#figure-tp_vary_eps&#34;&gt;FIGURE&lt;/a&gt; below.&lt;/p&gt;


















&lt;figure  id=&#34;figure-tp_vary_eps&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The difference between $\tilde{P}$ and the truth $P$&#34; srcset=&#34;
               /media/posts/path-dev/tp_vary_eps_hu645d32e7b6c2b6ffa7e1d579f7af0927_23355_81adf42d1864c024aaccc18ab5094087.webp 400w,
               /media/posts/path-dev/tp_vary_eps_hu645d32e7b6c2b6ffa7e1d579f7af0927_23355_1df5b8d059878a83c8bd590c38f0a9ba.webp 760w,
               /media/posts/path-dev/tp_vary_eps_hu645d32e7b6c2b6ffa7e1d579f7af0927_23355_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/path-dev/tp_vary_eps_hu645d32e7b6c2b6ffa7e1d579f7af0927_23355_81adf42d1864c024aaccc18ab5094087.webp&#34;
               width=&#34;564&#34;
               height=&#34;342&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      The difference between $\tilde{P}$ and the truth $P$
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;2-pathwise-derivatives&#34;&gt;2. Pathwise Derivatives&lt;/h2&gt;
&lt;p&gt;A &lt;mark&gt;pathwise derivative&lt;/mark&gt; is &lt;font color=&#34;red&#34;&gt;the derivative of $T\left(P_\epsilon\right)$ with respect to $\epsilon$&lt;/font&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It&amp;rsquo;s useful to think of a derivative as a direction: for each value of $\epsilon$, the pathwise derivative tells us which direction the functional $T\left(P_\epsilon\right)$ is moving.&lt;/p&gt;
&lt;/blockquote&gt;


















&lt;figure  id=&#34;figure-tp_pathwise&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The pathwise derivative at $\epsilon=1$&#34; srcset=&#34;
               /media/posts/path-dev/tp_pathwise_hucf546d1153bd73be506196aa75dac6e3_23792_1f63684b021986fdd75ad0d58ad77dce.webp 400w,
               /media/posts/path-dev/tp_pathwise_hucf546d1153bd73be506196aa75dac6e3_23792_9338c322d3fc09c44791e6cfda45d745.webp 760w,
               /media/posts/path-dev/tp_pathwise_hucf546d1153bd73be506196aa75dac6e3_23792_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/path-dev/tp_pathwise_hucf546d1153bd73be506196aa75dac6e3_23792_1f63684b021986fdd75ad0d58ad77dce.webp&#34;
               width=&#34;564&#34;
               height=&#34;342&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      The pathwise derivative at $\epsilon=1$
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;When $\epsilon=1, \tilde{P}$ is the estimated distribution. The pathwise derivative at that point tells us how our estimate of $T(\tilde{P})$ would change if we nudged our estimate in the right direction, towards the true distribution.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-one-step-estimator&#34;&gt;3. One-step Estimator&lt;/h2&gt;
&lt;p&gt;We can use this to form a better estimate of the functional by using this pathwise derivative to approximate the trajectory as &lt;strong&gt;a linear function&lt;/strong&gt; (shown in the following &lt;a href=&#34;#figure-tp_onestep&#34;&gt;FIGURE&lt;/a&gt;). This is called a &lt;mark&gt;&amp;ldquo;one-step&amp;rdquo; estimator&lt;/mark&gt;, because it&amp;rsquo;s like performing a single step of Newton&amp;rsquo;s method.&lt;/p&gt;


















&lt;figure  id=&#34;figure-tp_onestep&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The one-step estimate&#34; srcset=&#34;
               /media/posts/path-dev/tp_onestep_hu546b4e2afde3a6ef21b949680015f560_27062_e2ce109324cc8f37c662df42b0d70b9b.webp 400w,
               /media/posts/path-dev/tp_onestep_hu546b4e2afde3a6ef21b949680015f560_27062_03de0ad093fa116efaceff309222b7e4.webp 760w,
               /media/posts/path-dev/tp_onestep_hu546b4e2afde3a6ef21b949680015f560_27062_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/path-dev/tp_onestep_hu546b4e2afde3a6ef21b949680015f560_27062_e2ce109324cc8f37c662df42b0d70b9b.webp&#34;
               width=&#34;564&#34;
               height=&#34;342&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      The one-step estimate
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    In any practical setting, we would need to use the observed data to estimate the pathwise derivative.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    One-step estimators don&amp;rsquo;t respect any bounds on the target functional, which can lead to nonsensical results. An alternative family of estimators, &lt;strong&gt;Targeted Maximum Likelihood Estimators&lt;/strong&gt; (TMLE), provide one way around this problem
  &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;4-pathwise-differentiability&#34;&gt;4. Pathwise differentiability&lt;/h2&gt;
&lt;p&gt;A functional is called &lt;strong&gt;pathwise differentiable&lt;/strong&gt; if it&amp;rsquo;s possible to compute it&amp;rsquo;s pathwise derivative.&lt;/p&gt;
&lt;p&gt;This is a desirable property:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it allows for the creation of one-step estimators&lt;/li&gt;
&lt;li&gt;it also allows for other techniques like Targeted Maximum Likelihood Estimation (TMLE).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fortunately, many parameters of real-world interest are pathwise differentiable: for example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;the average treatment effect&lt;/strong&gt; for a binary intervention is a pathwise differentiable parameter.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;5-influence-function&#34;&gt;5. Influence function&lt;/h2&gt;
&lt;p&gt;The term &lt;strong&gt;influence function&lt;/strong&gt; comes from its origins in robust statistics, where it was developed to quantify &lt;font color=&#34;red&#34;&gt;how much an estimator will change when its input changes&lt;/font&gt; (&lt;mark&gt;in other words, how much influence each input data point has on the outcome&lt;/mark&gt;). In robust statistics, they&amp;rsquo;re mostly concerned with what happens when your data is perturbed in a bad direction, so they can understand how things like outliers will impact an estimator.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;In our case, we are more interested in how estimators change when our estimates are nudged in the right direction, towards the true data distribution.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Notes of &lt;a href=&#34;chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://myweb.uiowa.edu/pbreheny/uk/teaching/621/notes/8-28.pdf&#34;&gt;Statistical functionals and influence functions&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Is the plug-in estimator $T(\tilde F)$ a good estimator?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Glivenko-Cantelli Theorem says that $\hat{F} \stackrel{\text { a.s. }}{\longrightarrow} F$; does this mean that $T(\hat{F}) \stackrel{\text { a.s. }}{\longrightarrow} T(F)$ ?&lt;/li&gt;
&lt;li&gt;The answer turns out to be a complicated &amp;ldquo;sometimes&amp;rdquo;; often &amp;ldquo;yes&amp;rdquo;, but not always


















&lt;figure  id=&#34;figure-tf_density_example&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The difference between $\tilde{P}$ and the truth $P$&#34; srcset=&#34;
               /media/posts/path-dev/TF_density_example_huba61f51cfb0925473f4e859d3bd8a8e8_18949_9244cdc563a13689237c9296df953988.webp 400w,
               /media/posts/path-dev/TF_density_example_huba61f51cfb0925473f4e859d3bd8a8e8_18949_3c869501e1da317d601867c331ff7e0c.webp 760w,
               /media/posts/path-dev/TF_density_example_huba61f51cfb0925473f4e859d3bd8a8e8_18949_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/path-dev/TF_density_example_huba61f51cfb0925473f4e859d3bd8a8e8_18949_9244cdc563a13689237c9296df953988.webp&#34;
               width=&#34;659&#34;
               height=&#34;259&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      The difference between $\tilde{P}$ and the truth $P$
    &lt;/figcaption&gt;&lt;/figure&gt;

When is the plug-in estimator consistent $\Rightarrow$ requires certain conditions on the smoothness
(differentiability) of $T(F)$ $\Rightarrow$ &lt;font color=&#34;red&#34;&gt;What does it mean to take the derivative of a function w.r.t. a function&lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;51-the-gâteaux-derivative&#34;&gt;5.1 The Gâteaux derivative&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Gâteaux derivative of $T$ at $F$ in the direction $G$ is defined by
$$
L_F(T ; G)=\lim _{\epsilon \rightarrow 0}\left[\frac{T\{(1-\epsilon) F+\epsilon G\}-T(F)}{\epsilon}\right]
$$&lt;/li&gt;
&lt;li&gt;An equivalent way of stating the definition is to define $D=G-F$, and the above becomes
$$
L_F(T ; D)=\lim _{\epsilon \rightarrow 0}\left[\frac{T\{F+\epsilon D\}-T(F)}{\epsilon}\right]
$$&lt;/li&gt;
&lt;li&gt;Either way, the definition boils down to
$$
L_F(T)=\lim_{\epsilon \rightarrow 0}\left[\frac{T\left(F_\epsilon\right)-T(F)}{\epsilon}\right]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;From a &lt;strong&gt;mathematical perspective&lt;/strong&gt;, the Gâteaux derivative a generalization of the concept of a directional derivative to functional analysis&lt;/li&gt;
&lt;li&gt;From a &lt;strong&gt;statistical perspective&lt;/strong&gt;, it represents the rate of change in a statistical functional upon a small amount of contamination by another distribution $G$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;An example: how the Gâteaux derivative work&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;Suppose $F$ is a continuous CDF, and $G$ is the distribution that places all of its mass at the point $x_0$. What happens to the Gâteaux derivative of $T(F)=f\left(x_0\right)$ ?
$$
\begin{aligned}
L_F(T ; G) &amp;amp; =\lim_{\epsilon \rightarrow 0}\left[\frac{\frac{d}{d x}{(1-\epsilon) F(x)+\epsilon G(x)}_{x=x_0}-\left.\frac{d}{d x} F(x)\right|_{x=x_0}}{\epsilon}\right] \\
&amp;amp; =\lim_{\epsilon \rightarrow 0}\left[\frac{(1-\epsilon) f\left(x_0\right)+\epsilon g\left(x_0\right)-f(x_0)}{\epsilon}\right] \\
&amp;amp; =\infty
\end{aligned}
$$
&lt;strong&gt;Conclusions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Even though $F$ and $F_\epsilon$ differ from each other only infinitesimally $T(F)$ and $T\left(F_\epsilon\right)$ differ from each other by an infinite amount&lt;/li&gt;
&lt;li&gt;The Glivenko-Cantelli theorem does not help us here: &lt;mark&gt;$\sup _x|\hat{F}(x)-F(x)|$ may go to zero without $T(\hat{F}) \rightarrow T(F)$&lt;/mark&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h4 id=&#34;511-hadamard-differentiability&#34;&gt;5.1.1 Hadamard differentiability&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Gâteaux differentiability is &lt;strong&gt;too weak&lt;/strong&gt; to ensure that $T(\hat{F}) \rightarrow T(F)$&lt;/li&gt;
&lt;li&gt;Even if the Gâteaux derivative exists, it may not exist in an entirely &lt;font color=&#34;red&#34;&gt;unique&lt;/font&gt; way, and this is the subtle idea introduced by Hadamard differentiability&lt;/li&gt;
&lt;li&gt;A functional $T$ is &lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Hadamard differentiable&lt;/strong&gt;&lt;/font&gt; if, for any sequence $\epsilon_n \rightarrow 0$ and $D_n$ satisfying &lt;mark&gt;$\sup _x\left|D_n(x)-D(x)\right| \rightarrow 0$&lt;/mark&gt;, we have
$$
\frac{T\left(F+\epsilon_n D_n\right)-T(F)}{\epsilon_n} \rightarrow L_F(T ; D)
$$&lt;/li&gt;
&lt;li&gt;If $T$ is Hadamard differentiable, then $T(\hat{F}) \stackrel{\mathrm{P}}{\longrightarrow} T(F)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;512-bounded-functionals&#34;&gt;5.1.2 Bounded functionals&lt;/h4&gt;
&lt;p&gt;Another useful condition: if the functional is bounded, then the plug-in estimate will converge to the true value&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition:&lt;/strong&gt; Suppose that there exists a constant $C$ such that the following relation holds for all $G$ :
$$
|T(F)-T(G)| \leq C \sup _x|F(x)-G(x)|.
$$
Show that $T(\hat{F}) \stackrel{\text { a.s. }}{\longrightarrow} T(F)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;52-the-influence-function&#34;&gt;5.2 The influence function&lt;/h3&gt;
&lt;h4 id=&#34;521-contamination-by-a-point-mass&#34;&gt;5.2.1 Contamination by a point mass&lt;/h4&gt;
&lt;p&gt;The idea of contaminating a distribution with a small amount of additional data has a long history in statistics and the investigation of robust estimators. Statisticians usually do not work with the general Gâteaux derivative, but a special case of it called the &lt;font color=&#34;red&#34;&gt;&lt;strong&gt;influence function&lt;/strong&gt;&lt;/font&gt;, in which $G$ places a point mass of 1 at $x$ :
$$
\delta_x(u)= \begin{cases}0 &amp;amp; \text { if } u&amp;lt;x \\ 1 &amp;amp; \text { if } u \geq x\end{cases}
$$&lt;/p&gt;
&lt;h4 id=&#34;522-influence-function--empirical-influence-function&#34;&gt;5.2.2 Influence function &amp;amp; empirical influence function&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The influence function is usually written as a function of $x$, and defined as
$$
L(x)=\lim_{\epsilon \rightarrow 0}\left[\frac{T\left\{(1-\epsilon) F+\epsilon \delta_x\right\}-T(F)}{\epsilon}\right]
$$&lt;/li&gt;
&lt;li&gt;A closely related concept is that of the empirical influence function:
$$
\hat{L}(x)=\lim _{\epsilon \rightarrow 0}\left[\frac{T\left\{(1-\epsilon) \hat{F}+\epsilon \delta_x\right\}-T(\hat{F})}{\epsilon}\right]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: if $T(F)=\mu$, then we have $L(x)=x-\mu$ and $\hat L(x)=x-\bar x$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;523-linear-functionals&#34;&gt;5.2.3 Linear functionals&lt;/h4&gt;
&lt;p&gt;A functional $T(F)$ is a linear functional iff $T(F)=\int a(x) d F(x)$. For the linear functionals, we have
$$
\begin{aligned}
&amp;amp; L(x)=a(x)-T(F) \\
&amp;amp; \hat{L}(x)=a(x)-T(\hat{F})
\end{aligned}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: if $T(F)=\sigma^2$, then we have $L(x)=(x-\mu)^2-\sigma^2$ and $\hat L(x)=(x-\bar x)^2-\hat\sigma^2$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Visually Communicating and Teaching Intuition for Influence Functions &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/00031305.2020.1717620&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Fisher &amp;amp; Kennedy 2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Semiparametric Theory &lt;a href=&#34;https://arxiv.org/abs/1709.06418&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Kennedy 2017)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tutorial: Deriving The Efficient Influence Curve For Large Models &lt;a href=&#34;chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1903.01706.pdf&#34;&gt;(Levy 2019)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Notes on &#39;&#39;The Asymptotic Variance of Semiparametric Estimators&#39;&#39;</title>
      <link>https://ikerlz.github.io/notes/var-semiparam/</link>
      <pubDate>Sun, 05 Nov 2023 22:16:45 +0800</pubDate>
      <guid>https://ikerlz.github.io/notes/var-semiparam/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#0-background-semiparametric-model&#34;&gt;0. Background: Semiparametric Model&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-the-pathwise-derivative-formula-for-the-asymptotic-variance&#34;&gt;2. The Pathwise Derivative Formula For the Asymptotic Variance&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#regular-path--regular-estimator&#34;&gt;Regular path &amp;amp; regular estimator&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-pathwise-derivative-asymptotic-variance-formula&#34;&gt;The pathwise derivative asymptotic variance formula&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-semiparametric-m-estimators&#34;&gt;3. Semiparametric $M$-estimators&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#when-will-the-adjustment-term-be-zero-&#34;&gt;When will the adjustment term be zero ?&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#a-more-direct-condition&#34;&gt;A more direct condition&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-functions-of-mean-square-projection-and-densities&#34;&gt;4. Functions of Mean-square Projection and Densities&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-regularity-conditions&#34;&gt;5. Regularity Conditions&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-series-estimation-of-projection-functionals&#34;&gt;6. Series Estimation of Projection Functionals&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-power-series-estimators-for-semiparametric-individual-effects-and-average-derivatives&#34;&gt;7. Power Series Estimators For Semiparametric Individual Effects and Average Derivatives&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;hr&gt;
&lt;h2 id=&#34;0-background-semiparametric-model&#34;&gt;0. Background: Semiparametric Model&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;: &lt;a href=&#34;http://www.stat.columbia.edu/~bodhi/Talks/SPThNotes.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lecture notes&lt;/a&gt; from Prof. Bodhisattva Sen (Columbia University)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A semiparametric model is a statistical model that involves both &lt;strong&gt;parametric&lt;/strong&gt; and &lt;strong&gt;nonparametric&lt;/strong&gt; (infinite-dimensional) components. However, we are mostly interested in estimation and inference of a &lt;mark&gt;finite-dimensional parameter&lt;/mark&gt; in the model.&lt;/p&gt;
&lt;hr&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Example 1 (population mean)&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;Suppose that $X_1, \ldots, X_n$ are i.i.d. $P$ belonging to the class $\mathcal{P}$ of distribution. Let $\psi(P) \equiv \mathbb{E}_P\left[X_1\right]$, the mean of the distribution, be the parameter of interest.&lt;/p&gt;
&lt;p style=&#34;color: red&#34;&gt;Question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Suppose that $\mathcal{P}$ is the class of all distributions that have a finite variance.&lt;/li&gt;
&lt;li&gt;What is the most efficient estimator of $\psi(P)$, i.e., what is the estimator with the best asymptotic performance?&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    A model $\mathcal{P}$ is simply a collection of probability distributions for the data we observe.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Example 2 (partial linear regression model)&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;Suppose that we observe i.i.d. data $\left\{X_i \equiv\left(Y_i, Z_i, V_i\right): i=1, \ldots, n\right\}$ from the following partial linear regression model:
$$
Y_i=Z_i^{\top} \beta+g\left(V_i\right)+\epsilon_i
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Y_i$ is the scalar response variable&lt;/li&gt;
&lt;li&gt;$Z_i$ and $V_i$ are vectors of predictors&lt;/li&gt;
&lt;li&gt;$g(\cdot)$ is the unknown (nonparametric) function&lt;/li&gt;
&lt;li&gt;$\epsilon_i$ is the unobserved error.&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;color: red&#34;&gt;For simplicity and to focus on the semiparametric nature of the problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assume that $\left(Z_i, V_i\right) \sim f(\cdot, \cdot)$, where we assume that the density $f(\cdot, \cdot)$ is known, is independent of $\epsilon_i \sim N\left(0, \sigma^2\right)$ (with $\sigma^2$ known).&lt;/li&gt;
&lt;li&gt;The model, under these assumptions, has a &lt;mark&gt;parametric component&lt;/mark&gt; $\beta$ and a &lt;mark&gt;nonparametric component&lt;/mark&gt; $g(\cdot)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;mark&gt;&lt;strong&gt;&amp;ldquo;Separated semiparametric model&amp;rdquo;&lt;/strong&gt;&lt;/mark&gt;: We say that the model $\mathcal{P}=\left\{P_{\nu, \eta}\right\}$ is a &amp;ldquo;separated semiparametric model&amp;rdquo;, where $\nu$ is a &lt;strong&gt;&amp;ldquo;Euclidean parameter&amp;rdquo;&lt;/strong&gt; and $\eta$ runs through a nonparametric class of distributions (or some infinite-dimensional set). This gives a semiparametric model in the strict sense, in which we &lt;font color=&#34;red&#34;&gt;aim at estimating $\nu$ and consider $\eta$ as a nuisance parameter.&lt;/font&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;mark&gt;&lt;strong&gt;&amp;ldquo;Frequent questions for semiparametric model&amp;rdquo;&lt;/strong&gt;&lt;/mark&gt;: consider the estimation of a parameter of interest $\nu=\nu(P)$, where the data has distribution $P \in \mathcal{P}$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(Q1) How well can we estimate $\nu=\nu(P)$ ? What is our &amp;ldquo;gold standard&amp;rdquo;?&lt;/li&gt;
&lt;li&gt;(Q2) Can we compare absolute &amp;ldquo;in principle&amp;rdquo; standards for estimation of $\nu$ in a model $\mathcal{P}$ with estimation of $\nu$ in a submodel $\mathcal{P}_0 \subset \mathcal{P}$ ? What is the effect of &lt;em&gt;not knowing&lt;/em&gt; $\eta$ on estimation of $\nu$ when $\mathcal{P}=\left\{P_\theta: \theta \equiv(\nu, \eta) \in \Theta\right\}$ ?&lt;/li&gt;
&lt;li&gt;(Q3) How do we construct &lt;strong&gt;efficient&lt;/strong&gt; estimators of $\nu(P)$ ?&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    A model $\mathcal{P}$ is simply a collection of probability distributions for the data we observe.
  &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Develop a general form for &lt;mark&gt;the asymptotic variance of semiparametric estimators&lt;/mark&gt; that depend on nonparametric estimators of functions.&lt;/li&gt;
&lt;li&gt;The formula is often straightforward to derive, requiring only some calculus.&lt;/li&gt;
&lt;li&gt;Although the formula is not based on primitive conditions, it should be useful for semiparametric estimators, just as analogous formulae are for parametric estimators.&lt;/li&gt;
&lt;li&gt;The formula gives the form of remainder terms, which facilitates specification of primitive conditions.&lt;/li&gt;
&lt;li&gt;It can also be used to make asymptotic efficiency comparisons and to find an efficient estimator in some class.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;markmap&#34; style=&#34;height: 500px;&#34;&gt;

&lt;pre&gt;- 
  - Derive the formula: **Section 2**
  - Propositions about semiparametric estimator
    - **Section 3**
    - **Section 4**
  - High-level regularity conditions: **Section 5**
  - Conditions for $\sqrt{n}$-consistency and asymptotic normality: **Section 6**
  - Primitive conditions for the examples: **Section 7**&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&#34;2-the-pathwise-derivative-formula-for-the-asymptotic-variance&#34;&gt;2. The Pathwise Derivative Formula For the Asymptotic Variance&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Preliminary: &lt;a href=&#34;https://ikerlz.github.io/notes/path-dev/&#34;&gt;one-step estimators and pathwise derivatives&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The formula is based on the observation that &lt;font color=&#34;red&#34;&gt;$\sqrt{n}$-consistent&lt;/font&gt; &lt;strong&gt;nonparametric&lt;/strong&gt; estimators are often efficient.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example, the sample mean is known to be an efficient estimator of the population mean in a &lt;strong&gt;nonparametric&lt;/strong&gt; model where no
restrictions, other than regularity conditions (e.g. &lt;strong&gt;existence of the second moment&lt;/strong&gt;) are placed on the distribution of the data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Idea&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Calculate the asymptotic variance of a semiparametric estimator as the variance bound for the &lt;em&gt;functional&lt;/em&gt; that it nonparametrically estimates.&lt;/li&gt;
&lt;li&gt;In other words, the formula is &lt;font color=&#34;red&#34;&gt;the variance bound for the functional that is the limit of the estimator under general misspecification.&lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Let $z_1, \ldots, z_n$ be i.i.d. data, with &lt;font color=&#34;red&#34;&gt;(true) distribution $F_0$&lt;/font&gt; of $z_i$, and let $\hat{\beta}=\beta_n\left(z_1, \ldots, z_n\right)$ denote a $q \times 1$ vector of estimators. Suppose $\hat{\beta}$ can be associated with a family of distributions and a functional as
$$
\hat{\beta} \rightarrow \begin{cases}\mathscr{F}=\{F\} ; &amp;amp; \text { general family of distributions of } z \\ \mu: \mathscr{F} \rightarrow \mathbb{R}^q ; &amp;amp; \text { if } z_i \text { has distribution } F \text { then } {\color{red}\operatorname{plim}(\hat{\beta})=\mu(F)}\end{cases}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The word &amp;ldquo;general&amp;rdquo; is taken to mean that $\mathscr{F}$ is unrestricted, except for regularity conditions, and allows for general misspecification.&lt;/li&gt;
&lt;li&gt;This equation also specifies that &lt;mark&gt;$\mu(F)$ is the limit of $\hat{\beta}$ when $z_i$ has distribution $F$&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;$\mu(F)$ traces out the limits of $\hat{\beta}$ as $F$ varies within the general family $\mathscr{F}$.&lt;/li&gt;
&lt;li&gt;The variance formula for $\hat{\beta}$ is the semiparametric bound for estimation of $\mu(F)$, calculated as in &lt;em&gt;Koshevnik and Levit (1976)&lt;/em&gt;, &lt;em&gt;Pfanzagl and Wefelmeyer (1982)&lt;/em&gt;, and others.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Let &lt;mark&gt;$\{F_\theta: F_\theta \in \mathscr{F}\}$&lt;/mark&gt; denote a one-dimensional subfamily of $\mathscr{F}$, i.e. a path in $\mathscr{F}$, that is equal to the true distribution $F_0$ when $\theta=0$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Suppose that $F_\theta$ has a density $d F_\theta$ and a corresponding score
$${\color{red}S(z)=\frac{\partial \ln \left(d F_\theta\right)}{ \partial \theta}\Big|_{\theta=0}}.$$&lt;/li&gt;
&lt;li&gt;Suppose that the set of scores can approximate in mean square any mean zero, finite variance function of $z$.&lt;/li&gt;
&lt;li&gt;Let $E[\cdot]$ denote the expectation at the true distribution $F_0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;pathwise derivative&lt;/strong&gt; of $\mu(F)$ is a $q \times 1$ vector $d(z)$ with &lt;mark&gt;$E[d(z)]=0$ and $E\left[|d(z)|^2\right]&amp;lt;\infty$&lt;/mark&gt; such that for every path,
$$
{\color{red}\frac{\partial \mu\left(F_\theta\right)}{\partial \theta}=E[d(z) S(z)].}\qquad(\star)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;mark&gt;The variance bound for estimation of $\mu(F)$ is $\operatorname{Var}(d(z))$&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;Thus, the asymptotic variance formula suggested here is the variance of the &lt;strong&gt;pathwise derivative&lt;/strong&gt; of the functional $\mu(F)$ that is estimated under general misspecification.&lt;/li&gt;
&lt;/ul&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;Consider the parameter $\beta_0=\int f_0(z)^2 d z$, where $f_0(z)$ is the density function of $z_i$.&lt;/li&gt;
&lt;li&gt;One estimator is &lt;mark&gt;$\tilde{\beta}=\sum_{i=1}^n \hat{f}\left(z_i\right) / n$&lt;/mark&gt;, for a nonparametric density estimator $\hat{f}(z)$ of $z_i$.&lt;/li&gt;
&lt;li&gt;Suppose $z$ is symmetrically distributed around zero. Then one might hope to improve efficiency by using the antithetic estimate $\hat{f}(-z)$ of the density to form
$$\hat{\beta}=\sum_{i=1}^n\frac{[\hat{f}(z_i)+\hat{f}(-z_i)]}{2}.$$&lt;/li&gt;
&lt;li&gt;The asymptotic variance can be found by calculating the limit of $\hat{\beta}$ under &lt;mark&gt;general misspecification&lt;/mark&gt;, where $z$ need not be symmetric about zero, and the pathwise derivative of this limit.
&lt;ul&gt;
&lt;li&gt;Let $E_F[\cdot]$ denote the expectation at a distribution $F$ and let $E_\theta[\cdot]=E_{F_\theta}[\cdot]$ for a path $F_\theta$.&lt;/li&gt;
&lt;li&gt;By an appropriate uniform law of large numbers the limit of $\hat{\beta}$ is $\mu(F)=\int[f(z)+f(-z)] f(z) d z / 2$.&lt;/li&gt;
&lt;li&gt;Assuming that differentiation inside the integral is allowed,
$$
\begin{aligned}
\frac{\partial \mu\left(F_\theta\right)}{ \partial \theta} &amp;amp;=\int\left[\frac{\partial f_\theta(z) }{ \partial \theta}\right] f_0(z) d z\\
&amp;amp;\qquad +\frac{1}{2}\left\{\int\left[\frac{\partial f_\theta(-z)}{ \partial \theta}\right] f_0(z) d z\right.\\
&amp;amp;\qquad\qquad\qquad\qquad\qquad +\left.\int\left[\frac{\partial f_\theta(z) }{ \partial \theta}\right] f_0(-z) d z\right\}\\
&amp;amp;=E\left[\left\{f_0(z)+f_0(-z)\right\} S(z)\right]\\
&amp;amp;=E[d(z) S(z)]
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Thus, in this example the asymptotic variance formula is ${\color{red}\operatorname{Var}\left(2 f_0(z)\right)}$, which is the well known asymptotic variance of $\tilde{\beta}$, so &lt;mark&gt;no efficiency improvement results&lt;/mark&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;The &lt;strong&gt;pathwise derivative&lt;/strong&gt; generalizes the Gateaux derivative formula for von-Mises estimators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The pathwise derivative formula works for estimators that are explicit functions of densities or expectations, where the domain of $\mu(F)$ may only include &lt;strong&gt;continuous distributions&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The Gateaux derivative formula only applies when the domain of $\mu(F)$ also includes &lt;strong&gt;discrete distributions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;A precise justification for the asymptotic variance formula is available when $\hat\beta$ is asymptotically equivalent to a sample average: define $\hat{\beta}$ to be asymptotically linear with &lt;strong&gt;influence function&lt;/strong&gt; $\psi(z)$ when $z_i$ has distribution $F_0$:
$$
\begin{aligned}
&amp;amp; \sqrt{n}\left(\hat{\beta}-\beta_0\right)=\sum_{i=1}^n \psi\left(z_i\right) / \sqrt{n}+O_p(1), \\
&amp;amp; E[\psi(z)]=0, \quad \operatorname{Var}(\psi(z)) \text { finite. }
\end{aligned}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Asymptotic linearity and the central limit theorem imply $\hat{\beta}$ is asymptotically normal with variance $\operatorname{Var}(\psi(z))$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;regular-path--regular-estimator&#34;&gt;Regular path &amp;amp; regular estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Define the path &lt;font color=&#34;red&#34;&gt;$\left\{F_\theta: \theta \in(-\varepsilon, \varepsilon) \subset \mathbb{R}, \varepsilon&amp;gt;0, F_\theta \in \mathscr{F}\right\}$&lt;/font&gt; to be regular if &lt;mark&gt;each distribution is absolutely continuous w.r.t the same dominating measure and $S(z)$ satisfies the mean-square derivative condition&lt;/mark&gt;
$$
\int\left[\theta^{-1}\left(d F_\theta^{1 / 2}-d F_0^{1 / 2}\right)-\frac{1}{2} S(z) d F_0^{1 / 2}\right]^2 d z \rightarrow 0, \text { as } \quad \theta \rightarrow 0 .
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Define $\hat{\beta}$ to be a &lt;strong&gt;regular estimator&lt;/strong&gt; of $\mu(F)$ if for any regular path and $\theta_n=$ $0(1 / \sqrt{n})$, when $z_i$ has distribution $F_{\theta_n}, \sqrt{n}\left(\hat{\beta}-\mu\left(F_{\theta_n}\right)\right)$ has a limiting distribution that &lt;mark&gt;does not depend on $\left\{\theta_n\right\}_{n=1}^{\infty}$&lt;/mark&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-pathwise-derivative-asymptotic-variance-formula&#34;&gt;The pathwise derivative asymptotic variance formula&lt;/h3&gt;
&lt;p&gt;&lt;mark&gt;&lt;strong&gt;THEOREM 2.1&lt;/strong&gt;&lt;/mark&gt;: Suppose that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(i) the set of scores for regular paths is linear;&lt;/li&gt;
&lt;li&gt;(ii) for any $\varepsilon&amp;gt;0$ and measureable $s(z)$ with $E[s(z)]=0$ and $E\left[s(z)^2\right]&amp;lt;\infty$, there is a regular path with score $S(z)$ satisfying $E\left[|s(z)-S(z)|^2\right]&amp;lt;\varepsilon$;&lt;/li&gt;
&lt;li&gt;(iii) $\hat{\beta}$ is asymptotically linear and regular.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Then there is $d(z)$ such that equation $(\star)$ is satisfied and $\psi(z)=d(z)$&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Condition (ii), that the scores can approximate any mean zero function, is the precise version of the &amp;ldquo;generality&amp;rdquo; property of $\mathscr{F}$.&lt;/li&gt;
&lt;li&gt;Regularity of $\hat{\beta}$ is the precise condition that specifies that &lt;mark&gt;$\hat{\beta}$ is a nonparametric estimator of $\mu(F)$&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;Innovations: calculating the bound for the functional $\mu(F)$ is nonparametrically estimated by $\hat\beta$&lt;/li&gt;
&lt;li&gt;Asymptotic linearity and regularity imply pathwise differentiability&lt;/li&gt;
&lt;li&gt;Condition (ii) implies that there is &lt;strong&gt;only one influence function&lt;/strong&gt; and that it equals the pathwise derivative&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;Theorem 2.1 give a justification for the pathwise derivative formula, rather than an approach to showing asymptotic normality.&lt;/li&gt;
&lt;li&gt;A better approach:
&lt;ul&gt;
&lt;li&gt;Solve equation $(\star)$ for the pathwise derivative, as a candidate for the influence function&lt;/li&gt;
&lt;li&gt;Formulate regularity conditions for the remainder $\sqrt{n}\left(\hat{\theta}-\theta_0\right)-\sum_{i=1}^n \psi\left(z_i\right) / \sqrt{n}$ to be small.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The formula is a very important part of this approach, because it provides the form of the remainder.&lt;/li&gt;
&lt;li&gt;This approach, with formal calculation followed by regularity conditions, is similar to that used in parametric asymptotic theory (e.g. for Edgeworth expansions).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-semiparametric-m-estimators&#34;&gt;3. Semiparametric $M$-estimators&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let $h$ denote a function, that can depend on the parameters $\beta$ and the data $z$.&lt;/li&gt;
&lt;li&gt;Let $m(z, \beta, h)$ be a vector of functions with the same dimension as $\beta$. Here $m(z, \beta, h)$ can depend on the entire function $h$, rather than just its value at particular points, so $m(z, \beta, h)$ is a vector of functionals.&lt;/li&gt;
&lt;li&gt;Suppose that $E\left[m\left(z, \beta_0, h_0\right)\right]=0$ for the true values $\beta_0$ and $h_0$.&lt;/li&gt;
&lt;li&gt;Let $\hat{h}$ denote an estimator of $h$. A semiparametric $m$-estimator is one that solves a moment equation of the form
$$
{\color{red}\frac{1}{n}\sum_{i=1}^n m\left(z_i, \beta, \hat{h}\right)=0} .\qquad (\Delta)
$$&lt;/li&gt;
&lt;li&gt;The general idea here is that $\hat{\beta}$ is obtained by a procedure that &amp;ldquo;plugs-in&amp;rdquo; an estimated function $\hat{h}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- &lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;Example 3.1 Quasi-maximum Likelihood for a Conditional Mean Index&lt;/summary&gt;
  &lt;p&gt;&amp;ndash;&amp;gt;&lt;/p&gt;
&lt;!--&lt;/p&gt;
&lt;/details&gt; --&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-7&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Example 3.1 Quasi-maximum Likelihood for a Conditional Mean Index&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;The conditional mean index model: $E[y \mid x]=\tau\left(v\left(x, \beta_0\right)\right)$ for a &lt;strong&gt;known&lt;/strong&gt; function $v(x, \beta)$ and an &lt;strong&gt;unknown&lt;/strong&gt; function $\tau(\cdot)$.&lt;/li&gt;
&lt;li&gt;Let $\hat{h}(x, \beta)$ be a nonparametric estimator of $E[y \mid v(x, \beta)]$, such as a kernel estimator.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;An estimator of $\beta_0$ suggested by Ichimura (1993) minimizes $\sum_{i=1}^n\left[y_i-\hat{h}\left(x_i, \beta\right)\right]^2$.&lt;/li&gt;
&lt;li&gt;When $y_i$ is binary, Klein and Spady (1993) have suggested maximizing
$$\sum_{i=1}^n\left\{y_i \ln [\hat{h}(x_i, \beta)]+(1-y_i) \ln [1-\hat{h}(x_i, \beta)]\right\}$$&lt;/li&gt;
&lt;li&gt;A generalization of these estimators is a quasi-maximum likelihood estimator (QMLE) for an exponential family.&lt;/li&gt;
&lt;li&gt;The estimator will be efficient when the true distribution has the exponential form.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;To describe the estimator, let $l(u, \nu)=\exp (A(\nu)+B(u)+C(\nu) u)$ be a linear exponential density, with mean $\nu$.&lt;/li&gt;
&lt;li&gt;Consider an estimator $\hat{\beta}$ that maximizes $\sum_{i=1}^n \ln l\left(y_i, \hat{h}\left(x_i, \beta\right)\right)$.&lt;/li&gt;
&lt;li&gt;The first order conditions for this estimator make it a special case of equation $(\Delta)$, with
$$
m(z, \beta, h)=\left[A_\nu(h(x, \beta))+C_\nu(h(x, \beta)) y\right] \frac{\partial h(x, \beta) } {\partial \beta}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Example 3.2: Inverse Density Weighted Least Squares&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;Let $w(x)=r((x-$ $\zeta)^{\prime} \Omega(x-\zeta)$ ) be an elliptically symmetric density function, where $\zeta$ is a vector and $\Omega$ a positive definite matrix.&lt;/li&gt;
&lt;li&gt;Let $\hat{h}\left(x_l\right)$ be an estimator of the density of $x$, such as a kernel estimator.&lt;/li&gt;
&lt;li&gt;As shown by Ruud (1986), the weighted least squares estimator $\hat{\beta}=\left[\sum_{l=1}^n w\left(x_i\right) \hat{h}\left(x_l\right)^{-1} x_l x_l^{\prime}\right]^{-1} \sum_{l=1}^n w\left(x_l\right) \hat{h}\left(x_l\right)^{-1} x_i y_i$ will be consistent up to scale, for the coefficients $\gamma_0$ of an index model $E[y \mid x]=\tau\left(x^{\prime} \gamma_0\right)$.&lt;/li&gt;
&lt;li&gt;This example is a special case of equation $(\Delta)$ with
$$
m(z, \beta, h)=w(x) h(x)^{-1} x\left(y-x^{\prime} \beta\right) .
$$&lt;/li&gt;
&lt;li&gt;This estimator will be used to illustrate the correction term for density estimates.&lt;/li&gt;
&lt;li&gt;Asymptotic normality and $\sqrt{n}$-consistency for $\hat{h}(x)$ a kernel estimator, are shown in Newey and Ruud (1991).&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;p&gt;To use the pathwise derivative formula in this derivation, it is necessary to &lt;mark&gt;identify the functional that is nonparametrically estimated by $\hat{\beta}$.&lt;/mark&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let $h(F)$ denote the limit of $\hat{h}$ when $z$ has distribution $F$.&lt;/li&gt;
&lt;li&gt;The limit $\mu(F)$ of $\hat{\beta}$ for a general $F$ should be the solution to
$$
E_F[m(z, \mu, h(F))]=0 .\qquad(\odot)
$$&lt;/li&gt;
&lt;li&gt;Equation $(\Delta)$ sets $\hat{\beta}$ so that sample moments are zero, and the sample moments have a limit of $E_F[m(z, \beta, h(F))]$
&lt;ul&gt;
&lt;li&gt;by the law of large numbers and $h(F)$ equal to the limit of $\hat{h}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt;$\Rightarrow$ $\hat{\beta}$ is consistent for the solution of $(\odot)$.&lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Remarks&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the estimators depend only on the limit $h(F)$, and not on the particular form of the estimator $\hat{h}$.&lt;/li&gt;
&lt;li&gt;Different nonparametric estimators of the same functions should result in &lt;mark&gt;the same asymptotic variance&lt;/mark&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;mark&gt;&lt;strong&gt;Proposition 1&lt;/strong&gt;&lt;/mark&gt;: &lt;font color=&#34;blue&#34;&gt;The asymptotic variance of semiparametric estimators depends only on the function that is nonparametrically estimated, and not on the type of estimator.&lt;/font&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;For a path $\left\{F_\theta\right\}$, let $h(\theta)=h\left(F_\theta\right)$. Here, $\mu\left(F_\theta\right)$ will satisfy the population moment equation
$$
E_\theta[m(z, \mu, h(\theta))]=0 .
$$&lt;/li&gt;
&lt;li&gt;Let $m(z, h)=m\left(z, \beta_0, h\right)$. Differentiation under the integral gives
$$
\frac{\partial E_\theta\left[m\left(z, h_0\right)\right]}{ \partial \theta}=\int m\left(z, h_0\right)\left[\frac{\partial d F_\theta }{ \partial \theta}\right] d z=E\left[m\left(z, h_0\right) S(z)\right] .
$$&lt;/li&gt;
&lt;li&gt;Using the chain rule, it follows that
$$
\frac{\partial E_\theta[m(z, h(\theta))] }{ \partial \theta}=E\left[m\left(z, h_0\right) S(z)\right]+\frac{\partial E[m(z, h(\theta))] }{ \partial \theta} .
$$&lt;/li&gt;
&lt;li&gt;Assuming $M \equiv \partial E\left[m\left(z, \beta, h_0\right)\right] /\left.\partial \beta\right|_{\beta_0}$ is nonsingular, by the implicit function theorem
$$
\frac{\partial \mu\left(F_\theta\right) }{ \partial \theta}=-M^{-1}\left\{E\left[m\left(z, h_0\right) S(z)\right]+\frac{\partial E[m(z, h(\theta))] }{ \partial \theta}\right\} .
$$&lt;/li&gt;
&lt;li&gt;The first term is already in an outer product form, so that the pathwise derivative can be found by &lt;mark&gt;putting the second term in singular form&lt;/mark&gt;&lt;/li&gt;
&lt;li&gt;Suppose there is a $\alpha(z)$ such that $E[\alpha(z)]=0$ and
$$\frac{\partial E[m(z, h(\theta))] }{ \partial \theta}=E[\alpha(z) S(z)]\qquad(\oplus)$$.&lt;/li&gt;
&lt;li&gt;Move $-M^{-1}$ inside the expectation, it follows that the pathwise derivative is &lt;font color=&#34;red&#34;&gt;$d(z)=-M^{-1}\left\{m\left(z, h_0\right)+\alpha(z)\right\}$&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;By Theorem 2.1 the influence function of $\hat{\beta}$ is
$$
\psi(z)=-M^{-1}\left\{m\left(z, \beta_0, h_0\right)+\alpha(z)\right\}
$$
&lt;strong&gt;Remarks&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The leading term $-M^{-1} m\left(z, \beta_0, h_0\right)$ is the usual formula for the influence function of an $m$-estimator with moment functions $m\left(z, \beta, {\color{red}h_0}\right)$.&lt;/li&gt;
&lt;li&gt;The solution to equation $(\oplus)$ is an adjustment term for the estimation of $h_0$.&lt;/li&gt;
&lt;li&gt;Solving equation $(\oplus)$ is therefore the essential step in discovering &lt;mark&gt;how the estimation of $h$ affects the asymptotic variance&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;This solution can be interpreted as
&lt;ul&gt;
&lt;li&gt;the pathwise derivative of the functional $-M^{-1} E\left[m\left(z, \beta_0, h(F)\right)\right]$&lt;/li&gt;
&lt;li&gt;the influence function of $-M^{-1} \int m\left(z, \beta_0, \hat{h}\right) d F_0(z)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;when-will-the-adjustment-term-be-zero-&#34;&gt;When will the adjustment term be zero ?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If the adjustment term is zero, then it should not be necessary to account for the presence of $\hat{h}$, i.e. &lt;font color=&#34;red&#34;&gt;$\hat{h}$ can be treated as if it were equal to $h_0$&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;One case: equation $(\Delta)$ is &lt;mark&gt;the first-order condition&lt;/mark&gt; to a maximization problem, and $\hat{h}$ has a limit that &lt;mark&gt;maximizes the population value of the same function&lt;/mark&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;To be specific&lt;/strong&gt;, suppose that there is a function $q(z, \beta, h)$ and a set of functions $\mathscr{H}(\beta)$, possibly depending on $\beta$ but not on the distribution $F$ of $z$, such that
$$
\begin{aligned}
&amp;amp; m(z, \beta, h)=\partial q(z, \beta, h) / \partial \beta, \\
&amp;amp; h(F)=\operatorname{argmax}_{\tilde{h} \in \mathscr{H}(\beta)} E_F[q(z, \beta, \tilde{h})] .\qquad(\otimes)
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$m(z, \beta, h)$ are the first order conditions for a maximum of the function $q$&lt;/li&gt;
&lt;li&gt;$h(F)$ maximizes the expected value of the same function
&lt;ul&gt;
&lt;li&gt;i.e. $h(F)$ has been &amp;ldquo;concentrated out&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For any parametric model $F_\theta$, since $h(\theta)=h\left(F_\theta\right)$ $\Rightarrow$ $E[q(z, \beta, h(\theta))]$ is maximized at $\theta=0$ $\Rightarrow$ $\partial E[q(z, \beta, h(\theta))] / \partial \theta=0$. Differentiating again with respect to $\beta$,
$$
\begin{aligned}
0 &amp;amp; =\partial^2 E[q(z, \beta, h(\theta))] / \partial \theta \partial \beta\\
&amp;amp;=\partial E[\partial q(z, \beta, h(\theta)) / \partial \beta] / \partial \theta \\
&amp;amp; =\partial E[m(z, \beta, h(\theta))] / \partial \theta .
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evaluating this equation at $\beta_0$, it follows that $\alpha(z)=0$ will solve equation $(\oplus)$, and hence the adjustment term is zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Proposition 2&lt;/strong&gt;: &lt;font color=&#34;blue&#34;&gt;If equation $(\otimes)$ is satisfied, then the estimation of $h$ can be ignored in calculating the asymptotic variance, i.e. it is the same as if $\hat{h}=h_0$&lt;/font&gt;&lt;/p&gt;
&lt;h3 id=&#34;a-more-direct-condition&#34;&gt;A more direct condition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Suppose that $m(z, h)$ depends on $h$ only through its value $h(v)$ at a subvector $v$ of $z$, i.e. $m(z, h)=m(z, h(v))$ where the last function depends on a real vector argument in $m(z, h)$.&lt;/li&gt;
&lt;li&gt;Let $h(v, \theta)$ denote the limiting value of $\hat{h}\left(v, \beta_0\right)$ for a path. For $D(z)=\partial m\left(z, \beta_0, h\right) /\left.\partial h\right|_{h=h_0(v)}$, differentiation gives
$$
\frac{\partial E[m(z, h(\theta))] }{ \partial \theta}=E\left[D(z) \frac{\partial h(v, \theta) }{ \partial \theta}\right]=\frac{\partial E[D(z) h(v, \theta)] }{ \partial \theta} .
$$&lt;/li&gt;
&lt;li&gt;If this derivative is zero for all $h(v, \theta)$, then $\alpha(z)=0$ will solve equation $(\oplus)$, and the adjustment term is zero.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One simple condition&lt;/strong&gt; for this is that &lt;mark&gt;$E[D(z) \mid v]=0$&lt;/mark&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;More generally, the adjustment term will be zero if $h(v, \theta)$ is an element of a set to which $D(z)$ is orthogonal.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proposition 3&lt;/strong&gt;: &lt;font color=&#34;blue&#34;&gt;If $E[D(z) \mid v]=0$, or more generally, for all $F, h(v, F)$ is an element of a set $\mathscr{H}$ such that $E[D(z) \tilde{h}(v)]=0$ for all $\tilde{h} \in \mathscr{H}$, then estimation of $h$ can be ignored in calculating the asymptotic variance.&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remarks&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;This condition can be checked by straightforward calculation, unlike Proposition 2, which requires finding $q(z, \beta, h)$ satisfying equation $(\otimes)$.&lt;/p&gt;
&lt;h2 id=&#34;4-functions-of-mean-square-projection-and-densities&#34;&gt;4. Functions of Mean-square Projection and Densities&lt;/h2&gt;
&lt;h2 id=&#34;5-regularity-conditions&#34;&gt;5. Regularity Conditions&lt;/h2&gt;
&lt;h2 id=&#34;6-series-estimation-of-projection-functionals&#34;&gt;6. Series Estimation of Projection Functionals&lt;/h2&gt;
&lt;h2 id=&#34;7-power-series-estimators-for-semiparametric-individual-effects-and-average-derivatives&#34;&gt;7. Power Series Estimators For Semiparametric Individual Effects and Average Derivatives&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;KOSHEVNIK, Y. A., AND B. Y. LEVIT (1976): &amp;ldquo;&lt;a href=&#34;https://www.mathnet.ru/php/archive.phtml?wshow=paper&amp;amp;jrnid=tvp&amp;amp;paperid=3421&amp;amp;option_lang=eng&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On a Non-parametric Analogue of the Information Matrix&lt;/a&gt;&amp;rdquo;, Theory of Probability and Applications, 21, 738-753.&lt;/li&gt;
&lt;li&gt;PFANZAGL, J., AND WEFELMEYER (1982): &amp;ldquo;&lt;a href=&#34;https://www.degruyter.com/document/doi/10.1524/strm.1985.3.34.379/html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contributions to a General Asymptotic Statistical Theory&lt;/a&gt;&amp;rdquo;, New York: Springer-Verlag.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>混合高斯模型的代码实现</title>
      <link>https://ikerlz.github.io/post/gmm/</link>
      <pubDate>Thu, 02 Nov 2023 00:39:27 +0800</pubDate>
      <guid>https://ikerlz.github.io/post/gmm/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#一生成混合高斯数据&#34;&gt;一、生成混合高斯数据&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1原理&#34;&gt;1、原理&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2python实现&#34;&gt;2、Python实现&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3r实现&#34;&gt;3、R实现&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#二em算法&#34;&gt;二、EM算法&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1原理-1&#34;&gt;1、原理&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2python实现-1&#34;&gt;2、Python实现&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3r实现-1&#34;&gt;3、R实现&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4第三方工具库&#34;&gt;4、第三方工具库&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#三真实数据&#34;&gt;三、真实数据&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1python实现&#34;&gt;1、Python实现&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2r实现&#34;&gt;2、R实现&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#四后记&#34;&gt;四、后记&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;h2 id=&#34;一生成混合高斯数据&#34;&gt;一、生成混合高斯数据&lt;/h2&gt;
&lt;h3 id=&#34;1原理&#34;&gt;1、原理&lt;/h3&gt;
&lt;p&gt;利用混合高斯模型的公式：&lt;/p&gt;
&lt;p&gt;$$
P(y \mid \theta)=\sum_{k=1}^{K} \alpha_{k} \phi\left(y \mid \theta_{k}\right)
$$&lt;/p&gt;
&lt;p&gt;只需要给定：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;各组分布的均值向量及协方差矩阵$\theta_k = \left(\mu_k, \Sigma_k \right)$&lt;/li&gt;
&lt;li&gt;需要生成的数据总数$N$以及各组权重$\alpha = (\alpha_1,\ldots,\alpha_K)$
即可生成GMM数据&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;注意：$\Sigma_k$需要是一个对称正定的矩阵&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;两种思路：&lt;/strong&gt;&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;思路一&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;利用权重$\alpha = (\alpha_1,\ldots,\alpha_K)$进行带权重的抽样，从$[1,2,\ldots,K]$中抽样$N$次得到一个长度为$N$的列表（向量），记为&lt;code&gt;group_list&lt;/code&gt;，第$i$个元素表示数据应该从第$i$个分模型中生成&lt;/li&gt;
&lt;li&gt;遍历列表（向量）&lt;code&gt;group_list&lt;/code&gt;中的每个元素，若第$i$次遍历中，&lt;code&gt;group_list[i]=k&lt;/code&gt;，则表示从$\mathcal{N}(\mu_k,\Sigma_k)$中生成一个数据$X_i$&lt;/li&gt;
&lt;li&gt;将$X_1,\ldots,X_N$排列合并得到一个矩阵$X=(X_1,\ldots,X_N)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;思路二&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;利用$\alpha = (\alpha_1,\ldots,\alpha_K)$，乘上总数$N$，得到每个类别大致的数量，记为&lt;code&gt;group_num_list&lt;/code&gt;，第$k$个元素表示有&lt;code&gt;group_num_list[k]&lt;/code&gt;个数据来源于第$k$个分模型&lt;/li&gt;
&lt;li&gt;遍历列表（向量）&lt;code&gt;group_num_list&lt;/code&gt;，若在第$k$次遍历中，&lt;code&gt;group_num_list[k]=&lt;/code&gt;$n_k$，则表示从$\mathcal{N}(\mu_k,\Sigma_k)$中独立生成$n_k$个数据$X^k=(X_1,\ldots,X_{n_k})$&lt;/li&gt;
&lt;li&gt;将$X^1,\ldots,X^k$合并得到矩阵$X=(X_1,\ldots,X_N)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;blockquote&gt;
&lt;p&gt;注意：思路一的方法是严格按照GMM的定理来生成的，复杂度为$O(N)$；思路二严格来说与理论有少许差异，但是时间复杂度会低一些，为$O(K)$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;2python实现&#34;&gt;2、Python实现&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;generate_gmm_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    :return: 返回一个自变量矩阵X_mat以及一个因变量向量y_vec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;assert&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;weight_vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 判断类别数量是否等于参数的数量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;num_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;weight_vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;num_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 取整后可能存在加和与总数不等的情况，考虑在最后一个组上做处理&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;assert&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;X_mat_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;multivariate_normal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;X_mat_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;normal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;y_vec_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_mat_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_vec_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vstack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_mat_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hstack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_vec_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;shuffle_index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shuffle_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shuffle_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# if len(X_mat.shape) == 3:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;#     X_mat = X_mat[0]  # 维度为1的情况会升维&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shuffle_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;3r实现&#34;&gt;3、R实现&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;generate_gmm_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;function&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;params_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;NULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;TRUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# params_list是一个list类型，应当包含以下几个数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# total_num: 需要生成数据的数量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# weight_vec: 各类别的权重&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# mean_list: 均值列表&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# cov_mat_list: 协方差矩阵列表&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kr&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;is.list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;params_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;weight_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;params_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;weight_vec&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;params_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;params_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;params_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;cluster_num&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;weight_vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 如果没有给定参数列表，则使用默认设置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;weight_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0.3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;-0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;cluster_num&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;hhh&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 判断cluster数量与mean_list和cov_mat_list中元素个数是否相等&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kr&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;weight_vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cluster_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;stop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;The length of weight vector is not equal to number of cluster&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kr&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cluster_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;stop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;The length of mean list is not equal to number of cluster&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kr&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cluster_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;stop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;The length of covariance matrix list is not equal to number of cluster&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 各类别的数量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;num_before_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kr&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cluster_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kr&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cluster_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;num_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;floor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;weight_vec[k]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 采用向下取整的策略，并在最后一组做处理&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;num_before_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_before_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;num_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_before_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;X_mat_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;rmvnorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean_list[[k]]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cov_mat_list[[k]]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 生成一个num_k * p 维的matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;y_vec_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;rep&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kr&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_mat_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_vec_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;c1&#34;&gt;# 纵向合并&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;rbind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_mat_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_vec_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# shuffle&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kr&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;shuffle_index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_mat[shuffle_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_vec[shuffle_index]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 构造返回结果&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;res&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;as.matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kr&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;res&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;由于GMM类似于聚类模型，因此可以使用Python中Sklearn库中的&lt;code&gt;make_blobs&lt;/code&gt;函数快速生成数据&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;make_blobs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_samples&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;300&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;250&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;400&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_features&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;n&#34;&gt;centers&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;150&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;250&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;400&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;600&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;300&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;500&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;n&#34;&gt;cluster_std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;random_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;style&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ggplot&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figure&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Data&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;marker&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;o&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;squeeze&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  id=&#34;figure-利用sklearn生成的聚类数据&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;利用Sklearn生成的聚类数据&#34; srcset=&#34;
               /media/posts/gmm/%E6%88%AA%E5%B1%8F2021-09-10_%E4%B8%8B%E5%8D%883.03.05_huac7b4589a068f84ddc9e0bbdc94a3032_125835_774ebaa6eae3826fbe2da6c37a5c8ece.webp 400w,
               /media/posts/gmm/%E6%88%AA%E5%B1%8F2021-09-10_%E4%B8%8B%E5%8D%883.03.05_huac7b4589a068f84ddc9e0bbdc94a3032_125835_10ba755b7b6fc40a17238a0d363bd3f1.webp 760w,
               /media/posts/gmm/%E6%88%AA%E5%B1%8F2021-09-10_%E4%B8%8B%E5%8D%883.03.05_huac7b4589a068f84ddc9e0bbdc94a3032_125835_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/posts/gmm/%E6%88%AA%E5%B1%8F2021-09-10_%E4%B8%8B%E5%8D%883.03.05_huac7b4589a068f84ddc9e0bbdc94a3032_125835_774ebaa6eae3826fbe2da6c37a5c8ece.webp&#34;
               width=&#34;445&#34;
               height=&#34;291&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      利用Sklearn生成的聚类数据
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;二em算法&#34;&gt;二、EM算法&lt;/h2&gt;
&lt;h3 id=&#34;1原理-1&#34;&gt;1、原理&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GMM的EM实现使用的是传统的EM算法框架，Python实现中，主要使用了&lt;code&gt;Numpy&lt;/code&gt;做矩阵运算，R实现中，其自带的矩阵运算已经能满足要求，不需要另外导包&lt;/li&gt;
&lt;li&gt;由于EM算法本身是一个迭代求解算法，因此需要给出终止条件，在本文的实现中，使用了两个终止条件：
&lt;ul&gt;
&lt;li&gt;最大迭代次数：&lt;code&gt;max_iter&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;对数似然更新阈值：&lt;code&gt;eps&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;print_log&lt;/code&gt;参数用于控制是否输出每次迭代的信息&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;思路&#34;&gt;思路&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;首先进行初始化，即初始化$\alpha_k$、$\mu_k$、$\Sigma_k$；可以采取多种初始化方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kmeans方法（第三方包常用）&lt;/li&gt;
&lt;li&gt;$\alpha_k$初始化为$\frac{1}{K}$，$\mu_k$选择$K$个数据点的值作为初始化的值，$\Sigma_k$可以选择整体的协方差矩阵作为初始化值&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;E-Step：利用下式更新$\Gamma\in \mathbb{R}^{N\times K}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\widehat{\gamma}_{j k}=\frac{\alpha_{k} \phi\left(y_{j} \mid \theta_{k}\right)}{\sum_{k=1}^{K} \alpha_{k} \phi\left(y_{j} \mid \theta_{k}\right)}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;M-Step：利用下式更新待估计的参数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\widehat{\mu}_{k}=\frac{\sum_{j=1}^{N} \widehat{\gamma}_{j k} y_{j}}{\sum_{j=1}^{N} \widehat{y}_{j k}}\quad
\widehat{\sigma}_{k}^{2}=\frac{\sum_{j=1}^{N} \widehat{\gamma}_{j k}\left(y_{j}-\mu_{k}\right)^{2}}{\sum_{j=1}^{N} \widehat{\gamma}_{j k}}\quad
\widehat{\alpha}_{k}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}}{N}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;重复直到收敛，这里的收敛条件可以是$|\theta^{(i+1)}-\theta^{(i)}|_2 &amp;lt; \epsilon$，也可以是$|L(\theta^{(i+1)})-L(\theta^{(i)})|&amp;lt;\epsilon$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2python实现-1&#34;&gt;2、Python实现&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;em_algorithm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e-3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;print_log&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    实现EM算法
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    :return: 返回各组数据的属于每个分类的概率列表
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 初始化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 各组概率&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pi_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 各组均值: 采用随机选择的策略&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;initial_index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;choice&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])],&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;replace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tolist&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initial_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 各组协方差&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;initial_total_cov&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;initial_total_cov&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 其他&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;diff_loglikelihood&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;loglikelihood_old&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dimension&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 每个个体属于哪个组&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 迭代求解&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;abs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;diff_loglikelihood&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# E-Step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# 对于每个个体，计算其属于第k个组的概率&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;pi_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;multivariate_normal&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:])&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 归一化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# M-Step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;gamma_k_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tolist&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 列求和, 得到每个组的gamma求和, size=K&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# 更新均值&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_k_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# 更新协方差矩阵&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;e_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 一个N*p维的矩阵&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;e_mat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;e_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_k_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# 更新权重&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;pi_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_k_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;loglikelihood_new&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pi_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pi_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                            &lt;span class=&#34;nb&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;multivariate_normal&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logpdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 更新step和loglikehood&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;diff_loglikelihood&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loglikelihood_new&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loglikelihood_old&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;loglikelihood_old&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loglikelihood_new&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;print_log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;-----------------------------------------------&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Iteration: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Loglikehood Diff: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;diff_loglikelihood&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;print_log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;============================================&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Total Iteration: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prediction&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;argmax&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tolist&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean_prediction&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean_list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cov_mat_prediction&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pi_prediction&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pi_list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;3r实现-1&#34;&gt;3、R实现&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;gmm_em_algorithm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;function&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_clusters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1e-3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;print_log&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;TRUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 实现混合高斯的EM算法&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# data: N * p维的一个matrix或者dataframe&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# num_clusters: 类别数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# eps: 迭代的阈值&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# max_iter: 最大迭代次数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# print_log: 每次迭代是否打印日志&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;as.matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;[1]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;dimension&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;[2]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 初始化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 各组概率&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;pi_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;rep&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_clusters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_clusters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 各组均值: 采用随机选择的策略&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;initial_index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_clusters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;mean_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data[initial_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 是一个num_clusters * dimension的矩阵&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;init_cov_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;cov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kr&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_clusters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;cov_mat_list[[i]]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;init_cov_mat&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 其他&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;diff_loglikelihood&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;loglikelihood_old&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 每个个体属于哪个组&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_clusters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 迭代求解&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kr&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;abs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;diff_loglikelihood&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# E-Step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kr&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;kr&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_clusters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 计算第index个个体属于第k个类别的概率&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;gamma_mat[index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pi_vec[k]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;dmvnorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data[index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean_mat[k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cov_mat_list[[k]]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;c1&#34;&gt;# 归一化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;gamma_mat[index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_mat[index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma_mat[index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# M-Step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;gamma_k_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;colSums&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 列求和, 得到每个组的gamma求和, size=cluster_num&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kr&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_clusters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;c1&#34;&gt;# 更新均值&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;mean_mat[k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;colSums&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_mat[&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_k_vec[k]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;c1&#34;&gt;# 更新协方差矩阵&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;e_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean_mat[k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 一个dimension * N的矩阵&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;cov_mat_list[[k]]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e_mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%*%&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;e_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_mat[&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_k_vec[k]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;c1&#34;&gt;# 更新权重&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;pi_vec[k]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gamma_k_vec[k]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 计算对数似然&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;part1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;total_num&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pi_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pi_vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;part2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kr&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_clusters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;c1&#34;&gt;# print(cov_mat_list[[k]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;part2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;part2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma_mat[&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;dmvnorm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean_mat[k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cov_mat_list[[k]]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;loglikelihood_new&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;part1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;part2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 更新step和loglikehood&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;diff_loglikelihood&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loglikelihood_new&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loglikelihood_old&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;loglikelihood_old&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loglikelihood_new&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;kr&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;print_log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;-----------------------------------------------&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;paste0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Iteration: &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;as.character&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;paste0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Loglikehood Diff: &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;as.character&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;diff_loglikelihood&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kr&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;print_log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;============================================&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;paste0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Total Iteration: &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;as.character&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 输出结果&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nf&#34;&gt;colnames&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_clusters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;res&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;weight_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pi_vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mean_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;cov_mat_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cov_mat_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;label_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;as.numeric&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;function&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;colnames&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma_mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;[which.max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;4第三方工具库&#34;&gt;4、第三方工具库&lt;/h3&gt;
&lt;p&gt;在Python中，&lt;code&gt;Sklearn&lt;/code&gt;库实现了GMM，R中，&lt;code&gt;mclust&lt;/code&gt;包也实现了GMM；调用非常简单&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sklearn: (&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html?highlight=gaussianmixture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;API Reference&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mixture&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clf&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mixture&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GaussianMixture&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_components&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;covariance_type&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;full&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# K为类别数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# data为需要拟合的数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;mclust: (&lt;a href=&#34;https://cran.r-project.org/web/packages/mclust/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;API Reference&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 构建EM算法模型，指定分K类&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;EM_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;Mclust&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# data为需要拟合的数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查看基本信息&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;summary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;EM_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;三真实数据&#34;&gt;三、真实数据&lt;/h2&gt;
&lt;p&gt;我们使用常用的&lt;code&gt;Iris&lt;/code&gt;数据集，分别在R、Python上实现并比较手动实现与第三方工具库实现&lt;/p&gt;
&lt;h3 id=&#34;1python实现&#34;&gt;1、Python实现&lt;/h3&gt;

















&lt;div class=&#34;gallery-grid&#34;&gt;

  
  
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-/posts/gmm/gmm_python&#34; href=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_python/2021-09-10-16-19-25.jpg&#34; &gt;
      &lt;img src=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_python/2021-09-10-16-19-25_hu1ac5e3310b62caec945caa8ce12c7549_1229090_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;2021-09-10-16-19-25.jpg&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-/posts/gmm/gmm_python&#34; href=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_python/2021-09-10-16-19-31.jpg&#34; &gt;
      &lt;img src=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_python/2021-09-10-16-19-31_hu1ac5e3310b62caec945caa8ce12c7549_1229090_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;2021-09-10-16-19-31.jpg&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-/posts/gmm/gmm_python&#34; href=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_python/2021-09-10-16-19-36.jpg&#34; &gt;
      &lt;img src=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_python/2021-09-10-16-19-36_hu1ac5e3310b62caec945caa8ce12c7549_1229090_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;2021-09-10-16-19-36.jpg&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;手动实现准确率：97.33%&lt;/li&gt;
&lt;li&gt;Sklearn实现准确率：96.67%&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2r实现&#34;&gt;2、R实现&lt;/h3&gt;

















&lt;div class=&#34;gallery-grid&#34;&gt;

  
  
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-/posts/gmm/gmm_R&#34; href=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_r/2021-09-10-23-45-01.jpg&#34; &gt;
      &lt;img src=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_r/2021-09-10-23-45-01_hu197cdcdf1f6d9a29a4ee3537d8c5a9d8_1374858_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;2021-09-10-23-45-01.jpg&#34; width=&#34;710&#34; height=&#34;484&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-/posts/gmm/gmm_R&#34; href=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_r/2021-09-10-23-45-24.jpg&#34; &gt;
      &lt;img src=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_r/2021-09-10-23-45-24_hu197cdcdf1f6d9a29a4ee3537d8c5a9d8_1374858_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;2021-09-10-23-45-24.jpg&#34; width=&#34;710&#34; height=&#34;484&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-/posts/gmm/gmm_R&#34; href=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_r/2021-09-10-23-45-53.jpg&#34; &gt;
      &lt;img src=&#34;https://ikerlz.github.io/media/albums/posts/gmm/gmm_r/2021-09-10-23-45-53_hu197cdcdf1f6d9a29a4ee3537d8c5a9d8_1374858_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;2021-09-10-23-45-53.jpg&#34; width=&#34;710&#34; height=&#34;484&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;手动实现准确率：80%&lt;/li&gt;
&lt;li&gt;mclust实现准确率：96.67%&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;四后记&#34;&gt;四、后记&lt;/h2&gt;
&lt;p&gt;手动实现的代码虽能较好实现所需功能，但相较而言没有第三方工具库实现的稳定性高，运行速度上也相差很大；可以进一步探究第三方包在实现上采用了哪些技巧？&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using the Propensity Score in Regressions for Causal Effects</title>
      <link>https://ikerlz.github.io/slides/causalinferencechapter14/</link>
      <pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://ikerlz.github.io/slides/causalinferencechapter14/</guid>
      <description>
&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;ZIBIN.jpg&#34;
  &gt;

&lt;h1 id=&#34;using-the-propensity-score-in-regressions-for-causal-effects&#34;&gt;Using the Propensity Score in Regressions for Causal Effects&lt;/h1&gt;
&lt;p&gt;$$
\begin{aligned}
\
\end{aligned}
$$&lt;/p&gt;
&lt;center&gt; Li Zhe &lt;center&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;School of Data Science, Fudan University &lt;center&gt; 
&lt;p&gt;$$$$&lt;/p&gt;
&lt;center&gt;November 1, 2023&lt;center&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This chapter discusses two simple methods to use the propensity score:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the propensity score as a covariate in regressions&lt;/li&gt;
&lt;li&gt;running regressions weighted by the inverse of the propensity score&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reasons&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;they are easy to implement, which involve only standard statistical software packages for regressions;&lt;/li&gt;
&lt;li&gt;their properties are comparable to many more complex methods;&lt;/li&gt;
&lt;li&gt;they can be easily extended to allow for flexible statistical models including machine learning algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;outline&#34;&gt;Outline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Regressions with the propensity score as a covariate
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Theorem 14.1&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Proposition 14.1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regressions weighted by the inverse of the propensity score
&lt;ul&gt;
&lt;li&gt;Average causal effect
&lt;ul&gt;
&lt;li&gt;Theorem 14.2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Average causal effect on the treated units
&lt;ul&gt;
&lt;li&gt;Table 14.1&lt;/li&gt;
&lt;li&gt;Proposition 14.2&lt;/li&gt;
&lt;li&gt;Theorem 14.3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regressions-with-the-propensity-score-as-a-covariate&#34;&gt;Regressions with the propensity score as a covariate&lt;/h3&gt;
&lt;p&gt;$$
\text { Theorem 11.1 If } Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid X \text {, then } {\color{red}Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid e(X)} \text {. }
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By Theorem 11.1, if unconfoundedness holds conditioning on $X$, then it also holds conditioning on $e(X)$: $\color{red}{Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid e(X) }.$&lt;/li&gt;
&lt;li&gt;Analogous to (10.5), $\tau$ is also &lt;mark&gt;nonparametrically&lt;/mark&gt; identified by
$$
\tau=E[E\{Y \mid Z=1, e(X)\}-E\{Y \mid Z=0, e(X)\}],
$$&lt;/li&gt;
&lt;li&gt;$\Rightarrow$ The simplest regression specification is the OLS fit of $Y$ on $\{1, Z, e(X)\}$, with the coefficient of $Z$ as an estimator, denoted by $\tau_e$:
$$
\arg \min _{a, b, c} E\{Y-a-b Z-c e(X)\}^2
$$&lt;/li&gt;
&lt;li&gt;$\tau_e$ defined as the coefficient of $Z$.&lt;/li&gt;
&lt;li&gt;It is consistent for $\tau$ if
&lt;ul&gt;
&lt;li&gt;have a correct propensity score model&lt;/li&gt;
&lt;li&gt;the outcome model is indeed linear in $Z$ and $e(X)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\tau_e$ estimates $\tau_{\mathrm{O}}$ if we have a correct propensity score model even if the outcome model is &lt;mark&gt;completely misspecified&lt;/mark&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regressions-with-the-propensity-score-as-a-covariate-1&#34;&gt;Regressions with the propensity score as a covariate&lt;/h3&gt;
&lt;p&gt;&lt;mark&gt;Theorem 14.1&lt;/mark&gt; If $Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid X$, then the coefficient of $Z$ in the OLS fit of $Y$ on $\{1, Z, e(X)\}$ equals
$$
\tau_e=\tau_{\mathrm{O}}=\frac{E\left\{h_{\mathrm{O}}(X) \tau(X)\right\}}{E\left\{h_{\mathrm{O}}(X)\right\}},
$$
recalling that $h_{\mathrm{O}}(X)=e(X)\{1-e(X)\}$ and $\tau(X)=E\{Y(1)-Y(0) \mid X\}$.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\
\
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Corollary 14.1&lt;/mark&gt; If $Z \perp\!\!\!\perp\{Y(1), Y(0)\} \mid X$, then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the coefficient of $Z-e(X)$ in the OLS fit of $Y$ on $Z-e(X)$ or $\{1, Z-e(X)\}$ equals $\tau_{\mathrm{O}}$;&lt;/li&gt;
&lt;li&gt;the coefficient of $Z$ in the OLS fit of $Y$ on $\{1, Z, e(X), X\}$ equals $\tau_{\mathrm{O}}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regressions-with-the-propensity-score-as-a-covariate-2&#34;&gt;Regressions with the propensity score as a covariate&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An unusual feature of Theorem 14.1 is that &lt;strong&gt;the overlap condition&lt;/strong&gt; ($0 &amp;lt; e(x) &amp;lt; 1$) is not needed any more.&lt;/li&gt;
&lt;li&gt;Even if some units have propensity score $e(X)$ equaling 0 or 1, their associate weight $e(X)\{1-e(X)\}$ is zero so that they do not contribute anything to the final parameter $\tau_O$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h2 id=&#34;frischwaughlovell-theorem&#34;&gt;Frisch–Waugh–Lovell Theorem&lt;/h2&gt;
&lt;p&gt;The Frisch–Waugh–Lovell (FWL) theorem reduces multivariate OLS to univariate OLS and therefore facilitate the understanding and calculation of the OLS coefficients.&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Theorem A2.2 (sample FWL)&lt;/mark&gt; With data $\left(Y, X_1, X_2, \ldots, X_p\right)$ containing column vectors, the coefficient of $X_1$ equals the coefficient of $\tilde{X}_1$ in the OLS fit of $Y$ or $\tilde{Y}$ on $\tilde{X}_1$, where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\tilde{Y}$ is the residual vector from the OLS fit of $Y$ on $\left(X_2, \ldots, X_p\right)$&lt;/li&gt;
&lt;li&gt;$\tilde{X}_1$ is the residual from the OLS fit of $X_1$ on $\left(X_2, \ldots, X_p\right)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;p&gt;Based on the FWL theorem, we can obtain $\tau_e$ in two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, we obtain the residual $\tilde{Z}$ from the OLS fit of $Z$ on ${1, e(X)}$;&lt;/li&gt;
&lt;li&gt;then, we obtain $\tau_e$ from the OLS fit of $Y$ on $\tilde{Z}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;proof-of-theorem-141&#34;&gt;Proof of Theorem 14.1&lt;/h3&gt;
&lt;p&gt;The coefficient of $e(X)$ in the OLS fit of $Z$ on $\{1, e(X)\}$ is
$$
\begin{aligned}
\frac{\operatorname{cov}\{Z, e(X)\}}{\operatorname{var}\{e(X)\}} &amp;amp; =\frac{E[\operatorname{cov}\{Z, e(X) \mid X\}]+\operatorname{cov}\{E(Z \mid X), e(X)\}}{\operatorname{var}\{e(X)\}} \\ &amp;amp;=\frac{0+\operatorname{var}\{e(X)\}}{\operatorname{var}\{e(X)\}}=1,
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the intercept is $E(Z)-E\{e(X)\}=0$&lt;/li&gt;
&lt;li&gt;the residual is $\tilde{Z}=Z-e(X)$ (This makes sense since $Z-e(X)$ is uncorrelated with any function of $X$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, we can obtain $\tau_e$ from the univariate OLS fit of $Y$ on $Z-e(X)$ :
$$\small{\tau_e=\frac{\operatorname{cov}\{Z-e(X), Y\}}{\operatorname{var}\{Z-e(X)\}}}$$
The denominator simplifies to
$$
\begin{aligned}
\operatorname{var}\{Z-e(X)\} &amp;amp; =E\{Z-e(X)\}^2 =e(X)+e(X)^2-2 e(X)^2=h_{\mathrm{O}}(X)
\end{aligned}
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;proof-of-theorem-141-1&#34;&gt;Proof of Theorem 14.1&lt;/h3&gt;
&lt;p&gt;The numerator simplifies to
$$
\begin{aligned}
&amp;amp; \operatorname{cov}\{Z-e(X), Y\} \\
= &amp;amp; E[\{Z-e(X)\} Y] \\
= &amp;amp; E[\{Z-e(X)\} Z Y(1)]+E[\{Z-e(X)\}(1-Z) Y(0)] \\
&amp;amp; \quad \quad \quad{\color{red}(\text { since } Y=Z Y(1)+(1-Z) Y(0))} \\
= &amp;amp; E[\{Z-Z e(X)\} Y(1)]-E[e(X)(1-Z) Y(0)] \\
= &amp;amp; E[Z\{1-e(X)\} Y(1)]-E[e(X)(1-Z) Y(0)] \\
= &amp;amp; E\left[e(X)\{1-e(X)\} \mu_1(X)\right]-E\left[e(X)\{1-e(X)\} \mu_0(X)\right] \\
&amp;amp; \quad \quad \quad\text { {\color{red}(tower property and ignorability)} } \\
= &amp;amp; E\left\{h_{\mathrm{O}}(X) \tau(X)\right\} .
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From the proof of Theorem 14.1, we can simply run the OLS of $Y$ on the centered treatment $\tilde{Z} = Z - e(X)$.&lt;/li&gt;
&lt;li&gt;Moreover, we can also include $X$ in the OLS fit which may improve efficiency in finite sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;comments-of-theorem-141-and-corollary-141&#34;&gt;Comments of Theorem 14.1 and Corollary 14.1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Theorem 14.1 motivates a two-step estimator for $\tau_{\mathrm{O}}$:
&lt;ul&gt;
&lt;li&gt;first, fit a propensity score model to obtain $\hat{e}\left(X_i\right)$;&lt;/li&gt;
&lt;li&gt;second, run OLS of $Y_i$ on $\left(1, X_i, \hat{e}\left(X_i\right)\right)$ to obtain the coefficient of $Z_i$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Corollary 14.1 motivates another two-step estimator for $\tau_{\mathrm{O}}$:
&lt;ul&gt;
&lt;li&gt;first, fit a propensity score model to obtain $\hat{e}\left(X_i\right)$;&lt;/li&gt;
&lt;li&gt;second, run OLS of $Y_i$ on $Z_i-\hat{e}\left(X_i\right)$ to obtain the coefficient of $Z_i$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;: OLS is convenient for obtaining point estimators, the corresponding standard errors are incorrect due to &lt;mark&gt;the uncertainty in the first step estimation of the propensity score&lt;/mark&gt;. We can use the bootstrap to approximate the standard errors.&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h4 id=&#34;regressions-weighted-by-the-inverse-of-the-propensity-score&#34;&gt;Regressions weighted by the inverse of the propensity score&lt;/h4&gt;
&lt;p&gt;We first re-examine the Hajek estimator of $\tau$ :
$$
\hat{\tau}^{\text {hajek }}=\frac{\sum_{i=1}^n \frac{Z_i Y_i}{\hat{e}\left(X_i\right)}}{\sum_{i=1}^n \frac{Z_i}{\hat{e}\left(X_i\right)}}-\frac{\sum_{i=1}^n \frac{\left(1-Z_i\right) Y_i}{1-\hat{e}\left(X_i\right)}}{\sum_{i=1}^n \frac{1-Z_i}{1-\hat{e}\left(X_i\right)}},
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;which equals the difference between the weighted means of the outcomes in the treatment and control groups.&lt;/li&gt;
&lt;li&gt;Numerically, it is identical to the coefficient of $Z_i$ in the following weighted least squares (WLS) of $Y_i$ on $\left(1, Z_i\right)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;mark&gt;Proposition 14.1&lt;/mark&gt; $\hat{\tau}^{\text {hajek }}$ equals $\hat{\beta}$ from the following $WLS$ :&lt;/p&gt;
&lt;p&gt;$$
(\hat{\alpha}, \hat{\beta})=\arg \min_{\alpha, \beta} \sum_{i=1}^n w_i\left(Y_i-\alpha-\beta Z_i\right)^2
$$&lt;/p&gt;
&lt;p&gt;with weights
$$
w_i=\frac{Z_i}{\hat{e}\left(X_i\right)}+\frac{1-Z_i}{1-\hat{e}\left(X_i\right)}= \begin{cases}\frac{1}{\hat{e}\left(X_i\right)} &amp;amp; \text { if } Z_i=1 \\ \frac{1}{1-\hat{e}\left(X_i\right)} &amp;amp; \text { if } Z_i=0\end{cases}
$$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;average-causal-effect&#34;&gt;Average causal effect&lt;/h3&gt;
&lt;ul style=&#34;color: black&#34;&gt;
&lt;li&gt;By Proposition 14.1, it is convenient to obtain $\hat{\tau}^{\text {hajek }}$ based on WLS.&lt;/li&gt;
&lt;li&gt;However, due to the uncertainty in the estimated propensity score, the standard error &lt;mark&gt;reported by WLS is incorrect&lt;/mark&gt; for the true standard error $\Rightarrow$ &lt;strong&gt;bootstrap&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul style=&#34;color: red&#34;&gt;
&lt;li&gt;Why does the WLS give a consistent estimator for $\tau$ ?&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Recall that in the CRE with a constant propensity score, we can simply use the coefficient of $Z_i$ in the OLS fit of $Y_i$ on $(1, Z_i)$ to estimate $\tau$.&lt;/li&gt;
&lt;li&gt;In observational studies, units have different probabilities of receiving the treatment and control, respectively.&lt;/li&gt;
&lt;li&gt;If we weight the treated units by $1 / e(X_i)$ and the control units by $1 /\{1-e(X_i)\}$, then they can represent the whole population and we effectively have &lt;strong&gt;a pseudo randomized experiment&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Consequently, the difference between the weighted means are consistent for $\tau$.&lt;/li&gt;
&lt;li&gt;The numerical equivalence of $\hat{\tau}^{\text {hajek }}$ and WLS is not only a fun numerical fact itself but also useful for motivation more complex estimator with covariate adjustment.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;an-extension-to-a-more-complex-estimator&#34;&gt;An extension to a more complex estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In the CRE, we can use the coefficient of $Z_i$ in the OLS fit of $Y_i$ on $(1, Z_i, X_i, Z_i X_i)$ to estimate $\tau$, where the covariates are centered with $\bar{X}=0$.&lt;/li&gt;
&lt;li&gt;This is Lin (2013)&amp;rsquo;s estimator which uses covariates to improve efficiency.&lt;/li&gt;
&lt;li&gt;A natural extension to observational studies is to estimate $\tau$ using the coefficient of $Z_i$ in the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ with weights defined in (14.1).&lt;/li&gt;
&lt;li&gt;If the linear models
$$
E(Y \mid Z=1, X)=\beta_{10}+\beta_{1 x}^{\top} X, \quad E(Y \mid Z=0, X)=\beta_{00}+\beta_{0 x}^{\top} X,
$$
are correctly specified, then &lt;mark&gt;both OLS and WLS give consistent estimators for the coefficients&lt;/mark&gt; and the estimators of the coefficient of $Z$ is consistent for $\tau$.&lt;/li&gt;
&lt;li&gt;More interestingly, the estimator of the coefficient of $Z$ based on WLS is also consistent for $\tau$ if &lt;strong&gt;the propensity score model is correct and the outcome model is incorrect&lt;/strong&gt;. $\Rightarrow$ the estimator based on WLS is &lt;strong&gt;doubly robust&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;a-doubly-robust-estimator&#34;&gt;A doubly robust estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Let $\hat{e}(X_i)$ be the fitted propensity score and $(\mu_1(X_i, \hat{\beta}_1), \mu_0(X_i, \hat{\beta}_0))$ be the fitted values of the outcome means based on the WLS.&lt;/li&gt;
&lt;li&gt;The outcome regression estimator is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{wls}}^{\mathrm{reg}}= \frac{1}{n}\sum_{i=1}^n\mu_1\left(X_i, \hat{\beta}_1\right)-\frac{1}{n} \sum_{i=1}^n \mu_0\left(X_i, \hat{\beta}_0\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The doubly robust estimator for $\tau$ is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{wls}}^{\mathrm{reg}}+\frac{1}{n} \sum_{i=1}^n \frac{Z_i\left\{Y_i-\mu_1\left(X_i, \hat{\beta}_1\right)\right\}}{\hat{e}\left(X_i\right)}-\frac{1}{n} \sum_{i=1}^n \frac{\left(1-Z_i\right)\left\{Y_i-\mu_0\left(X_i, \hat{\beta}_0\right)\right\}}{1-\hat{e}\left(X_i\right)} .
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An interesting result is that this doubly robust estimator equals the outcome regression estimator, which reduces to the coefficient of $Z_i$ in the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ if we use weights (14.1).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;theorem-142&#34;&gt;Theorem 14.2&lt;/h3&gt;
&lt;p style=&#34;color: blue&#34;&gt;If $\bar{X}=0$ and $(\mu_1(X_i, \hat{\beta}_1), \mu_0(X_i, \hat{\beta}_0))=(\hat{\beta}_{10}+\hat{\beta}_{1 x}^{\top} X_i, \hat{\beta}_{00}+\hat{\beta}_{0 x}^{\top} X_i)$ based on the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ with weights (14.1), then&lt;/p&gt;
&lt;p style=&#34;color: blue&#34;&gt;$$
\hat{\tau}_{\mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{wls}}^{\mathrm{reg}}=\hat{\beta}_{10}-\hat{\beta}_{00},
$$&lt;/p&gt;
&lt;p style=&#34;color: blue&#34;&gt;which is the coefficient of $Z_i$ in the WLS fit.&lt;/p&gt;
&lt;p&gt;$$$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Freedman and Berk (2008) showed that when the outcome model is correct, the WLS estimator is worse than the OLS estimator&lt;/li&gt;
&lt;li&gt;When the errors have variance proportional to the inverse of the propensity scores, the WLS estimator will be more effcient than the OLS estimator.&lt;/li&gt;
&lt;li&gt;The estimated standard error based on the WLS fit is not consistent for the true standard error because it &lt;strong&gt;ignores the uncertainty in the estimated propensity score&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;This can be easily fixed by using the bootstrap to approximate the variance of the WLS estimator.&lt;/li&gt;
&lt;li&gt;Nevertheless, they found that &amp;ldquo;weighting may help under some circumstances&amp;rdquo; because when the outcome model is incorrect, the
WLS estimator is still consistent if the propensity score model is correct.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;average-causal-effect-on-the-treated-units&#34;&gt;Average causal effect on the treated units&lt;/h3&gt;
&lt;p&gt;&lt;mark&gt;Proposition 14.2&lt;/mark&gt; $\hat{\tau}_{\mathrm{T}}^{\text {hajek }}$ is numerically identical to $\hat{\beta}$ in the following WLS:
$$
(\hat{\alpha}, \hat{\beta})=\arg \min_{\alpha, \beta} \sum_{i=1}^n w_{\mathrm{T} i}\left(Y_i-\alpha-\beta Z_i\right)^2
$$
with weights
$$
w_{\mathrm{T} i}=Z_i+\left(1-Z_i\right) \hat{o}\left(X_i\right)= \begin{cases}1 &amp;amp; \text { if } Z_i=1 \\ \hat{o}\left(X_i\right) &amp;amp; \text { if } Z_i=0\end{cases}
$$
where $\hat{o}\left(X_i\right)=\hat{e}\left(X_i\right) /\{1-\hat{e}\left(X_i\right)\}$&lt;/p&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;regression-estimators&#34;&gt;Regression estimators&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;CRE&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;unconfounded observational studies&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;without $X$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim Z_i$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim Z_i$ with weights $w_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;with $X$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim\left(Z_i, X_i, Z_i X_i\right)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$Y_i \sim\left(Z_i, X_i, Z_i X_i\right)$ with weights $w_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;body1.jpg&#34;
  &gt;

&lt;h3 id=&#34;average-causal-effect-on-the-treated-units-1&#34;&gt;Average causal effect on the treated units&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If we center covariates with $\hat{X}(1)=0$, then we can estimate $\tau_{\mathrm{T}}$ using the coefficient of $Z_i$ in the WLS fit of $Y_i$ on $\left(1, Z_i, X_i, Z_i X_i\right)$ with weights defined in (14.2). Similarly, this estimator equals the regression estimator&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{reg}}=\hat{\bar{Y}}(1)-\frac{1}{n_1} \sum_{i=1}^n Z_i \mu_0\left(X_i, \hat{\beta}_0\right)
$$&lt;/p&gt;
&lt;p&gt;which also equals the doubly robust estimator&lt;/p&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{reg}}-\frac{1}{n_1} \sum_{i=1}^n \hat{o}\left(X_i\right)\left(1-Z_i\right)\left\{Y_i-\mu_0\left(X_i, \hat{\beta}_0\right)\right\}.
$$&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Theorem 14.3&lt;/mark&gt; If $\hat{\bar{X}}(1)=0$ and $\mu_0(X_i, \hat{\beta}_0)=\hat{\beta}_{00}+\hat{\beta}_{0x}^{\top} X_i$ based on the WLS fit of $Y_i$ on $(1, Z_i, X_i, Z_i X_i)$ with weights (14.2), then&lt;/p&gt;
&lt;p&gt;$$
\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{dr}}=\hat{\tau}_{\mathrm{T}, \mathrm{wls}}^{\mathrm{reg}}=\hat{\beta}_{10}-\hat{\beta}_{00},
$$&lt;/p&gt;
&lt;p&gt;which is the coefficient of $Z_i$ in the $W L S$ fit.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Distributed Community Detection Algorithm for Large Scale Networks Under Stochastic Block Models</title>
      <link>https://ikerlz.github.io/publication/dcd/</link>
      <pubDate>Sat, 27 May 2023 00:00:00 +0000</pubDate>
      <guid>https://ikerlz.github.io/publication/dcd/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
  </channel>
</rss>
