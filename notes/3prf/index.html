<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: December 16, 2023 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.6" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.32e2e32cf1a4c1ea152e519f8b1fda79.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/github-dark.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="Li Zhe" />





  

<meta name="description" content="Table of Contents 1. Introduction 2. The three-pass regression filter 2.1. The estimator 2.2. Assumptions 2.3. Consistency 2.4. Asymptotic distributions 2.5. Proxy selection 3. Related procedures 3.1. Constrained least squares 3." />



<link rel="alternate" hreflang="en-us" href="https://ikerlz.github.io/notes/3prf/" />
<link rel="canonical" href="https://ikerlz.github.io/notes/3prf/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu9ccd2acdcd774e20fa34966445b706a8_6997380_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu9ccd2acdcd774e20fa34966445b706a8_6997380_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  






<meta property="twitter:card" content="summary_large_image" />

  <meta property="twitter:site" content="@wowchemy" />
  <meta property="twitter:creator" content="@wowchemy" />
<meta property="twitter:image" content="https://ikerlz.github.io/notes/3prf/featured.jpg" />



  

<meta property="og:type" content="website" />
<meta property="og:site_name" content="Zhe Li" />
<meta property="og:url" content="https://ikerlz.github.io/notes/3prf/" />
<meta property="og:title" content="Notes on &#39;&#39;The three-pass regression filter: A new approach to forecasting using many predictors&#39;&#39; | Zhe Li" />
<meta property="og:description" content="Table of Contents 1. Introduction 2. The three-pass regression filter 2.1. The estimator 2.2. Assumptions 2.3. Consistency 2.4. Asymptotic distributions 2.5. Proxy selection 3. Related procedures 3.1. Constrained least squares 3." /><meta property="og:image" content="https://ikerlz.github.io/notes/3prf/featured.jpg" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2023-11-20T22:16:45&#43;08:00"
    />
  
  
    <meta property="article:modified_time" content="2023-11-20T22:16:45&#43;08:00">
  







  




  
  
  

  
  

  


  
  <title>Notes on &#39;&#39;The three-pass regression filter: A new approach to forecasting using many predictors&#39;&#39; | Zhe Li</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="cb83c79cd79d6d0dabc1fa683061dca9" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.3a6bdbdff5d8a89d6e651adb3deec035.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Zhe Li</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Zhe Li</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#slides"><span>Slides</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/uploads/resume.pdf"><span>CV</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  






















  
  



<div class="article-container pt-3">
  <h1>Notes on &#39;&#39;The three-pass regression filter: A new approach to forecasting using many predictors&#39;&#39;</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Nov 20, 2023
  </span>
  

  

  

  
  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/factor-model/">factor-model</a></span>
  

</div>

  





</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 437px;">
  <div style="position: relative">
    <img src="/notes/3prf/featured_hu589ef1c3cf94117706fc6f31fee1d989_111854_ee3a439ea5520bee617ba125cc18afef.webp" width="720" height="437" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      

<details class="toc-inpage d-print-none  " open>
  <summary class="font-weight-bold">Table of Contents</summary>
  <nav id="TableOfContents">
  <ul>
    <li><a href="#1-introduction">1. Introduction</a></li>
    <li><a href="#2-the-three-pass-regression-filter">2. The three-pass regression filter</a>
      <ul>
        <li><a href="#21-the-estimator">2.1. The estimator</a></li>
        <li><a href="#22-assumptions">2.2. Assumptions</a></li>
        <li><a href="#23-consistency">2.3. Consistency</a></li>
        <li><a href="#24-asymptotic-distributions">2.4. Asymptotic distributions</a></li>
        <li><a href="#25-proxy-selection">2.5. Proxy selection</a></li>
      </ul>
    </li>
    <li><a href="#3-related-procedures">3. Related procedures</a>
      <ul>
        <li><a href="#31-constrained-least-squares">3.1. Constrained least squares</a></li>
        <li><a href="#32-partial-least-squares">3.2. Partial least squares</a></li>
      </ul>
    </li>
    <li><a href="#4-empirical-evidence">4. Empirical evidence</a>
      <ul>
        <li><a href="#41-forecasting-macroeconomic-aggregates">4.1. Forecasting macroeconomic aggregates</a></li>
        <li><a href="#42-forecasting-market-returns">4.2. Forecasting market returns</a></li>
      </ul>
    </li>
  </ul>
</nav>
</details>

<hr>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="" srcset="
               /media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_4521b0390b02ac01acfcf8802e497a33.webp 400w,
               /media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_a6ae2965e4ec4df5bfad2235d8225cf9.webp 760w,
               /media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
               src="/media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_4521b0390b02ac01acfcf8802e497a33.webp"
               width="760"
               height="296"
               loading="lazy" data-zoomable /></div>
  </div></figure>

<hr>
<h2 id="1-introduction">1. Introduction</h2>
<ul>
<li>
<p>How to use the vast predictive information to forecast important economic aggregates like national product or stock market value.</p>
<ul>
<li>If the predictors number near or more than the number of observations, the standard OLS forecaster is known to be poorly behaved or nonexistent (Huber, 1973)</li>
<li>A economic view: data are generated from a model in which <strong>latent factors</strong> drive the systematic variation of <mark>both the forecast target, $\boldsymbol{y}$, and the matrix of predictors, $\boldsymbol{X}$</mark> $\Rightarrow$ <font color="red">the best prediction of $\boldsymbol{y}$ is infeasible</font> since the factors are unobserved $\Rightarrow$ require a factor estimation step</li>
<li>A benchmark method: extract factors that are significant drivers of variation in $\boldsymbol{X}$ and then uses these to forecast $\boldsymbol{y}$.</li>
</ul>
</li>
<li>
<p><strong>Motivations</strong></p>
<ul>
<li>the factors that are relevant to $\boldsymbol{y}$ may be a strict subset of all the factors driving $\boldsymbol{X}$</li>
<li>The method, called the <font color="red">three-pass regression filter</font> (3PRF), selectively identifies only the subset of factors that <mark>influence the forecast target</mark> while discarding factors that <mark>are irrelevant for the target but that may be pervasive among predictors</mark></li>
<li>The 3PRF has the advantage of being expressed in closed form and virtually instantaneous to compute.</li>
</ul>
</li>
<li>
<p><strong>Contributions</strong></p>
<ol>
<li>develop asymptotic theory for the 3PRF</li>
<li>verify the finite sample accuracy of the asymptotic theory</li>
<li>compare the 3PRF to other methods</li>
<li>provide empirical support for the 3PRF&rsquo;s strong forecasting performance</li>
</ol>
</li>
</ul>
<hr>
<h2 id="2-the-three-pass-regression-filter">2. The three-pass regression filter</h2>
<h3 id="21-the-estimator">2.1. The estimator</h3>
<details class="spoiler "  id="spoiler-2">
  <summary><strong>The environment for 3PRF</strong></summary>
  <p><ul>
<li>There is <mark>a target variable</mark> which we wish to forecast.</li>
<li>There exist many predictors which may contain information useful for predicting the target variable.
<ul>
<li>The number of predictors <font color="red">$N$ may be large and number near or more than the available time series observations $T$</font>, which makes OLS problematic.</li>
</ul>
</li>
<li>Therefore we look to reduce the dimension of predictive information $\Rightarrow$ <font color="red">assume the data can be described by an approximate factor model.</font></li>
<li>In order to make forecasts, the 3PRF uses <strong>proxies</strong>:
<ul>
<li>These are variables, driven by the factors (and as we emphasize below, driven by target-relevant factors in particular), which we show are always available from
<ul>
<li>the target and predictors themselves</li>
<li>the econometrician on the basis of economic theory.</li>
</ul>
</li>
</ul>
</li>
<li>The target is a linear function of a subset of the latent factors plus some unforecastable noise.</li>
<li>The optimal forecast therefore comes from a regression on the true underlying relevant factors. However, since these factors are unobservable, we call this the <font color="red">infeasible best forecast</font>.</li>
</ul>
</p>
</details>
<p>$$
\boldsymbol{y} = \boldsymbol{Z} \boldsymbol{\beta} + \boldsymbol{\epsilon}
$$</p>
<ul>
<li>$\boldsymbol{y}\in\mathbb{R}^{T \times 1}$: the target variable time series from $2,3, \ldots, T+1$</li>
<li>$\boldsymbol{X}\in\mathbb{R}^{T \times N}$: the predictors that have been standardized to have unit time series variance.
<ul>
<li>Temporal dimension: $\boldsymbol{X}=\left(\boldsymbol{x}_1^{\prime}, \boldsymbol{x}_2^{\prime}, \ldots, \boldsymbol{x}_T^{\prime}\right)^{\prime}$</li>
<li>Cross section: $\boldsymbol{X}=\left(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\right)$</li>
</ul>
</li>
<li>$\boldsymbol{Z}\in\mathbb{R}^{T \times L}$: stacks period-by-period <strong>proxy data</strong> as $\boldsymbol{Z}=\left(\boldsymbol{z}_1^{\prime}, \boldsymbol{z}_2^{\prime}, \ldots, \boldsymbol{z}_T^{\prime}\right)^{\prime}$</li>
<li>Make no assumption on the relationship between $N$ and $T$ but assume <font color="red">$L \ll \min (N, T)$</font></li>
</ul>
<hr>
<p><strong>Construct the 3PRF</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center">Pass</th>
<th style="text-align:center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1.</td>
<td style="text-align:center">Run <mark>time series regression</mark> of $\mathbf{x}_i$ on $\boldsymbol{Z}$ for $i=1, \ldots, N$,</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">$x_{i, t}=\phi_{0, i}+\boldsymbol{z}^{\prime} \boldsymbol{\phi}_i+\epsilon_{i t}$, retain slope estimate $\hat{\boldsymbol{\phi}}_i$.</td>
</tr>
<tr>
<td style="text-align:center">2.</td>
<td style="text-align:center">Run <mark>cross section regression</mark> of $\boldsymbol{x}_t$ on $\hat{\boldsymbol{\phi}}_i$ for $t=1, \ldots, T$,</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">$x_{i, t}=\phi_{0, t}+\hat{\boldsymbol{\phi}}^{\prime} \boldsymbol{F}_t+\varepsilon_{i t}$, retain slope estimate $\hat{\boldsymbol{F}}_t$.</td>
</tr>
<tr>
<td style="text-align:center">3.</td>
<td style="text-align:center">Run <mark>time series regression</mark> of $y_{t+1}$ on predictive factors $\hat{\boldsymbol{F}}_t$,</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">$y_{t+1}=\beta_0+\hat{\boldsymbol{F}}^{\prime} \boldsymbol{\beta}+\eta_{t+1}$, delivers forecast $\hat{y}_{t+1}$.</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Pass 1</strong> : the estimated coefficients describe the sensitivity of the predictor to factors represented by the proxies.</li>
<li><strong>Pass 2</strong> : first-stage coefficient estimates map the cross-sectional distribution of predictors to the latent factors. Second-stage cross section regressions use this map to back out estimates of the factors at each point in time.</li>
<li><strong>Pass 3</strong> : This is a single time series forecasting regression of the target variable $y_{t +1}$ on the second-pass estimated predictive factors $\hat{\boldsymbol{F}}_t$.  The third-pass fitted value $\beta_0+\hat{\boldsymbol{F}_t}^{\prime} \boldsymbol{\beta}$ is the 3PRF time $t$ forecast</li>
</ul>
<div class="alert alert-note">
  <div>
    <p><strong>An one-step closed form:</strong></p>
<p>$$
{\color{red}{\hat{\boldsymbol{y}}=\boldsymbol{\iota}_T \bar{y}+\boldsymbol{J}_T \boldsymbol{X} \boldsymbol{W}_{X Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{S}_{X X} \boldsymbol{W}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X y}}}
$$</p>
<ul>
<li>$\boldsymbol{J}_T \equiv \boldsymbol{I}_T-\frac{1}{T} \boldsymbol{\iota}_T \boldsymbol{\iota}_T^{\prime}$
<ul>
<li>$\boldsymbol{I}_T$: the $T$-dimensional identity matrix</li>
<li>$\boldsymbol{\iota}_T$: the $T$-vector of ones $\left(\boldsymbol{J}_N\right.$ is analogous)</li>
</ul>
</li>
<li>$\bar{y}=\boldsymbol{\iota}_T^{\prime} \boldsymbol{y} / T, \boldsymbol{W}_{X Z} \equiv$ $\boldsymbol{J}_{\boldsymbol{N}} \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{Z}, \boldsymbol{S}_{X X} \equiv \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{X}$ and $\boldsymbol{s}_{X \boldsymbol{y}} \equiv \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{y}$.</li>
</ul>
  </div>
</div>
<p><strong>Two advantages:</strong></p>
<ul>
<li>First, in practice (particularly with many predictors) one often faces unbalanced panels and missing data.</li>
<li>Second, it is useful for developing intuition behind the procedure and for understanding its relation to partial least squares.</li>
</ul>
<details class="spoiler "  id="spoiler-4">
  <summary><strong>Two interpretations</strong></summary>
  <p><ul>
<li>Rewrite the forecast as</li>
</ul>
<p>$$
\begin{aligned}
&amp; \hat{\boldsymbol{y}}=\boldsymbol{\iota}_T \bar{y}+\hat{\boldsymbol{F}} \hat{\boldsymbol{\beta}} \\
&amp; \hat{\boldsymbol{F}}^{\prime}=\boldsymbol{S}_{Z Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{X}^{\prime}, \\
&amp; \hat{\boldsymbol{\beta}}=\boldsymbol{S}_{Z Z} \boldsymbol{W}_{X Z} \boldsymbol{s}_{X Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X X} \boldsymbol{W}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X y}
\end{aligned}
$$
where $\boldsymbol{S}_{X Z} \equiv \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{Z}$.</p>
<ul>
<li>$\hat{\boldsymbol{F}}$ is the <mark>predictive factor</mark> and $\hat{\boldsymbol{\beta}}$ is <mark>the predictive coefficient</mark> on that factor.</li>
</ul>
<hr>
<ul>
<li>Also rewrite the forecast as</li>
</ul>
<p>$$
\begin{aligned}
&amp; \hat{\boldsymbol{y}}=\boldsymbol{\iota} \bar{y}+\boldsymbol{J}_T \boldsymbol{X} \hat{\boldsymbol{\alpha}} \\
&amp; \hat{\boldsymbol{\alpha}}=\boldsymbol{W}_{X Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X X} \boldsymbol{W}_{X Z}^{\prime}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X y}
\end{aligned}
$$</p>
<ul>
<li>$\hat{\alpha}$ is the predictive coefficient on individual predictors.</li>
<li>The regular OLS estimate of the projection coefficient $\boldsymbol{\alpha}$ is $\left(\boldsymbol{S}_{X X}\right)^{-1} \boldsymbol{s}_{X y}$.</li>
<li>This representation suggests that our approach can be interpreted as <mark>a constrained version of least squares</mark>.</li>
</ul>
</p>
</details>
<h3 id="22-assumptions">2.2. Assumptions</h3>
<details class="spoiler "  id="spoiler-5">
  <summary><strong>Necessary assumptions</strong></summary>
  <p><ul>
<li><font color="red"><strong>Assumption 1 (Factor Structure)</strong></font>. The data are generated by the following:
$$
\begin{array}{l}
\boldsymbol{x}_t=\boldsymbol{\phi}_0+\boldsymbol{\Phi} \boldsymbol{F}_t+\boldsymbol{\varepsilon}_t;\\
y_{t+1}=\beta_0+\boldsymbol{\beta}^{\prime} \boldsymbol{F}_t+\eta_{t+1};\\
\boldsymbol{z}_t=\boldsymbol{\lambda}_0+\boldsymbol{\Lambda} \boldsymbol{F}_t+\boldsymbol{\omega}_t \\
\boldsymbol{X}=\boldsymbol{\iota} \boldsymbol{\phi}_0^{\prime}+\boldsymbol{F} \boldsymbol{\Phi}^{\prime}+\boldsymbol{\varepsilon};\\
\boldsymbol{y}=\boldsymbol{\iota} \beta_0+\boldsymbol{F} \boldsymbol{\beta}+\boldsymbol{\eta};\\
\boldsymbol{Z}=\boldsymbol{\iota} \boldsymbol{\lambda}_0^{\prime}+\boldsymbol{F} \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\omega}
\end{array}
$$
where $\boldsymbol{F}_t=\left(\boldsymbol{f}_t^{\prime}, \mathbf{g}_t^{\prime}\right)^{\prime}, \boldsymbol{\Phi}=\left(\boldsymbol{\Phi}_f, \boldsymbol{\Phi}_g\right), \boldsymbol{\Lambda}=\left(\boldsymbol{\Lambda}_f, \boldsymbol{\Lambda}_g\right)$, and $\boldsymbol{\beta}=$ $\left(\boldsymbol{\beta}_f^{\prime}, \mathbf{0}^{\prime}\right)^{\prime}$ with $\left|\boldsymbol{\beta}_f\right|&gt;$ 0. $K_f&gt;0$ is the dimension of vector $\boldsymbol{f}_t$, $K_g \geq 0$ is the dimension of vector $g_t, L$ is the dimension of vector $z_t(0&lt;L&lt;\min (N, T))$, and $K=K_f+K_g$.</li>
</ul>
<blockquote>
<ul>
<li>The target&rsquo;s factor loadings $\big(\boldsymbol{\beta}=(\boldsymbol{\beta}_f^{\prime}, \mathbf{0}^{\prime})^{\prime}\big)$ allow the target to depend on a strict subset of the factors driving the predictors.</li>
<li>We refer to this subset as the relevant factors, which are denoted $\boldsymbol{f}_t$.</li>
<li>In contrast, irrelevant factors, $\mathbf{g}_t$, do not influence the forecast target but may drive the cross section of predictive information $\boldsymbol{x}_t$.</li>
<li>The proxies $\boldsymbol{z}_t$ are driven by factors and proxy noise.</li>
</ul>
</blockquote>
<ul>
<li>
<p><strong>Assumption 2 (Factors, Loadings and Residuals)</strong>. Let $M&lt;\infty$. For any $i, s, t$</p>
<ol>
<li>$\mathbb{E}\left|\boldsymbol{F}_t\right|^4&lt;M, T^{-1} \sum_{s=1}^T \boldsymbol{F}_s \underset{T \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{\mu}$ and $T^{-1} \boldsymbol{F}^{\prime} \boldsymbol{J}_T \boldsymbol{F} \underset{T \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{\Delta}_F$</li>
<li>$\mathbb{E}\left|\boldsymbol{\phi}_i\right|^4 \leq M, N^{-1} \sum_{j=1}^N \boldsymbol{\phi}_j \underset{T \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \overline{\boldsymbol{\phi}}, N^{-1} \boldsymbol{\Phi}^{\prime} \boldsymbol{J}_N \boldsymbol{\Phi} \underset{N \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \mathcal{P}$ and $N^{-1} \boldsymbol{\Phi}^{\prime} \boldsymbol{J}_N \boldsymbol{\phi}_0 \underset{N \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{P}_1^6$</li>
<li>$\mathbb{E}\left(\varepsilon_{i t}\right)=0, \mathbb{E}\left|\varepsilon_{i t}\right|^8 \leq M$</li>
<li>$\mathbb{E}\left(\boldsymbol{\omega}_t\right)=\mathbf{0}, \mathbb{E}\left|\boldsymbol{\omega}_t\right|^4 \leq M, T^{-1 / 2} \sum_{s=1}^T \boldsymbol{\omega}_s=\boldsymbol{o}_p(1)$ and $T^{-1} \boldsymbol{\omega}^{\prime} \mathbf{J}_T \boldsymbol{\omega} \underset{N \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{\Delta}_\omega$</li>
<li>$\mathbb{E}_t\left(\eta_{t+1}\right)=\mathbb{E}\left(\eta_{t+1} \mid y_t, F_t, y_{t-1}, F_{t-1}, \ldots\right)=0, \mathbb{E}\left(\eta_{t+1}^4\right) \leq M$, and $\eta_{t+1}$ is independent of $\phi_i(m)$ and $\varepsilon_{i, t}$.</li>
</ol>
</li>
<li>
<p><strong>Assumption 3 (Dependence)</strong>. Let $x(m)$ denote the $m$ th element of $\boldsymbol{x}$. For $M&lt;\infty$ and any $i, j, t, s, m_1, m_2$</p>
<ol>
<li>$\mathbb{E}\left(\varepsilon_{i t} \varepsilon_{j s}\right)=\sigma_{i j, t s},\left|\sigma_{i j, t s}\right| \leq \bar{\sigma}_{i j}$ and $\left|\sigma_{i j, t s}\right| \leq \tau_{t s}$, and</li>
</ol>
<ul>
<li>$N^{-1} \sum_{i, j=1}^N \bar{\sigma}_{i j} \leq M$</li>
<li>$T^{-1} \sum_{t, s=1}^T \tau_{t s} \leq M$</li>
<li>$N^{-1} \sum_{i, s}\left|\sigma_{i i, t s}\right| \leq M$</li>
<li>$N^{-1} T^{-1} \sum_{i, j, t, s}\left|\sigma_{i j, t s}\right| \leq M$</li>
</ul>
<ol start="2">
<li>$\mathbb{E}\left|N^{-1 / 2} T^{-1 / 2} \sum_{s=1}^T \sum_{i=1}^N\left[\varepsilon_{i s} \varepsilon_{i t}-\mathbb{E}\left(\varepsilon_{i s} \varepsilon_{i t}\right)\right]\right|^2 \leq M$</li>
<li>$\mathbb{E}\left|T^{-1 / 2} \sum_{t=1}^T F_t\left(m_1\right) \omega_t\left(m_2\right)\right|^2 \leq M$</li>
<li>$\mathbb{E}\left|T^{-1 / 2} \sum_{t=1}^T \omega_t\left(m_1\right) \varepsilon_{i t}\right|^2 \leq M$.</li>
</ol>
</li>
<li>
<p><strong>Assumption 4 (Central Limit Theorems)</strong>. For any $i, t$</p>
<ol>
<li>$N^{-1 / 2} \sum_{i=1}^N \phi_i \varepsilon_{i t} \stackrel{d}{\rightarrow} \mathcal{N}\left(0, \Gamma_{\Phi_{\varepsilon}}\right)$, where $\Gamma_{\Phi_{\varepsilon}}=\operatorname{plim}_{N \rightarrow \infty} N^{-1}$ $\sum_{i, j=1}^N \mathbb{E}\left[\boldsymbol{\phi}_i \boldsymbol{\phi}_j^{\prime} \varepsilon_{i t} \varepsilon_{j t}\right]$</li>
<li>$T^{-1 / 2} \sum_{t=1}^T \boldsymbol{F}_t \eta_{t+1} \stackrel{d}{\rightarrow} \mathcal{N}\left(0, \Gamma_{F \eta}\right)$, where $\boldsymbol{\Gamma}_{F \eta}=\operatorname{plim}_{T \rightarrow \infty} T^{-1}$ $\sum_{t=1}^T \mathbb{E}\left[\eta_{t+1}^2 \boldsymbol{F}_t \boldsymbol{F}_t^{\prime}\right]&gt;0$</li>
<li>$T^{-1 / 2} \sum_{t=1}^T \boldsymbol{F}_t \varepsilon_{i t} \stackrel{d}{\rightarrow} \mathcal{N}\left(0, \boldsymbol{\Gamma}_{F \varepsilon, i}\right)$, where $\boldsymbol{\Gamma}_{F \varepsilon, i}=\operatorname{plim}_{T \rightarrow \infty} T^{-1}$ $\sum_{t, s=1}^T \mathbb{E}\left[\boldsymbol{F}_t \boldsymbol{F}_s^{\prime} \varepsilon_{i t} \varepsilon_{i s}\right]&gt;0$.</li>
</ol>
</li>
<li>
<p><strong>Assumption 5 (Normalization)</strong>. $\mathcal{P}=\mathbf{I}, \boldsymbol{P}_1=\mathbf{0}$ and $\boldsymbol{\Delta}_F$ is diagonal, positive definite, and each diagonal element is unique.</p>
</li>
<li>
<p><strong>Assumption 6 (Relevant Proxies)</strong>. $\boldsymbol{\Lambda}=\left[\boldsymbol{\Lambda}_f, \mathbf{0}\right]$ and $\boldsymbol{\Lambda}_f$ is nonsingular.</p>
</li>
</ul>
</p>
</details>
<h3 id="23-consistency">2.3. Consistency</h3>
<p><font color="red"><strong>Theorem 1.</strong></font> Let Assumptions 1-6 hold. The three-pass regression filter forecast is consistent for the infeasible best forecast,
$$\hat{y}_{t+1} \underset{T, N \rightarrow \infty}{\stackrel{p}{\longrightarrow}}\beta_0+\boldsymbol{F}_t^{\prime} \boldsymbol{\beta}.$$</p>
<p><font color="red"><strong>Theorem 2.</strong></font> Let $\hat{\alpha}_i$ denote the $i$th element of $\hat{\boldsymbol{\alpha}}$, and let Assumptions 1-6 hold. Then for any $i$,
$$
N \hat{\alpha}_i \underset{T, N \rightarrow \infty}{\stackrel{p}{\longrightarrow}}\left(\boldsymbol{\phi}_i-\overline{\boldsymbol{\phi}}\right)^{\prime} \boldsymbol{\beta} .
$$</p>
<ul>
<li>3PRF uses only as many predictive factors as the number of factors relevant to $y_{t+1}$</li>
<li>the PCR forecast is asymptotically efficient when there are as many predictive factors as the total number of factors driving $\boldsymbol{x}_t$</li>
<li>if the factors driving the target are weak in the sense that they contribute a only small fraction of the total variability in the predictors, <strong>then principal components may have difficulty identifying them</strong>.</li>
</ul>
<p><font color="blue"><strong>Corollary 1.</strong></font> Let Assumptions 1–5 hold with the exception of Assumptions 2.4, 3.3 and 3.4. Additionally, assume that there is only one relevant factor. Then the target-proxy three-pass regression filter forecaster is consistent for the infeasible best forecast.</p>
<ul>
<li>Corollary 1 holds regardless of the number of irrelevant factors driving $\boldsymbol{X}$ and regardless of where the relevant factor stands in the principal component ordering for $\boldsymbol{X}$.</li>
<li>Compare this to PCR, whose first predictive factor is ensured to be the one that explains most of the covariance among $\boldsymbol{x}_t$, regardless of that factor&rsquo;s relationship to $y_{t+1}$.</li>
<li>Only if the relevant factor happens to also drive most of the variation within the predictors does the first component achieve the infeasible best.</li>
<li>$\Rightarrow$ the forecast performance of the 3PRF is robust to the presence of irrelevant factors.</li>
</ul>
<h3 id="24-asymptotic-distributions">2.4. Asymptotic distributions</h3>
<p><font color="red"><strong>Theorem 3.</strong></font> Under Assumptions 1-6, as $N, T \rightarrow \infty$ we have
$$
\frac{\sqrt{T} N\left(\hat{\alpha}_i-\tilde{\alpha}_i\right)}{A_i} \stackrel{d}{\rightarrow} \mathcal{N}(0,1)
$$
<details class="spoiler "  id="spoiler-6">
  <summary>where</summary>
  <p>$A_i^2$ is the ith diagonal element of $\widehat{\operatorname{Avar}}(\hat{\boldsymbol{\alpha}})=\boldsymbol{\Omega}_\alpha\left(\frac{1}{T} \sum_t \hat{\eta}_{t+1}^2\left(\boldsymbol{X}_t\right.\right.$ $\left.-\overline{\boldsymbol{X}})\left(\boldsymbol{X}_t-\overline{\boldsymbol{X}}\right)^{\prime}\right) \boldsymbol{\Omega}_\alpha^{\prime}, \hat{\eta}_{t+1}$ is the estimated 3PRF forecast error, $\tilde{\alpha}_i \equiv$ $\boldsymbol{S}_i \boldsymbol{G}_\alpha \boldsymbol{\beta}$, where $\boldsymbol{S}_i$ is selects the ith element of vector $\boldsymbol{G}_\alpha \boldsymbol{\beta}$ and
$$
\begin{aligned}
\boldsymbol{G}_\alpha&amp;=\boldsymbol{J}_N\left(T^{-1} \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{Z}\right)\left(T^{-3} N^{-2} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{S}_{X X} \boldsymbol{W}_{X Z}\right)^{-1}\times\\
&amp;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\left(N^{-1} T^{-2} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{F}\right),
\end{aligned}
$$
and
$$
\boldsymbol{\Omega}_\alpha=\boldsymbol{J}_N\left(\frac{1}{T} \boldsymbol{S}_{X Z}\right)\left(\frac{1}{T^3 N^2} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{S}_{X X} \boldsymbol{W}_{X Z}\right)^{-1}\left(\frac{1}{T N} \boldsymbol{W}_{X Z}^{\prime}\right)
$$</p>
</details></p>
<p><font color="red"><strong>Theorem 4.</strong></font> Under Assumptions $1-6$, as $N, T \rightarrow \infty$ we have
$$
\frac{\sqrt{T}\left(\hat{y}_{t+1}-\mathbb{E}_t y_{t+1}\right)}{Q_t} \stackrel{d}{\rightarrow} \mathcal{N}(0,1)
$$
where $\mathbb{E}_t y_{t+1}=\beta_0+\boldsymbol{\beta}^{\prime} \boldsymbol{F}_t$ and $Q_t^2$ is the th diagonal element of $\frac{1}{N^2} J_T \boldsymbol{X} \widehat{\operatorname{Avar}}(\hat{\boldsymbol{\alpha}}) \boldsymbol{X}^{\prime} \boldsymbol{J}_T$</p>
<p><font color="red"><strong>Theorem 5.</strong></font> Under Assumptions $1-6$, as $N, T \rightarrow \infty$ we have
$$
\sqrt{T}\left(\hat{\boldsymbol{\beta}}-\boldsymbol{G}_\beta \boldsymbol{\beta}\right) \stackrel{d}{\rightarrow} \mathcal{N}\left(\mathbf{0}, \boldsymbol{\Sigma}_\beta\right)
$$
where $\boldsymbol{\Sigma}_\beta=\boldsymbol{\Sigma}_z^{-1} \boldsymbol{\Gamma}_{F \eta} \boldsymbol{\Sigma}_z^{-1}$ and $\boldsymbol{\Sigma}_z=\mathbf{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\Delta}_\omega$. Furthermore,
$$
\begin{aligned}
\widehat{\operatorname{Avar}}(\hat{\boldsymbol{\beta}})= &amp; \left(T^{-1} \hat{\boldsymbol{F}}^{\prime} \boldsymbol{J}_T \hat{\boldsymbol{F}}\right)^{-1} T^{-1} \sum_t \hat{\eta}_{t+1}^2\left(\hat{\boldsymbol{F}}_t-\hat{\boldsymbol{\mu}}\right)\left(\hat{\boldsymbol{F}}_t-\hat{\boldsymbol{\mu}}\right)^{\prime} \
&amp; \times\left(T^{-1} \hat{\boldsymbol{F}}^{\prime} \boldsymbol{J}_T \hat{\boldsymbol{F}}\right)^{-1}
\end{aligned}
$$
is a consistent estimator of $\boldsymbol{\Sigma}_\beta$. $\boldsymbol{G}_\beta$ is defined in the Appendix.</p>
<p><font color="red"><strong>Theorem 6.</strong></font> Under Assumptions 1-6, as $N, T \rightarrow \infty$ we have for every $t$</p>
<ul>
<li>if $\sqrt{N} / T \rightarrow 0$, then
$$
\sqrt{N}\left[\hat{\boldsymbol{F}}_t-\left(\boldsymbol{H}_0+\boldsymbol{H} \boldsymbol{F}_t\right)\right] \stackrel{d}{\rightarrow} \mathcal{N}\left(\mathbf{0}, \boldsymbol{\Sigma}_F\right)
$$</li>
<li>if $\liminf \sqrt{N} / T \geq \tau \geq 0$, then
$$
T\left[\hat{\boldsymbol{F}}_t-\left(\boldsymbol{H}_0+\boldsymbol{H F}_t\right)\right]=\boldsymbol{O}_p(1)
$$
where $\boldsymbol{\Sigma}_F=\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\Delta}_\omega\right)\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F^2 \boldsymbol{\Lambda}^{\prime}\right)^{-1} \boldsymbol{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Gamma}_{\Phi \varepsilon} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F^2\right.$ $\left.\boldsymbol{\Lambda}^{\prime}\right)^{-1}\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\Delta}_\omega\right) . \boldsymbol{H}_0$ and $\boldsymbol{H}$ are defined in the Appendix.</li>
</ul>
<h3 id="25-proxy-selection">2.5. Proxy selection</h3>
<p><strong>How to select the proxies that depend only on relevant factors ?</strong></p>
<h4 id="251-automatic-proxies">2.5.1. Automatic proxies</h4>
<ul>
<li>Initialize $\boldsymbol{r}_0=\boldsymbol{y}$.</li>
<li>For $k=1, \ldots, L$ :
<ul>
<li>Define the $k$ th automatic proxy to be $\boldsymbol{r}_{k-1}$. Stop if $k=L$; otherwise proceed.</li>
<li>Compute the 3PRF for target $\boldsymbol{y}$ using cross section $\boldsymbol{X}$ using statistical proxies 1 through $k$. Denote the resulting forecast $\hat{\boldsymbol{y}}_k$.</li>
<li>Calculate $\boldsymbol{r}_k=\boldsymbol{y}-\hat{\boldsymbol{y}}_k$, advance $k$, and go to step 1.</li>
</ul>
</li>
</ul>
<p><font color="red"><strong>Theorem 7.</strong></font> Let Assumptions 1-5 hold with the exception of Assumptions 2.4, 3.3 and 3.4. Then the L-automatic-proxy three pass regression filter forecaster of $\boldsymbol{y}$ automatically satisfies Assumptions 2.4, 3.3, 3.4 and 6 when $L=K_f$. As a result, <mark>the $L$ automatic-proxy is consistent and asymptotically normal according to Theorems 1 and 4.</mark></p>
<h4 id="252-theory-proxies">2.5.2. Theory proxies</h4>
<ul>
<li>The filter may instead be employed using alternative disciplining variables (factor proxies) which may <mark>be distinct from the target and chosen on the basis of economic theory or by statistical arguments</mark>.</li>
<li>Consider a situation in which $K_f$ is one, so that the target and proxy are given by $y_{t+1}=\beta_0+\beta f_t+\eta_{t+1}$ and $z_t=\lambda_0+\Lambda f_t+\omega_t$.</li>
<li>Also suppose that the population $R^2$ of the proxy equation is substantially higher than the population $R^2$ of the target equation.
<ul>
<li>The forecasts from using either $z_t$ or the target as proxy are asymptotically identical.</li>
<li>However, in finite samples, forecasts can be improved by proxying with $z_t$ due to its higher signal-to-noise ratio.</li>
</ul>
</li>
</ul>
<h4 id="252-information-criteria">2.5.2 Information criteria</h4>
<p>&lsquo;&lsquo;Trace of the Krylov Representation&rsquo;&rsquo; method of Kramer and Sugiyama (2011).</p>
<p>$$
\begin{aligned}
&amp; \widehat{\operatorname{DoF}}(m)=1+\sum_{j=1}^m c_j \operatorname{trace}\left(\boldsymbol{K}^j\right)-\sum_{l, j=1}^m \boldsymbol{t}_l^{\prime} \mathbf{K}^j \boldsymbol{t}_l \\
&amp; ~~~~~~~~~~~~~~~~~~~~~~+\left(\boldsymbol{y}-\hat{\boldsymbol{y}}_m\right)^{\prime} \sum_{j=1}^m \boldsymbol{K}^j \boldsymbol{v}_j+m
\end{aligned}
$$
where $\boldsymbol{K}=\boldsymbol{X} \boldsymbol{X}^{\prime}, c_j$ are elements of the vector $\boldsymbol{c}=\boldsymbol{B}^{-1} \boldsymbol{T} \boldsymbol{y}, \boldsymbol{B}$ is a Krylov basis decomposition, $\boldsymbol{T}$ is the matrix of PLS factor estimate vectors $\boldsymbol{t}_j$, and $\boldsymbol{v}_j$ are columns of the matrix $\boldsymbol{T}\left(\boldsymbol{B}^{-1}\right)^{\prime}$. The BIC is then calculated as
$$\sum_t\left(y_t-\hat{y}_{m, t}\right)^2 / T+\log (T) \hat{\sigma}^2 \widehat{D o F}(m) / T$$
where $\hat{\sigma}=\sqrt{\sum_t\left(y_t-\hat{y}_{m, t}\right)^2 /(T-D o F(m))}$.</p>
<hr>
<h2 id="3-related-procedures">3. Related procedures</h2>
<h3 id="31-constrained-least-squares">3.1. Constrained least squares</h3>
<p><font color="red"><strong>Theorem 8.</strong></font> The three-pass regression filter&rsquo;s implied $N$-dimensional predictive coefficient, $\hat{\alpha}$, is the solution to
$$
\begin{aligned}
&amp; \arg \min _{\alpha_0, \boldsymbol{\alpha}}\left\|\boldsymbol{y}-\alpha_0-\boldsymbol{X} \boldsymbol{\alpha}\right\| \\
&amp; \text { subject to } \quad\left(\boldsymbol{I}-\boldsymbol{W}_{X Z}\left(\boldsymbol{S}_{X Z}^{\prime} \boldsymbol{W}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}\right) \boldsymbol{\alpha}=\mathbf{0} .\quad (5)
\end{aligned}
$$</p>
<ul>
<li>The 3PRF&rsquo;s answer is to impose the constraint in Eq. (5), which exploits the proxies and has an intuitive interpretation.</li>
<li>Premultiplying both sides of the equation by $\boldsymbol{J}_T \boldsymbol{X}$, we can rewrite the constraint as $\left(\boldsymbol{J}_T \boldsymbol{X}-\boldsymbol{J}_T \hat{\boldsymbol{F}} \hat{\boldsymbol{\Phi}}^{\prime}\right) \boldsymbol{\alpha}=$ 0. For large $N$ and $T$,
$$
\boldsymbol{J}_T \boldsymbol{X}-\boldsymbol{J}_T \hat{\boldsymbol{F}} \hat{\boldsymbol{\Phi}}^{\prime} \approx \boldsymbol{\varepsilon}+(\boldsymbol{F}-\boldsymbol{\mu})\left(\boldsymbol{I}-\boldsymbol{S}_{K_f}\right) \boldsymbol{\Phi}^{\prime}
$$</li>
<li>Because the covariance between $\boldsymbol{\alpha}$ and $\varepsilon$ is zero by the assumptions of the model, the constraint simply imposes that <mark>the product of $\alpha$ and the target-irrelevant common component of $\boldsymbol{X}$ is equal to zero</mark>.</li>
<li>This is because the matrix $\boldsymbol{I}-\boldsymbol{S}_{K_f}$ selects only the terms in the total common component $\boldsymbol{F} \boldsymbol{\Phi}^{\prime}$ that <mark>are associated with irrelevant factors</mark>.</li>
<li>This constraint is important because it ensures that <mark>factors irrelevant to $\boldsymbol{y}$ drop out of the 3PRF forecast</mark>. It also ensures that $\hat{\boldsymbol{\alpha}}$ is consistent for the factor model&rsquo;s population projection coefficient of $y_{t+1}$ on $\boldsymbol{x}_t$.</li>
</ul>
<h3 id="32-partial-least-squares">3.2. Partial least squares</h3>
<details class="spoiler "  id="spoiler-7">
  <summary>PLS</summary>
  <p><p>See <a href="https://aarmey.github.io/ml-for-bioe/public/Wk4-Lecture8.pdf" target="_blank" rel="noopener">reference1</a> and <a href="https://personal.utdallas.edu/~herve/Abdi-PLS-pretty.pdf" target="_blank" rel="noopener">reference2</a></p>
<ul>
<li>The goal of PLS regression is to predict $\mathbf{Y}$ from $\mathbf{X}$ and to <mark>describe their common structure</mark>.</li>
<li>When $\mathbf{Y}$ is a vector and $\mathbf{X}$ is full rank, this goal could be accomplished using <strong>ORDINARY MULTIPLE REGRESSION</strong>.</li>
<li>When the number of predictors is large compared to the number of observations, $\mathbf{X}$ is likely to be singular and the regression approach is no longer feasible (i.e., because of MULTICOLLINEARITY).</li>
<li>Several approaches have been developed to cope with this problem.
<ul>
<li>One approach is to eliminate some predictors (e.g., using stepwise methods)</li>
<li>Another one, called principal component regression (PCR): perform a PRINCIPAL COMPONENT ANALYSIS (PCA) of the $\mathbf{X}$ matrix and then use the principal components of $\mathbf{X}$ as regressors on $\mathbf{Y}$.</li>
</ul>
</li>
<li>How to choose an optimum subset of predictors ? A possible strategy is to keep only a few of the first components.
<ul>
<li>They are chosen to explain $\mathbf{X}$ rather than $\mathbf{Y}$, and so, <font color="red">nothing guarantees that the principal components, which &ldquo;explain&rdquo; $\mathbf{X}$, are relevant for $\mathbf{Y}$.</font></li>
</ul>
</li>
<li>By contrast, PLS regression finds components from $\mathbf{X}$ that are also relevant for $\mathbf{Y}$.</li>
<li>Specifically, PLS regression searches for a set of components (called latent vectors) that performs a simultaneous decomposition of $\mathbf{X}$ and $\mathbf{Y}$ with <mark>the constraint that these components explain as much as possible of the covariance between $\mathbf{X}$ and $\mathbf{Y}$</mark>. This step generalizes PCA.</li>
<li>It is followed by a regression step where the decomposition of $\mathbf{X}$ is used to predict $\mathbf{Y}$.</li>
</ul>
<hr>
<ul>
<li>partial least squares (PLS) constructs forecasting indices as linear combinations of the underlying predictors.</li>
<li>These predictive indices are referred to as &ldquo;directions&rdquo; in the language of PLS.</li>
<li>The PLS forecast based on the first $K$ PLS directions, $\hat{\boldsymbol{y}}^{(k)}$, is constructed according to the following algorithm (as stated in Hastie et al. (2009)):
<ol>
<li>Standardize each $\mathbf{x}_i$ to have mean zero and variance one by setting $\tilde{\mathbf{x}}_i=\frac{\mathbf{x}_i-\hat{\mathbb{E}}\left[\mathrm{x}_{i t}\right]}{\hat{\sigma}\left(\mathrm{x}_{i t}\right)}, i=1, \ldots, N$</li>
<li>Set $\hat{\boldsymbol{y}}^{(0)}=\bar{y}$, and $\mathbf{x}_i^{(0)}=\tilde{\mathbf{x}}_i, i=1, \ldots, N$</li>
<li>For $k=1,2, \ldots, K$</li>
</ol>
<ul>
<li>$\boldsymbol{u}_k=\sum_{i=1}^N \hat{\phi}_{k i} \mathbf{x}_i^{(k-1)}$, where $\hat{\phi}_{k i}=\widehat{\operatorname{Cov}}\left(\mathbf{x}_i^{(k-1)}, \boldsymbol{y}\right)$</li>
<li>$\hat{\beta}_k=\widehat{\operatorname{Cov}}\left(\boldsymbol{u}_k, \boldsymbol{y}\right) / \widehat{\operatorname{Var}}\left(\boldsymbol{u}_k\right)$</li>
<li>$\hat{\boldsymbol{y}}^{(k)}=\hat{\boldsymbol{y}}^{(k-1)}+\hat{\beta}_k \boldsymbol{u}_k$</li>
<li>Orthogonalize each $\mathbf{x}_i^{(k-1)}$ with respect to $\boldsymbol{u}_k$ :
$$
\begin{aligned}
\mathbf{x}_i^{(k)} &amp; =\mathbf{x}_i^{(k-1)}-\left(\widehat{\operatorname{Cov}}\left(\boldsymbol{u}_k, \mathbf{x}_i^{(k-1)}\right) / \widehat{\operatorname{Var}}\left(\boldsymbol{u}_k\right)\right) \boldsymbol{u}_k, \\
i &amp; =1,2, \ldots, N .
\end{aligned}
$$</li>
</ul>
</li>
</ul>
</p>
</details>
<ul>
<li>partial least squares forecasts are identical to those from the 3PRF when
<ol>
<li>the predictors are demeaned and variance-standardized in a preliminary step</li>
<li>the first two regression passes are run without constant terms</li>
<li>proxies are automatically selected.</li>
</ol>
</li>
</ul>
<p>Consider the case where a single predictive index is constructed from the partial least squares algorithm.</p>
<ul>
<li>Assume, for the time being, that each predictor has been previously standardized to have mean zero and variance one. Following the construction of the PLS forecast given above, we have</li>
</ul>
<ol>
<li>Set $\hat{\phi}_i=x_i^{\prime} y$, and $\hat{\boldsymbol{\Phi}}=\left(\hat{\phi}_1, \ldots, \hat{\phi}_N\right)^{\prime}$.</li>
<li>Set $\hat{u}_t=\boldsymbol{x}_t^{\prime} \hat{\Phi}$, and $\hat{\boldsymbol{u}}=\left(\hat{u}_1, \ldots, \hat{u}_T\right)^{\prime}$.</li>
<li>Run a predictive regression of $\boldsymbol{y}$ on $\hat{\boldsymbol{u}}$.</li>
</ol>
<ul>
<li>Constructing the forecast in this manner may be represented as a one-step estimator
$$
\hat{\boldsymbol{y}}^{\mathrm{PLS}}=\boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{y}\left(\boldsymbol{y}^{\prime} \boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{y}\right)^{-1} \boldsymbol{y}^{\prime} \boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{y}
$$</li>
<li><font color="red">which upon inspection is identical to the 1-automatic-proxy 3PRF forecast when constants are omitted from the first and second passes. </font></li>
</ul>
<hr>
<h2 id="4-empirical-evidence">4. Empirical evidence</h2>
<h3 id="41-forecasting-macroeconomic-aggregates">4.1. Forecasting macroeconomic aggregates</h3>
<ul>
<li>Quarterly data from Stock and Watson (2012) for the sample 1959:I-2009:IV</li>
<li>Take as our predictors a set of 108 macroeconomic variables compiled by Stock and Watson (2012)</li>
<li>Out-of-sample $R^2$ of one quarter ahead forecasts, in percentage.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">3PRF1</th>
<th style="text-align:center">PCR1</th>
<th style="text-align:center">PCLAR</th>
<th style="text-align:center">PCLAS</th>
<th style="text-align:center">10LAR</th>
<th style="text-align:center">FA1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">GDP</td>
<td style="text-align:center">30.12</td>
<td style="text-align:center">$35.18^{\mathrm{a}}$</td>
<td style="text-align:center">29.70</td>
<td style="text-align:center">29.51</td>
<td style="text-align:center">26.38</td>
<td style="text-align:center">20.11</td>
</tr>
<tr>
<td style="text-align:center">Consumption</td>
<td style="text-align:center">$23.20^{\mathrm{a},{ }^*}$</td>
<td style="text-align:center">7.06</td>
<td style="text-align:center">9.12</td>
<td style="text-align:center">7.32</td>
<td style="text-align:center">-14.85</td>
<td style="text-align:center">2.72</td>
</tr>
<tr>
<td style="text-align:center">Investment</td>
<td style="text-align:center">$38.88^{\mathrm{a}}$</td>
<td style="text-align:center">37.37</td>
<td style="text-align:center">36.81</td>
<td style="text-align:center">36.30</td>
<td style="text-align:center">24.01</td>
<td style="text-align:center">34.35</td>
</tr>
<tr>
<td style="text-align:center">Exports</td>
<td style="text-align:center">$16.75^{\mathrm{a}}$</td>
<td style="text-align:center">13.25</td>
<td style="text-align:center">-11.58</td>
<td style="text-align:center">-9.42</td>
<td style="text-align:center">-61.36</td>
<td style="text-align:center">16.44</td>
</tr>
<tr>
<td style="text-align:center">Imports</td>
<td style="text-align:center">$37.18^a$</td>
<td style="text-align:center">36.50</td>
<td style="text-align:center">18.46</td>
<td style="text-align:center">16.93</td>
<td style="text-align:center">22.77</td>
<td style="text-align:center">36.58</td>
</tr>
<tr>
<td style="text-align:center">Industrial Production</td>
<td style="text-align:center">$16.56^{\mathrm{a},{ }^*}$</td>
<td style="text-align:center">8.92</td>
<td style="text-align:center">5.67</td>
<td style="text-align:center">5.71</td>
<td style="text-align:center">11.04</td>
<td style="text-align:center">12.04</td>
</tr>
<tr>
<td style="text-align:center">Capacity Utilization</td>
<td style="text-align:center">54.32</td>
<td style="text-align:center">54.79</td>
<td style="text-align:center">53.77</td>
<td style="text-align:center">54.85</td>
<td style="text-align:center">$64.69^{a,{}^*}$</td>
<td style="text-align:center">55.58</td>
</tr>
<tr>
<td style="text-align:center">Total Hours</td>
<td style="text-align:center">$53.81^{\mathrm{a}}$</td>
<td style="text-align:center">50.47</td>
<td style="text-align:center">48.58</td>
<td style="text-align:center">47.39</td>
<td style="text-align:center">39.56</td>
<td style="text-align:center">42.53</td>
</tr>
<tr>
<td style="text-align:center">Total Employment</td>
<td style="text-align:center">$48.84^a$</td>
<td style="text-align:center">47.27</td>
<td style="text-align:center">38.14</td>
<td style="text-align:center">37.16</td>
<td style="text-align:center">18.91</td>
<td style="text-align:center">41.73</td>
</tr>
<tr>
<td style="text-align:center">Average Hours</td>
<td style="text-align:center">$20.12^{\mathrm{a}}$</td>
<td style="text-align:center">10.12</td>
<td style="text-align:center">18.52</td>
<td style="text-align:center">13.89</td>
<td style="text-align:center">17.55</td>
<td style="text-align:center">15.84</td>
</tr>
<tr>
<td style="text-align:center">Housing Starts</td>
<td style="text-align:center">26.97</td>
<td style="text-align:center">-0.14</td>
<td style="text-align:center">31.54</td>
<td style="text-align:center">29.66</td>
<td style="text-align:center">$46.89^a$</td>
<td style="text-align:center">0.13</td>
</tr>
<tr>
<td style="text-align:center">GDP Inflation</td>
<td style="text-align:center">0.64</td>
<td style="text-align:center">2.05</td>
<td style="text-align:center">-0.94</td>
<td style="text-align:center">1.38</td>
<td style="text-align:center">-5.89</td>
<td style="text-align:center">$2.80^{\mathrm{a}}$</td>
</tr>
<tr>
<td style="text-align:center">PCE Inflation</td>
<td style="text-align:center">-1.29</td>
<td style="text-align:center">-3.73</td>
<td style="text-align:center">10.60</td>
<td style="text-align:center">9.82</td>
<td style="text-align:center">$12.22^{\mathrm{a}}$</td>
<td style="text-align:center">-2.50</td>
</tr>
</tbody>
</table>
<h3 id="42-forecasting-market-returns">4.2. Forecasting market returns</h3>
<ul>
<li>Building from the present value identity, Kelly and Pruitt (2013) map the cross section of price–dividend ratios into the approximate latent factor model of Assumption 1, and argue that <mark>this set of predictors should possess forecasting power for log returns on the aggregate market.</mark></li>
<li>We estimate the extent of market return predictability using 25 log price-dividend ratios of portfolios sorted by market equity and book-to-market ratio.</li>
<li>The data is annual over the post-war period 1945-2010 (following Fama and French (1992)).</li>
<li>We assume that the predictors take
<ul>
<li>the form $p d_{i, t}=\phi_{i, 0}+\boldsymbol{\phi}_i^{\prime} \boldsymbol{F}_t+\varepsilon_{i, t}$</li>
<li>the target takes the form $r_{t+1}=\beta_0^r+\boldsymbol{F}_t^{\prime} \boldsymbol{\beta}^r+\eta_{t+1}^r$.</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">3PRF1</th>
<th style="text-align:center">3PRF2</th>
<th style="text-align:center">3PRF-IC</th>
<th style="text-align:center">PC1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Return $R^2$ <br> # of factors</td>
<td style="text-align:center">27.63</td>
<td style="text-align:center">$36.34^{\mathrm{a}}$</td>
<td style="text-align:center">31.15 <br> 1.36</td>
<td style="text-align:center">-10.45</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"><strong>PC2</strong></td>
<td style="text-align:center"><strong>PC-IC</strong></td>
<td style="text-align:center"><strong>10LAR</strong></td>
<td style="text-align:center"><strong>FA1</strong></td>
</tr>
<tr>
<td style="text-align:center">Return $R^2$ <br> # of factors</td>
<td style="text-align:center">-8.89</td>
<td style="text-align:center">27.00 <br> 4.58</td>
<td style="text-align:center">13.28</td>
<td style="text-align:center">-10.08</td>
</tr>
</tbody>
</table>

    </div>

    







<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fikerlz.github.io%2Fnotes%2F3prf%2F&amp;text=Notes&#43;on&#43;%27%27The&#43;three-pass&#43;regression&#43;filter%3A&#43;A&#43;new&#43;approach&#43;to&#43;forecasting&#43;using&#43;many&#43;predictors%27%27" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fikerlz.github.io%2Fnotes%2F3prf%2F&amp;t=Notes&#43;on&#43;%27%27The&#43;three-pass&#43;regression&#43;filter%3A&#43;A&#43;new&#43;approach&#43;to&#43;forecasting&#43;using&#43;many&#43;predictors%27%27" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=Notes%20on%20%27%27The%20three-pass%20regression%20filter%3A%20A%20new%20approach%20to%20forecasting%20using%20many%20predictors%27%27&amp;body=https%3A%2F%2Fikerlz.github.io%2Fnotes%2F3prf%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fikerlz.github.io%2Fnotes%2F3prf%2F&amp;title=Notes&#43;on&#43;%27%27The&#43;three-pass&#43;regression&#43;filter%3A&#43;A&#43;new&#43;approach&#43;to&#43;forecasting&#43;using&#43;many&#43;predictors%27%27" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=Notes&#43;on&#43;%27%27The&#43;three-pass&#43;regression&#43;filter%3A&#43;A&#43;new&#43;approach&#43;to&#43;forecasting&#43;using&#43;many&#43;predictors%27%27%20https%3A%2F%2Fikerlz.github.io%2Fnotes%2F3prf%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fikerlz.github.io%2Fnotes%2F3prf%2F&amp;title=Notes&#43;on&#43;%27%27The&#43;three-pass&#43;regression&#43;filter%3A&#43;A&#43;new&#43;approach&#43;to&#43;forecasting&#43;using&#43;many&#43;predictors%27%27" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://ikerlz.github.io/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu9ccd2acdcd774e20fa34966445b706a8_6997380_270x270_fill_lanczos_center_3.png" alt="Li Zhe"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://ikerlz.github.io/">Li Zhe</a></h5>
      <h6 class="card-subtitle">PhD Student</h6>
      <p class="card-text">My research interests include distributed statistical modelling &amp; inference, network data modelling.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/Ikerlz" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=vnHn7QkAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/uploads/resume.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2023 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.938a3a7554cd9f6602290411f64d2617.js"></script>




  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.62586ca65ca61821fe707eb9fa6268b7.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.9137013a66774049159934c29c3f0205.js" type="module"></script>


















</body>
</html>
