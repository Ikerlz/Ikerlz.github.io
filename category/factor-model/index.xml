<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>factor-model | Zhe Li</title>
    <link>https://ikerlz.github.io/category/factor-model/</link>
      <atom:link href="https://ikerlz.github.io/category/factor-model/index.xml" rel="self" type="application/rss+xml" />
    <description>factor-model</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 20 Nov 2023 22:16:45 +0800</lastBuildDate>
    <image>
      <url>https://ikerlz.github.io/media/icon_hu9ccd2acdcd774e20fa34966445b706a8_6997380_512x512_fill_lanczos_center_3.png</url>
      <title>factor-model</title>
      <link>https://ikerlz.github.io/category/factor-model/</link>
    </image>
    
    <item>
      <title>Notes on &#39;&#39;The three-pass regression filter: A new approach to forecasting using many predictors&#39;&#39;</title>
      <link>https://ikerlz.github.io/notes/3prf/</link>
      <pubDate>Mon, 20 Nov 2023 22:16:45 +0800</pubDate>
      <guid>https://ikerlz.github.io/notes/3prf/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-the-three-pass-regression-filter&#34;&gt;2. The three-pass regression filter&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#21-the-estimator&#34;&gt;2.1. The estimator&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#22-assumptions&#34;&gt;2.2. Assumptions&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#23-consistency&#34;&gt;2.3. Consistency&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#24-asymptotic-distributions&#34;&gt;2.4. Asymptotic distributions&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#25-proxy-selection&#34;&gt;2.5. Proxy selection&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-related-procedures&#34;&gt;3. Related procedures&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#31-constrained-least-squares&#34;&gt;3.1. Constrained least squares&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#32-partial-least-squares&#34;&gt;3.2. Partial least squares&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-empirical-evidence&#34;&gt;4. Empirical evidence&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#41-forecasting-macroeconomic-aggregates&#34;&gt;4.1. Forecasting macroeconomic aggregates&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#42-forecasting-market-returns&#34;&gt;4.2. Forecasting market returns&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;hr&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_4521b0390b02ac01acfcf8802e497a33.webp 400w,
               /media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_a6ae2965e4ec4df5bfad2235d8225cf9.webp 760w,
               /media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/notes/3PRF/paper_intro_huf5f032f5e0ce25d4704501cb81607dc7_130609_4521b0390b02ac01acfcf8802e497a33.webp&#34;
               width=&#34;760&#34;
               height=&#34;296&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;How to use the vast predictive information to forecast important economic aggregates like national product or stock market value.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the predictors number near or more than the number of observations, the standard OLS forecaster is known to be poorly behaved or nonexistent (Huber, 1973)&lt;/li&gt;
&lt;li&gt;A economic view: data are generated from a model in which &lt;strong&gt;latent factors&lt;/strong&gt; drive the systematic variation of &lt;mark&gt;both the forecast target, $\boldsymbol{y}$, and the matrix of predictors, $\boldsymbol{X}$&lt;/mark&gt; $\Rightarrow$ &lt;font color=&#34;red&#34;&gt;the best prediction of $\boldsymbol{y}$ is infeasible&lt;/font&gt; since the factors are unobserved $\Rightarrow$ require a factor estimation step&lt;/li&gt;
&lt;li&gt;A benchmark method: extract factors that are significant drivers of variation in $\boldsymbol{X}$ and then uses these to forecast $\boldsymbol{y}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Motivations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the factors that are relevant to $\boldsymbol{y}$ may be a strict subset of all the factors driving $\boldsymbol{X}$&lt;/li&gt;
&lt;li&gt;The method, called the &lt;font color=&#34;red&#34;&gt;three-pass regression filter&lt;/font&gt; (3PRF), selectively identifies only the subset of factors that &lt;mark&gt;influence the forecast target&lt;/mark&gt; while discarding factors that &lt;mark&gt;are irrelevant for the target but that may be pervasive among predictors&lt;/mark&gt;&lt;/li&gt;
&lt;li&gt;The 3PRF has the advantage of being expressed in closed form and virtually instantaneous to compute.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Contributions&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;develop asymptotic theory for the 3PRF&lt;/li&gt;
&lt;li&gt;verify the finite sample accuracy of the asymptotic theory&lt;/li&gt;
&lt;li&gt;compare the 3PRF to other methods&lt;/li&gt;
&lt;li&gt;provide empirical support for the 3PRF&amp;rsquo;s strong forecasting performance&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-the-three-pass-regression-filter&#34;&gt;2. The three-pass regression filter&lt;/h2&gt;
&lt;h3 id=&#34;21-the-estimator&#34;&gt;2.1. The estimator&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;The environment for 3PRF&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;There is &lt;mark&gt;a target variable&lt;/mark&gt; which we wish to forecast.&lt;/li&gt;
&lt;li&gt;There exist many predictors which may contain information useful for predicting the target variable.
&lt;ul&gt;
&lt;li&gt;The number of predictors &lt;font color=&#34;red&#34;&gt;$N$ may be large and number near or more than the available time series observations $T$&lt;/font&gt;, which makes OLS problematic.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Therefore we look to reduce the dimension of predictive information $\Rightarrow$ &lt;font color=&#34;red&#34;&gt;assume the data can be described by an approximate factor model.&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;In order to make forecasts, the 3PRF uses &lt;strong&gt;proxies&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;These are variables, driven by the factors (and as we emphasize below, driven by target-relevant factors in particular), which we show are always available from
&lt;ul&gt;
&lt;li&gt;the target and predictors themselves&lt;/li&gt;
&lt;li&gt;the econometrician on the basis of economic theory.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The target is a linear function of a subset of the latent factors plus some unforecastable noise.&lt;/li&gt;
&lt;li&gt;The optimal forecast therefore comes from a regression on the true underlying relevant factors. However, since these factors are unobservable, we call this the &lt;font color=&#34;red&#34;&gt;infeasible best forecast&lt;/font&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;$$
\boldsymbol{y} = \boldsymbol{Z} \boldsymbol{\beta} + \boldsymbol{\epsilon}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol{y}\in\mathbb{R}^{T \times 1}$: the target variable time series from $2,3, \ldots, T+1$&lt;/li&gt;
&lt;li&gt;$\boldsymbol{X}\in\mathbb{R}^{T \times N}$: the predictors that have been standardized to have unit time series variance.
&lt;ul&gt;
&lt;li&gt;Temporal dimension: $\boldsymbol{X}=\left(\boldsymbol{x}_1^{\prime}, \boldsymbol{x}_2^{\prime}, \ldots, \boldsymbol{x}_T^{\prime}\right)^{\prime}$&lt;/li&gt;
&lt;li&gt;Cross section: $\boldsymbol{X}=\left(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\right)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\boldsymbol{Z}\in\mathbb{R}^{T \times L}$: stacks period-by-period &lt;strong&gt;proxy data&lt;/strong&gt; as $\boldsymbol{Z}=\left(\boldsymbol{z}_1^{\prime}, \boldsymbol{z}_2^{\prime}, \ldots, \boldsymbol{z}_T^{\prime}\right)^{\prime}$&lt;/li&gt;
&lt;li&gt;Make no assumption on the relationship between $N$ and $T$ but assume &lt;font color=&#34;red&#34;&gt;$L \ll \min (N, T)$&lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Construct the 3PRF&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Pass&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Run &lt;mark&gt;time series regression&lt;/mark&gt; of $\mathbf{x}_i$ on $\boldsymbol{Z}$ for $i=1, \ldots, N$,&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_{i, t}=\phi_{0, i}+\boldsymbol{z}^{\prime} \boldsymbol{\phi}_i+\epsilon_{i t}$, retain slope estimate $\hat{\boldsymbol{\phi}}_i$.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Run &lt;mark&gt;cross section regression&lt;/mark&gt; of $\boldsymbol{x}_t$ on $\hat{\boldsymbol{\phi}}_i$ for $t=1, \ldots, T$,&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_{i, t}=\phi_{0, t}+\hat{\boldsymbol{\phi}}^{\prime} \boldsymbol{F}_t+\varepsilon_{i t}$, retain slope estimate $\hat{\boldsymbol{F}}_t$.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3.&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Run &lt;mark&gt;time series regression&lt;/mark&gt; of $y_{t+1}$ on predictive factors $\hat{\boldsymbol{F}}_t$,&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$y_{t+1}=\beta_0+\hat{\boldsymbol{F}}^{\prime} \boldsymbol{\beta}+\eta_{t+1}$, delivers forecast $\hat{y}_{t+1}$.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pass 1&lt;/strong&gt; : the estimated coefficients describe the sensitivity of the predictor to factors represented by the proxies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pass 2&lt;/strong&gt; : first-stage coefficient estimates map the cross-sectional distribution of predictors to the latent factors. Second-stage cross section regressions use this map to back out estimates of the factors at each point in time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pass 3&lt;/strong&gt; : This is a single time series forecasting regression of the target variable $y_{t +1}$ on the second-pass estimated predictive factors $\hat{\boldsymbol{F}}_t$.  The third-pass fitted value $\beta_0+\hat{\boldsymbol{F}_t}^{\prime} \boldsymbol{\beta}$ is the 3PRF time $t$ forecast&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;An one-step closed form:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
{\color{red}{\hat{\boldsymbol{y}}=\boldsymbol{\iota}_T \bar{y}+\boldsymbol{J}_T \boldsymbol{X} \boldsymbol{W}_{X Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{S}_{X X} \boldsymbol{W}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X y}}}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol{J}_T \equiv \boldsymbol{I}_T-\frac{1}{T} \boldsymbol{\iota}_T \boldsymbol{\iota}_T^{\prime}$
&lt;ul&gt;
&lt;li&gt;$\boldsymbol{I}_T$: the $T$-dimensional identity matrix&lt;/li&gt;
&lt;li&gt;$\boldsymbol{\iota}_T$: the $T$-vector of ones $\left(\boldsymbol{J}_N\right.$ is analogous)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\bar{y}=\boldsymbol{\iota}_T^{\prime} \boldsymbol{y} / T, \boldsymbol{W}_{X Z} \equiv$ $\boldsymbol{J}_{\boldsymbol{N}} \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{Z}, \boldsymbol{S}_{X X} \equiv \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{X}$ and $\boldsymbol{s}_{X \boldsymbol{y}} \equiv \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{y}$.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Two advantages:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, in practice (particularly with many predictors) one often faces unbalanced panels and missing data.&lt;/li&gt;
&lt;li&gt;Second, it is useful for developing intuition behind the procedure and for understanding its relation to partial least squares.&lt;/li&gt;
&lt;/ul&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Two interpretations&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;Rewrite the forecast as&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; \hat{\boldsymbol{y}}=\boldsymbol{\iota}_T \bar{y}+\hat{\boldsymbol{F}} \hat{\boldsymbol{\beta}} \\
&amp;amp; \hat{\boldsymbol{F}}^{\prime}=\boldsymbol{S}_{Z Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{X}^{\prime}, \\
&amp;amp; \hat{\boldsymbol{\beta}}=\boldsymbol{S}_{Z Z} \boldsymbol{W}_{X Z} \boldsymbol{s}_{X Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X X} \boldsymbol{W}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X y}
\end{aligned}
$$
where $\boldsymbol{S}_{X Z} \equiv \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{Z}$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\boldsymbol{F}}$ is the &lt;mark&gt;predictive factor&lt;/mark&gt; and $\hat{\boldsymbol{\beta}}$ is &lt;mark&gt;the predictive coefficient&lt;/mark&gt; on that factor.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;Also rewrite the forecast as&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; \hat{\boldsymbol{y}}=\boldsymbol{\iota} \bar{y}+\boldsymbol{J}_T \boldsymbol{X} \hat{\boldsymbol{\alpha}} \\
&amp;amp; \hat{\boldsymbol{\alpha}}=\boldsymbol{W}_{X Z}\left(\boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X X} \boldsymbol{W}_{X Z}^{\prime}\right)^{-1} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{s}_{X y}
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\alpha}$ is the predictive coefficient on individual predictors.&lt;/li&gt;
&lt;li&gt;The regular OLS estimate of the projection coefficient $\boldsymbol{\alpha}$ is $\left(\boldsymbol{S}_{X X}\right)^{-1} \boldsymbol{s}_{X y}$.&lt;/li&gt;
&lt;li&gt;This representation suggests that our approach can be interpreted as &lt;mark&gt;a constrained version of least squares&lt;/mark&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;h3 id=&#34;22-assumptions&#34;&gt;2.2. Assumptions&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Necessary assumptions&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Assumption 1 (Factor Structure)&lt;/strong&gt;&lt;/font&gt;. The data are generated by the following:
$$
\begin{array}{l}
\boldsymbol{x}_t=\boldsymbol{\phi}_0+\boldsymbol{\Phi} \boldsymbol{F}_t+\boldsymbol{\varepsilon}_t;\\
y_{t+1}=\beta_0+\boldsymbol{\beta}^{\prime} \boldsymbol{F}_t+\eta_{t+1};\\
\boldsymbol{z}_t=\boldsymbol{\lambda}_0+\boldsymbol{\Lambda} \boldsymbol{F}_t+\boldsymbol{\omega}_t \\
\boldsymbol{X}=\boldsymbol{\iota} \boldsymbol{\phi}_0^{\prime}+\boldsymbol{F} \boldsymbol{\Phi}^{\prime}+\boldsymbol{\varepsilon};\\
\boldsymbol{y}=\boldsymbol{\iota} \beta_0+\boldsymbol{F} \boldsymbol{\beta}+\boldsymbol{\eta};\\
\boldsymbol{Z}=\boldsymbol{\iota} \boldsymbol{\lambda}_0^{\prime}+\boldsymbol{F} \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\omega}
\end{array}
$$
where $\boldsymbol{F}_t=\left(\boldsymbol{f}_t^{\prime}, \mathbf{g}_t^{\prime}\right)^{\prime}, \boldsymbol{\Phi}=\left(\boldsymbol{\Phi}_f, \boldsymbol{\Phi}_g\right), \boldsymbol{\Lambda}=\left(\boldsymbol{\Lambda}_f, \boldsymbol{\Lambda}_g\right)$, and $\boldsymbol{\beta}=$ $\left(\boldsymbol{\beta}_f^{\prime}, \mathbf{0}^{\prime}\right)^{\prime}$ with $\left|\boldsymbol{\beta}_f\right|&amp;gt;$ 0. $K_f&amp;gt;0$ is the dimension of vector $\boldsymbol{f}_t$, $K_g \geq 0$ is the dimension of vector $g_t, L$ is the dimension of vector $z_t(0&amp;lt;L&amp;lt;\min (N, T))$, and $K=K_f+K_g$.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The target&amp;rsquo;s factor loadings $\big(\boldsymbol{\beta}=(\boldsymbol{\beta}_f^{\prime}, \mathbf{0}^{\prime})^{\prime}\big)$ allow the target to depend on a strict subset of the factors driving the predictors.&lt;/li&gt;
&lt;li&gt;We refer to this subset as the relevant factors, which are denoted $\boldsymbol{f}_t$.&lt;/li&gt;
&lt;li&gt;In contrast, irrelevant factors, $\mathbf{g}_t$, do not influence the forecast target but may drive the cross section of predictive information $\boldsymbol{x}_t$.&lt;/li&gt;
&lt;li&gt;The proxies $\boldsymbol{z}_t$ are driven by factors and proxy noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2 (Factors, Loadings and Residuals)&lt;/strong&gt;. Let $M&amp;lt;\infty$. For any $i, s, t$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\mathbb{E}\left|\boldsymbol{F}_t\right|^4&amp;lt;M, T^{-1} \sum_{s=1}^T \boldsymbol{F}_s \underset{T \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{\mu}$ and $T^{-1} \boldsymbol{F}^{\prime} \boldsymbol{J}_T \boldsymbol{F} \underset{T \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{\Delta}_F$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\left|\boldsymbol{\phi}_i\right|^4 \leq M, N^{-1} \sum_{j=1}^N \boldsymbol{\phi}_j \underset{T \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \overline{\boldsymbol{\phi}}, N^{-1} \boldsymbol{\Phi}^{\prime} \boldsymbol{J}_N \boldsymbol{\Phi} \underset{N \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \mathcal{P}$ and $N^{-1} \boldsymbol{\Phi}^{\prime} \boldsymbol{J}_N \boldsymbol{\phi}_0 \underset{N \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{P}_1^6$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\left(\varepsilon_{i t}\right)=0, \mathbb{E}\left|\varepsilon_{i t}\right|^8 \leq M$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\left(\boldsymbol{\omega}_t\right)=\mathbf{0}, \mathbb{E}\left|\boldsymbol{\omega}_t\right|^4 \leq M, T^{-1 / 2} \sum_{s=1}^T \boldsymbol{\omega}_s=\boldsymbol{o}_p(1)$ and $T^{-1} \boldsymbol{\omega}^{\prime} \mathbf{J}_T \boldsymbol{\omega} \underset{N \rightarrow \infty}{\stackrel{p}{\longrightarrow}} \boldsymbol{\Delta}_\omega$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}_t\left(\eta_{t+1}\right)=\mathbb{E}\left(\eta_{t+1} \mid y_t, F_t, y_{t-1}, F_{t-1}, \ldots\right)=0, \mathbb{E}\left(\eta_{t+1}^4\right) \leq M$, and $\eta_{t+1}$ is independent of $\phi_i(m)$ and $\varepsilon_{i, t}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3 (Dependence)&lt;/strong&gt;. Let $x(m)$ denote the $m$ th element of $\boldsymbol{x}$. For $M&amp;lt;\infty$ and any $i, j, t, s, m_1, m_2$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\mathbb{E}\left(\varepsilon_{i t} \varepsilon_{j s}\right)=\sigma_{i j, t s},\left|\sigma_{i j, t s}\right| \leq \bar{\sigma}_{i j}$ and $\left|\sigma_{i j, t s}\right| \leq \tau_{t s}$, and&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;$N^{-1} \sum_{i, j=1}^N \bar{\sigma}_{i j} \leq M$&lt;/li&gt;
&lt;li&gt;$T^{-1} \sum_{t, s=1}^T \tau_{t s} \leq M$&lt;/li&gt;
&lt;li&gt;$N^{-1} \sum_{i, s}\left|\sigma_{i i, t s}\right| \leq M$&lt;/li&gt;
&lt;li&gt;$N^{-1} T^{-1} \sum_{i, j, t, s}\left|\sigma_{i j, t s}\right| \leq M$&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;$\mathbb{E}\left|N^{-1 / 2} T^{-1 / 2} \sum_{s=1}^T \sum_{i=1}^N\left[\varepsilon_{i s} \varepsilon_{i t}-\mathbb{E}\left(\varepsilon_{i s} \varepsilon_{i t}\right)\right]\right|^2 \leq M$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\left|T^{-1 / 2} \sum_{t=1}^T F_t\left(m_1\right) \omega_t\left(m_2\right)\right|^2 \leq M$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\left|T^{-1 / 2} \sum_{t=1}^T \omega_t\left(m_1\right) \varepsilon_{i t}\right|^2 \leq M$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 4 (Central Limit Theorems)&lt;/strong&gt;. For any $i, t$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$N^{-1 / 2} \sum_{i=1}^N \phi_i \varepsilon_{i t} \stackrel{d}{\rightarrow} \mathcal{N}\left(0, \Gamma_{\Phi_{\varepsilon}}\right)$, where $\Gamma_{\Phi_{\varepsilon}}=\operatorname{plim}_{N \rightarrow \infty} N^{-1}$ $\sum_{i, j=1}^N \mathbb{E}\left[\boldsymbol{\phi}_i \boldsymbol{\phi}_j^{\prime} \varepsilon_{i t} \varepsilon_{j t}\right]$&lt;/li&gt;
&lt;li&gt;$T^{-1 / 2} \sum_{t=1}^T \boldsymbol{F}_t \eta_{t+1} \stackrel{d}{\rightarrow} \mathcal{N}\left(0, \Gamma_{F \eta}\right)$, where $\boldsymbol{\Gamma}_{F \eta}=\operatorname{plim}_{T \rightarrow \infty} T^{-1}$ $\sum_{t=1}^T \mathbb{E}\left[\eta_{t+1}^2 \boldsymbol{F}_t \boldsymbol{F}_t^{\prime}\right]&amp;gt;0$&lt;/li&gt;
&lt;li&gt;$T^{-1 / 2} \sum_{t=1}^T \boldsymbol{F}_t \varepsilon_{i t} \stackrel{d}{\rightarrow} \mathcal{N}\left(0, \boldsymbol{\Gamma}_{F \varepsilon, i}\right)$, where $\boldsymbol{\Gamma}_{F \varepsilon, i}=\operatorname{plim}_{T \rightarrow \infty} T^{-1}$ $\sum_{t, s=1}^T \mathbb{E}\left[\boldsymbol{F}_t \boldsymbol{F}_s^{\prime} \varepsilon_{i t} \varepsilon_{i s}\right]&amp;gt;0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 5 (Normalization)&lt;/strong&gt;. $\mathcal{P}=\mathbf{I}, \boldsymbol{P}_1=\mathbf{0}$ and $\boldsymbol{\Delta}_F$ is diagonal, positive definite, and each diagonal element is unique.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 6 (Relevant Proxies)&lt;/strong&gt;. $\boldsymbol{\Lambda}=\left[\boldsymbol{\Lambda}_f, \mathbf{0}\right]$ and $\boldsymbol{\Lambda}_f$ is nonsingular.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;h3 id=&#34;23-consistency&#34;&gt;2.3. Consistency&lt;/h3&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 1.&lt;/strong&gt;&lt;/font&gt; Let Assumptions 1-6 hold. The three-pass regression filter forecast is consistent for the infeasible best forecast,
$$\hat{y}_{t+1} \underset{T, N \rightarrow \infty}{\stackrel{p}{\longrightarrow}}\beta_0+\boldsymbol{F}_t^{\prime} \boldsymbol{\beta}.$$&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 2.&lt;/strong&gt;&lt;/font&gt; Let $\hat{\alpha}_i$ denote the $i$th element of $\hat{\boldsymbol{\alpha}}$, and let Assumptions 1-6 hold. Then for any $i$,
$$
N \hat{\alpha}_i \underset{T, N \rightarrow \infty}{\stackrel{p}{\longrightarrow}}\left(\boldsymbol{\phi}_i-\overline{\boldsymbol{\phi}}\right)^{\prime} \boldsymbol{\beta} .
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3PRF uses only as many predictive factors as the number of factors relevant to $y_{t+1}$&lt;/li&gt;
&lt;li&gt;the PCR forecast is asymptotically efficient when there are as many predictive factors as the total number of factors driving $\boldsymbol{x}_t$&lt;/li&gt;
&lt;li&gt;if the factors driving the target are weak in the sense that they contribute a only small fraction of the total variability in the predictors, &lt;strong&gt;then principal components may have difficulty identifying them&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;strong&gt;Corollary 1.&lt;/strong&gt;&lt;/font&gt; Let Assumptions 1–5 hold with the exception of Assumptions 2.4, 3.3 and 3.4. Additionally, assume that there is only one relevant factor. Then the target-proxy three-pass regression filter forecaster is consistent for the infeasible best forecast.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Corollary 1 holds regardless of the number of irrelevant factors driving $\boldsymbol{X}$ and regardless of where the relevant factor stands in the principal component ordering for $\boldsymbol{X}$.&lt;/li&gt;
&lt;li&gt;Compare this to PCR, whose first predictive factor is ensured to be the one that explains most of the covariance among $\boldsymbol{x}_t$, regardless of that factor&amp;rsquo;s relationship to $y_{t+1}$.&lt;/li&gt;
&lt;li&gt;Only if the relevant factor happens to also drive most of the variation within the predictors does the first component achieve the infeasible best.&lt;/li&gt;
&lt;li&gt;$\Rightarrow$ the forecast performance of the 3PRF is robust to the presence of irrelevant factors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;24-asymptotic-distributions&#34;&gt;2.4. Asymptotic distributions&lt;/h3&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 3.&lt;/strong&gt;&lt;/font&gt; Under Assumptions 1-6, as $N, T \rightarrow \infty$ we have
$$
\frac{\sqrt{T} N\left(\hat{\alpha}_i-\tilde{\alpha}_i\right)}{A_i} \stackrel{d}{\rightarrow} \mathcal{N}(0,1)
$$
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;where&lt;/summary&gt;
  &lt;p&gt;$A_i^2$ is the ith diagonal element of $\widehat{\operatorname{Avar}}(\hat{\boldsymbol{\alpha}})=\boldsymbol{\Omega}_\alpha\left(\frac{1}{T} \sum_t \hat{\eta}_{t+1}^2\left(\boldsymbol{X}_t\right.\right.$ $\left.-\overline{\boldsymbol{X}})\left(\boldsymbol{X}_t-\overline{\boldsymbol{X}}\right)^{\prime}\right) \boldsymbol{\Omega}_\alpha^{\prime}, \hat{\eta}_{t+1}$ is the estimated 3PRF forecast error, $\tilde{\alpha}_i \equiv$ $\boldsymbol{S}_i \boldsymbol{G}_\alpha \boldsymbol{\beta}$, where $\boldsymbol{S}_i$ is selects the ith element of vector $\boldsymbol{G}_\alpha \boldsymbol{\beta}$ and
$$
\begin{aligned}
\boldsymbol{G}_\alpha&amp;amp;=\boldsymbol{J}_N\left(T^{-1} \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{Z}\right)\left(T^{-3} N^{-2} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{S}_{X X} \boldsymbol{W}_{X Z}\right)^{-1}\times\\
&amp;amp;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\left(N^{-1} T^{-2} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{X}^{\prime} \boldsymbol{J}_T \boldsymbol{F}\right),
\end{aligned}
$$
and
$$
\boldsymbol{\Omega}_\alpha=\boldsymbol{J}_N\left(\frac{1}{T} \boldsymbol{S}_{X Z}\right)\left(\frac{1}{T^3 N^2} \boldsymbol{W}_{X Z}^{\prime} \boldsymbol{S}_{X X} \boldsymbol{W}_{X Z}\right)^{-1}\left(\frac{1}{T N} \boldsymbol{W}_{X Z}^{\prime}\right)
$$&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 4.&lt;/strong&gt;&lt;/font&gt; Under Assumptions $1-6$, as $N, T \rightarrow \infty$ we have
$$
\frac{\sqrt{T}\left(\hat{y}_{t+1}-\mathbb{E}_t y_{t+1}\right)}{Q_t} \stackrel{d}{\rightarrow} \mathcal{N}(0,1)
$$
where $\mathbb{E}_t y_{t+1}=\beta_0+\boldsymbol{\beta}^{\prime} \boldsymbol{F}_t$ and $Q_t^2$ is the th diagonal element of $\frac{1}{N^2} J_T \boldsymbol{X} \widehat{\operatorname{Avar}}(\hat{\boldsymbol{\alpha}}) \boldsymbol{X}^{\prime} \boldsymbol{J}_T$&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 5.&lt;/strong&gt;&lt;/font&gt; Under Assumptions $1-6$, as $N, T \rightarrow \infty$ we have
$$
\sqrt{T}\left(\hat{\boldsymbol{\beta}}-\boldsymbol{G}_\beta \boldsymbol{\beta}\right) \stackrel{d}{\rightarrow} \mathcal{N}\left(\mathbf{0}, \boldsymbol{\Sigma}_\beta\right)
$$
where $\boldsymbol{\Sigma}_\beta=\boldsymbol{\Sigma}_z^{-1} \boldsymbol{\Gamma}_{F \eta} \boldsymbol{\Sigma}_z^{-1}$ and $\boldsymbol{\Sigma}_z=\mathbf{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\Delta}_\omega$. Furthermore,
$$
\begin{aligned}
\widehat{\operatorname{Avar}}(\hat{\boldsymbol{\beta}})= &amp;amp; \left(T^{-1} \hat{\boldsymbol{F}}^{\prime} \boldsymbol{J}_T \hat{\boldsymbol{F}}\right)^{-1} T^{-1} \sum_t \hat{\eta}_{t+1}^2\left(\hat{\boldsymbol{F}}_t-\hat{\boldsymbol{\mu}}\right)\left(\hat{\boldsymbol{F}}_t-\hat{\boldsymbol{\mu}}\right)^{\prime} \
&amp;amp; \times\left(T^{-1} \hat{\boldsymbol{F}}^{\prime} \boldsymbol{J}_T \hat{\boldsymbol{F}}\right)^{-1}
\end{aligned}
$$
is a consistent estimator of $\boldsymbol{\Sigma}_\beta$. $\boldsymbol{G}_\beta$ is defined in the Appendix.&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 6.&lt;/strong&gt;&lt;/font&gt; Under Assumptions 1-6, as $N, T \rightarrow \infty$ we have for every $t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $\sqrt{N} / T \rightarrow 0$, then
$$
\sqrt{N}\left[\hat{\boldsymbol{F}}_t-\left(\boldsymbol{H}_0+\boldsymbol{H} \boldsymbol{F}_t\right)\right] \stackrel{d}{\rightarrow} \mathcal{N}\left(\mathbf{0}, \boldsymbol{\Sigma}_F\right)
$$&lt;/li&gt;
&lt;li&gt;if $\liminf \sqrt{N} / T \geq \tau \geq 0$, then
$$
T\left[\hat{\boldsymbol{F}}_t-\left(\boldsymbol{H}_0+\boldsymbol{H F}_t\right)\right]=\boldsymbol{O}_p(1)
$$
where $\boldsymbol{\Sigma}_F=\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\Delta}_\omega\right)\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F^2 \boldsymbol{\Lambda}^{\prime}\right)^{-1} \boldsymbol{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Gamma}_{\Phi \varepsilon} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F^2\right.$ $\left.\boldsymbol{\Lambda}^{\prime}\right)^{-1}\left(\boldsymbol{\Lambda} \boldsymbol{\Delta}_F \boldsymbol{\Lambda}^{\prime}+\boldsymbol{\Delta}_\omega\right) . \boldsymbol{H}_0$ and $\boldsymbol{H}$ are defined in the Appendix.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;25-proxy-selection&#34;&gt;2.5. Proxy selection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;How to select the proxies that depend only on relevant factors ?&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;251-automatic-proxies&#34;&gt;2.5.1. Automatic proxies&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Initialize $\boldsymbol{r}_0=\boldsymbol{y}$.&lt;/li&gt;
&lt;li&gt;For $k=1, \ldots, L$ :
&lt;ul&gt;
&lt;li&gt;Define the $k$ th automatic proxy to be $\boldsymbol{r}_{k-1}$. Stop if $k=L$; otherwise proceed.&lt;/li&gt;
&lt;li&gt;Compute the 3PRF for target $\boldsymbol{y}$ using cross section $\boldsymbol{X}$ using statistical proxies 1 through $k$. Denote the resulting forecast $\hat{\boldsymbol{y}}_k$.&lt;/li&gt;
&lt;li&gt;Calculate $\boldsymbol{r}_k=\boldsymbol{y}-\hat{\boldsymbol{y}}_k$, advance $k$, and go to step 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 7.&lt;/strong&gt;&lt;/font&gt; Let Assumptions 1-5 hold with the exception of Assumptions 2.4, 3.3 and 3.4. Then the L-automatic-proxy three pass regression filter forecaster of $\boldsymbol{y}$ automatically satisfies Assumptions 2.4, 3.3, 3.4 and 6 when $L=K_f$. As a result, &lt;mark&gt;the $L$ automatic-proxy is consistent and asymptotically normal according to Theorems 1 and 4.&lt;/mark&gt;&lt;/p&gt;
&lt;h4 id=&#34;252-theory-proxies&#34;&gt;2.5.2. Theory proxies&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The filter may instead be employed using alternative disciplining variables (factor proxies) which may &lt;mark&gt;be distinct from the target and chosen on the basis of economic theory or by statistical arguments&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;Consider a situation in which $K_f$ is one, so that the target and proxy are given by $y_{t+1}=\beta_0+\beta f_t+\eta_{t+1}$ and $z_t=\lambda_0+\Lambda f_t+\omega_t$.&lt;/li&gt;
&lt;li&gt;Also suppose that the population $R^2$ of the proxy equation is substantially higher than the population $R^2$ of the target equation.
&lt;ul&gt;
&lt;li&gt;The forecasts from using either $z_t$ or the target as proxy are asymptotically identical.&lt;/li&gt;
&lt;li&gt;However, in finite samples, forecasts can be improved by proxying with $z_t$ due to its higher signal-to-noise ratio.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;252-information-criteria&#34;&gt;2.5.2 Information criteria&lt;/h4&gt;
&lt;p&gt;&amp;lsquo;&amp;lsquo;Trace of the Krylov Representation&amp;rsquo;&amp;rsquo; method of Kramer and Sugiyama (2011).&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; \widehat{\operatorname{DoF}}(m)=1+\sum_{j=1}^m c_j \operatorname{trace}\left(\boldsymbol{K}^j\right)-\sum_{l, j=1}^m \boldsymbol{t}_l^{\prime} \mathbf{K}^j \boldsymbol{t}_l \\
&amp;amp; ~~~~~~~~~~~~~~~~~~~~~~+\left(\boldsymbol{y}-\hat{\boldsymbol{y}}_m\right)^{\prime} \sum_{j=1}^m \boldsymbol{K}^j \boldsymbol{v}_j+m
\end{aligned}
$$
where $\boldsymbol{K}=\boldsymbol{X} \boldsymbol{X}^{\prime}, c_j$ are elements of the vector $\boldsymbol{c}=\boldsymbol{B}^{-1} \boldsymbol{T} \boldsymbol{y}, \boldsymbol{B}$ is a Krylov basis decomposition, $\boldsymbol{T}$ is the matrix of PLS factor estimate vectors $\boldsymbol{t}_j$, and $\boldsymbol{v}_j$ are columns of the matrix $\boldsymbol{T}\left(\boldsymbol{B}^{-1}\right)^{\prime}$. The BIC is then calculated as
$$\sum_t\left(y_t-\hat{y}_{m, t}\right)^2 / T+\log (T) \hat{\sigma}^2 \widehat{D o F}(m) / T$$
where $\hat{\sigma}=\sqrt{\sum_t\left(y_t-\hat{y}_{m, t}\right)^2 /(T-D o F(m))}$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-related-procedures&#34;&gt;3. Related procedures&lt;/h2&gt;
&lt;h3 id=&#34;31-constrained-least-squares&#34;&gt;3.1. Constrained least squares&lt;/h3&gt;
&lt;p&gt;&lt;font color=&#34;red&#34;&gt;&lt;strong&gt;Theorem 8.&lt;/strong&gt;&lt;/font&gt; The three-pass regression filter&amp;rsquo;s implied $N$-dimensional predictive coefficient, $\hat{\alpha}$, is the solution to
$$
\begin{aligned}
&amp;amp; \arg \min _{\alpha_0, \boldsymbol{\alpha}}\left\|\boldsymbol{y}-\alpha_0-\boldsymbol{X} \boldsymbol{\alpha}\right\| \\
&amp;amp; \text { subject to } \quad\left(\boldsymbol{I}-\boldsymbol{W}_{X Z}\left(\boldsymbol{S}_{X Z}^{\prime} \boldsymbol{W}_{X Z}\right)^{-1} \boldsymbol{W}_{X Z}\right) \boldsymbol{\alpha}=\mathbf{0} .\quad (5)
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The 3PRF&amp;rsquo;s answer is to impose the constraint in Eq. (5), which exploits the proxies and has an intuitive interpretation.&lt;/li&gt;
&lt;li&gt;Premultiplying both sides of the equation by $\boldsymbol{J}_T \boldsymbol{X}$, we can rewrite the constraint as $\left(\boldsymbol{J}_T \boldsymbol{X}-\boldsymbol{J}_T \hat{\boldsymbol{F}} \hat{\boldsymbol{\Phi}}^{\prime}\right) \boldsymbol{\alpha}=$ 0. For large $N$ and $T$,
$$
\boldsymbol{J}_T \boldsymbol{X}-\boldsymbol{J}_T \hat{\boldsymbol{F}} \hat{\boldsymbol{\Phi}}^{\prime} \approx \boldsymbol{\varepsilon}+(\boldsymbol{F}-\boldsymbol{\mu})\left(\boldsymbol{I}-\boldsymbol{S}_{K_f}\right) \boldsymbol{\Phi}^{\prime}
$$&lt;/li&gt;
&lt;li&gt;Because the covariance between $\boldsymbol{\alpha}$ and $\varepsilon$ is zero by the assumptions of the model, the constraint simply imposes that &lt;mark&gt;the product of $\alpha$ and the target-irrelevant common component of $\boldsymbol{X}$ is equal to zero&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;This is because the matrix $\boldsymbol{I}-\boldsymbol{S}_{K_f}$ selects only the terms in the total common component $\boldsymbol{F} \boldsymbol{\Phi}^{\prime}$ that &lt;mark&gt;are associated with irrelevant factors&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;This constraint is important because it ensures that &lt;mark&gt;factors irrelevant to $\boldsymbol{y}$ drop out of the 3PRF forecast&lt;/mark&gt;. It also ensures that $\hat{\boldsymbol{\alpha}}$ is consistent for the factor model&amp;rsquo;s population projection coefficient of $y_{t+1}$ on $\boldsymbol{x}_t$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;32-partial-least-squares&#34;&gt;3.2. Partial least squares&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-7&#34;&gt;
  &lt;summary&gt;PLS&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;See &lt;a href=&#34;https://aarmey.github.io/ml-for-bioe/public/Wk4-Lecture8.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reference1&lt;/a&gt; and &lt;a href=&#34;https://personal.utdallas.edu/~herve/Abdi-PLS-pretty.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reference2&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The goal of PLS regression is to predict $\mathbf{Y}$ from $\mathbf{X}$ and to &lt;mark&gt;describe their common structure&lt;/mark&gt;.&lt;/li&gt;
&lt;li&gt;When $\mathbf{Y}$ is a vector and $\mathbf{X}$ is full rank, this goal could be accomplished using &lt;strong&gt;ORDINARY MULTIPLE REGRESSION&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;When the number of predictors is large compared to the number of observations, $\mathbf{X}$ is likely to be singular and the regression approach is no longer feasible (i.e., because of MULTICOLLINEARITY).&lt;/li&gt;
&lt;li&gt;Several approaches have been developed to cope with this problem.
&lt;ul&gt;
&lt;li&gt;One approach is to eliminate some predictors (e.g., using stepwise methods)&lt;/li&gt;
&lt;li&gt;Another one, called principal component regression (PCR): perform a PRINCIPAL COMPONENT ANALYSIS (PCA) of the $\mathbf{X}$ matrix and then use the principal components of $\mathbf{X}$ as regressors on $\mathbf{Y}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How to choose an optimum subset of predictors ? A possible strategy is to keep only a few of the first components.
&lt;ul&gt;
&lt;li&gt;They are chosen to explain $\mathbf{X}$ rather than $\mathbf{Y}$, and so, &lt;font color=&#34;red&#34;&gt;nothing guarantees that the principal components, which &amp;ldquo;explain&amp;rdquo; $\mathbf{X}$, are relevant for $\mathbf{Y}$.&lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;By contrast, PLS regression finds components from $\mathbf{X}$ that are also relevant for $\mathbf{Y}$.&lt;/li&gt;
&lt;li&gt;Specifically, PLS regression searches for a set of components (called latent vectors) that performs a simultaneous decomposition of $\mathbf{X}$ and $\mathbf{Y}$ with &lt;mark&gt;the constraint that these components explain as much as possible of the covariance between $\mathbf{X}$ and $\mathbf{Y}$&lt;/mark&gt;. This step generalizes PCA.&lt;/li&gt;
&lt;li&gt;It is followed by a regression step where the decomposition of $\mathbf{X}$ is used to predict $\mathbf{Y}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;partial least squares (PLS) constructs forecasting indices as linear combinations of the underlying predictors.&lt;/li&gt;
&lt;li&gt;These predictive indices are referred to as &amp;ldquo;directions&amp;rdquo; in the language of PLS.&lt;/li&gt;
&lt;li&gt;The PLS forecast based on the first $K$ PLS directions, $\hat{\boldsymbol{y}}^{(k)}$, is constructed according to the following algorithm (as stated in Hastie et al. (2009)):
&lt;ol&gt;
&lt;li&gt;Standardize each $\mathbf{x}_i$ to have mean zero and variance one by setting $\tilde{\mathbf{x}}_i=\frac{\mathbf{x}_i-\hat{\mathbb{E}}\left[\mathrm{x}_{i t}\right]}{\hat{\sigma}\left(\mathrm{x}_{i t}\right)}, i=1, \ldots, N$&lt;/li&gt;
&lt;li&gt;Set $\hat{\boldsymbol{y}}^{(0)}=\bar{y}$, and $\mathbf{x}_i^{(0)}=\tilde{\mathbf{x}}_i, i=1, \ldots, N$&lt;/li&gt;
&lt;li&gt;For $k=1,2, \ldots, K$&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol{u}_k=\sum_{i=1}^N \hat{\phi}_{k i} \mathbf{x}_i^{(k-1)}$, where $\hat{\phi}_{k i}=\widehat{\operatorname{Cov}}\left(\mathbf{x}_i^{(k-1)}, \boldsymbol{y}\right)$&lt;/li&gt;
&lt;li&gt;$\hat{\beta}_k=\widehat{\operatorname{Cov}}\left(\boldsymbol{u}_k, \boldsymbol{y}\right) / \widehat{\operatorname{Var}}\left(\boldsymbol{u}_k\right)$&lt;/li&gt;
&lt;li&gt;$\hat{\boldsymbol{y}}^{(k)}=\hat{\boldsymbol{y}}^{(k-1)}+\hat{\beta}_k \boldsymbol{u}_k$&lt;/li&gt;
&lt;li&gt;Orthogonalize each $\mathbf{x}_i^{(k-1)}$ with respect to $\boldsymbol{u}_k$ :
$$
\begin{aligned}
\mathbf{x}_i^{(k)} &amp;amp; =\mathbf{x}_i^{(k-1)}-\left(\widehat{\operatorname{Cov}}\left(\boldsymbol{u}_k, \mathbf{x}_i^{(k-1)}\right) / \widehat{\operatorname{Var}}\left(\boldsymbol{u}_k\right)\right) \boldsymbol{u}_k, \\
i &amp;amp; =1,2, \ldots, N .
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;ul&gt;
&lt;li&gt;partial least squares forecasts are identical to those from the 3PRF when
&lt;ol&gt;
&lt;li&gt;the predictors are demeaned and variance-standardized in a preliminary step&lt;/li&gt;
&lt;li&gt;the first two regression passes are run without constant terms&lt;/li&gt;
&lt;li&gt;proxies are automatically selected.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consider the case where a single predictive index is constructed from the partial least squares algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assume, for the time being, that each predictor has been previously standardized to have mean zero and variance one. Following the construction of the PLS forecast given above, we have&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Set $\hat{\phi}_i=x_i^{\prime} y$, and $\hat{\boldsymbol{\Phi}}=\left(\hat{\phi}_1, \ldots, \hat{\phi}_N\right)^{\prime}$.&lt;/li&gt;
&lt;li&gt;Set $\hat{u}_t=\boldsymbol{x}_t^{\prime} \hat{\Phi}$, and $\hat{\boldsymbol{u}}=\left(\hat{u}_1, \ldots, \hat{u}_T\right)^{\prime}$.&lt;/li&gt;
&lt;li&gt;Run a predictive regression of $\boldsymbol{y}$ on $\hat{\boldsymbol{u}}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Constructing the forecast in this manner may be represented as a one-step estimator
$$
\hat{\boldsymbol{y}}^{\mathrm{PLS}}=\boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{y}\left(\boldsymbol{y}^{\prime} \boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{y}\right)^{-1} \boldsymbol{y}^{\prime} \boldsymbol{X} \boldsymbol{X}^{\prime} \boldsymbol{y}
$$&lt;/li&gt;
&lt;li&gt;&lt;font color=&#34;red&#34;&gt;which upon inspection is identical to the 1-automatic-proxy 3PRF forecast when constants are omitted from the first and second passes. &lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;4-empirical-evidence&#34;&gt;4. Empirical evidence&lt;/h2&gt;
&lt;h3 id=&#34;41-forecasting-macroeconomic-aggregates&#34;&gt;4.1. Forecasting macroeconomic aggregates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Quarterly data from Stock and Watson (2012) for the sample 1959:I-2009:IV&lt;/li&gt;
&lt;li&gt;Take as our predictors a set of 108 macroeconomic variables compiled by Stock and Watson (2012)&lt;/li&gt;
&lt;li&gt;Out-of-sample $R^2$ of one quarter ahead forecasts, in percentage.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;3PRF1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PCR1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PCLAR&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PCLAS&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;10LAR&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;FA1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;GDP&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;30.12&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$35.18^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;29.70&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;29.51&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;26.38&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Consumption&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$23.20^{\mathrm{a},{ }^*}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;7.06&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.12&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;7.32&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-14.85&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Investment&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$38.88^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.37&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;36.81&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;36.30&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;24.01&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;34.35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Exports&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$16.75^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13.25&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-11.58&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-9.42&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-61.36&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Imports&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$37.18^a$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;36.50&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18.46&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16.93&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;22.77&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;36.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Industrial Production&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$16.56^{\mathrm{a},{ }^*}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;8.92&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5.67&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5.71&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;11.04&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;12.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Capacity Utilization&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;54.32&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;54.79&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;53.77&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;54.85&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$64.69^{a,{}^*}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;55.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Total Hours&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$53.81^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;50.47&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;48.58&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;47.39&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;39.56&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;42.53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Total Employment&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$48.84^a$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;47.27&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;38.14&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.16&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18.91&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;41.73&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Average Hours&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$20.12^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10.12&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18.52&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13.89&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17.55&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;15.84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Housing Starts&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;26.97&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-0.14&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;31.54&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;29.66&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$46.89^a$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;GDP Inflation&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.64&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.05&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-0.94&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.38&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-5.89&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$2.80^{\mathrm{a}}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PCE Inflation&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-1.29&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-3.73&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10.60&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.82&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$12.22^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-2.50&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;42-forecasting-market-returns&#34;&gt;4.2. Forecasting market returns&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Building from the present value identity, Kelly and Pruitt (2013) map the cross section of price–dividend ratios into the approximate latent factor model of Assumption 1, and argue that &lt;mark&gt;this set of predictors should possess forecasting power for log returns on the aggregate market.&lt;/mark&gt;&lt;/li&gt;
&lt;li&gt;We estimate the extent of market return predictability using 25 log price-dividend ratios of portfolios sorted by market equity and book-to-market ratio.&lt;/li&gt;
&lt;li&gt;The data is annual over the post-war period 1945-2010 (following Fama and French (1992)).&lt;/li&gt;
&lt;li&gt;We assume that the predictors take
&lt;ul&gt;
&lt;li&gt;the form $p d_{i, t}=\phi_{i, 0}+\boldsymbol{\phi}_i^{\prime} \boldsymbol{F}_t+\varepsilon_{i, t}$&lt;/li&gt;
&lt;li&gt;the target takes the form $r_{t+1}=\beta_0^r+\boldsymbol{F}_t^{\prime} \boldsymbol{\beta}^r+\eta_{t+1}^r$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;3PRF1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;3PRF2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;3PRF-IC&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PC1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Return $R^2$ &lt;br&gt; # of factors&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;27.63&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$36.34^{\mathrm{a}}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;31.15 &lt;br&gt; 1.36&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-10.45&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;PC2&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;PC-IC&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;10LAR&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;FA1&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Return $R^2$ &lt;br&gt; # of factors&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-8.89&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;27.00 &lt;br&gt; 4.58&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13.28&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-10.08&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Notes on &#39;&#39;On factor models with random missing: EM estimation, inference, and cross validation&#39;&#39;</title>
      <link>https://ikerlz.github.io/notes/factor-model/</link>
      <pubDate>Tue, 14 Nov 2023 22:16:45 +0800</pubDate>
      <guid>https://ikerlz.github.io/notes/factor-model/</guid>
      <description>

&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-large-dimensional-factor-models-with-random-missing&#34;&gt;2. Large dimensional factor models with random missing&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#21-em-estimation&#34;&gt;2.1. EM estimation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-determining-the-number-of-factors-via-cross-validation&#34;&gt;3. Determining the number of factors via cross validation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-monte-carlo-simulations&#34;&gt;4. Monte Carlo simulations&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-empirical-application-forecasting-macroeconomic-variables&#34;&gt;5. Empirical application: Forecasting macroeconomic variables&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;hr&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /media/notes/factor-model/paper_intro_hu1fc2d9e7412cda710028560e51ff3652_53793_21d412abc8825e11623e78b70c39680f.webp 400w,
               /media/notes/factor-model/paper_intro_hu1fc2d9e7412cda710028560e51ff3652_53793_c705a68ddc94c1a3588600831f692041.webp 760w,
               /media/notes/factor-model/paper_intro_hu1fc2d9e7412cda710028560e51ff3652_53793_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://ikerlz.github.io/media/notes/factor-model/paper_intro_hu1fc2d9e7412cda710028560e51ff3652_53793_21d412abc8825e11623e78b70c39680f.webp&#34;
               width=&#34;670&#34;
               height=&#34;260&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-large-dimensional-factor-models-with-random-missing&#34;&gt;2. Large dimensional factor models with random missing&lt;/h2&gt;
&lt;h3 id=&#34;21-em-estimation&#34;&gt;2.1. EM estimation&lt;/h3&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-determining-the-number-of-factors-via-cross-validation&#34;&gt;3. Determining the number of factors via cross validation&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;4-monte-carlo-simulations&#34;&gt;4. Monte Carlo simulations&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;5-empirical-application-forecasting-macroeconomic-variables&#34;&gt;5. Empirical application: Forecasting macroeconomic variables&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
